{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np, os, sys\n",
    "from tqdm.notebook import tqdm    \n",
    "from manipulations import get_classes, get_classes_from_header, get_Fs_from_header, load_challenge_data\n",
    "from sklearn.model_selection import KFold, StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_CLASS = 27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_RR = 750 # 60 beats/min => 60 beats/60 s ==> beat/1s ==> 500 samples / beat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from manipulations import get_abbr, get_name\n",
    "from global_vars import labels, Dx_map, Dx_map_unscored\n",
    "first_idx = {scored_code: None for scored_code in list(Dx_map['SNOMED CT Code'])}\n",
    "first_idx_unscored = {unscored_code: None for unscored_code in list(Dx_map_unscored['SNOMED CT Code'])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from global_vars import labels, normal_class, equivalent_mapping\n",
    "normal_idx = np.argwhere(labels==int(normal_class))\n",
    "def get_scored_class(code, labels):\n",
    "    return [1 if label in code else 0 for label in labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset  1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01ea3181f84b4b84856882c569d3de4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Done.\n",
      "Dataset  2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b97c8e60114f4786adeed50fde37014d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Done.\n",
      "Dataset  3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1515d9a324004d9e9b4ba847a045add5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Done.\n",
      "Dataset  4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "269c05162c7549fabdc7db4cfd2a6228",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Done.\n",
      "Dataset  5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76d19eb259fa4f2d85440cd2509114fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Done.\n",
      "Dataset  6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "206aaf0bcbd945c68c6df37bf6b8d99a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    Datas = []\n",
    "    Header_datas = []\n",
    "    Classes = []\n",
    "    Codes = []\n",
    "    \n",
    "    dataset_idx = {}\n",
    "    dataset_data_labels = {}\n",
    "    dataset_train_idx = {}\n",
    "    dataset_test_idx = {}\n",
    "    \n",
    "    global_idx = 0\n",
    "    datasets = [1,2,3,4,5,6]\n",
    "    for dataset in datasets:\n",
    "        print('Dataset ', dataset)\n",
    "        # Parse arguments.\n",
    "        if len(sys.argv) != 3:\n",
    "            raise Exception('Include the input and output directories as arguments, e.g., python driver.py input output.')\n",
    "\n",
    "        input_directory = '../NewData/{}/'.format(dataset)\n",
    "        output_directory = '../Output/'\n",
    "\n",
    "        # Find files.\n",
    "        input_files = []\n",
    "        for f in os.listdir(input_directory):\n",
    "            if os.path.isfile(os.path.join(input_directory, f)) and not f.lower().startswith('.') and f.lower().endswith('mat'):\n",
    "                input_files.append(f)\n",
    "\n",
    "        if not os.path.isdir(output_directory):\n",
    "            os.mkdir(output_directory)\n",
    "\n",
    "        classes=get_classes(input_directory,input_files)\n",
    "\n",
    "        num_files = len(input_files)\n",
    "        datas = []\n",
    "        header_datas = []\n",
    "        dataset_data_labels[dataset] = []\n",
    "        dataset_idx[dataset] = []\n",
    "        for i, f in tqdm(enumerate(input_files)):\n",
    "            #print('    {}/{}...'.format(i+1, num_files), f)\n",
    "            tmp_input_file = os.path.join(input_directory,f)\n",
    "            data,header_data = load_challenge_data(tmp_input_file)\n",
    "            \n",
    "            codes = get_classes_from_header(header_data)\n",
    "            data_labels = get_scored_class(codes, labels)\n",
    "            \n",
    "            datas.append(data[:,1000:7000])\n",
    "            header_datas.append(header_data)\n",
    "            dataset_data_labels[dataset].append(data_labels)\n",
    "            dataset_idx[dataset].append(global_idx)\n",
    "            global_idx += 1\n",
    "\n",
    "        Datas += datas\n",
    "        Header_datas += header_datas\n",
    "        Classes += classes\n",
    "        \n",
    "        kf = MultilabelStratifiedKFold(5)\n",
    "        train_idx, test_idx = next(kf.split(datas, np.array(dataset_data_labels[dataset])))\n",
    "\n",
    "\n",
    "        dataset_train_idx[dataset] = train_idx\n",
    "        dataset_test_idx[dataset] = test_idx\n",
    "        \n",
    "        \n",
    "        print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [1,2,3,4,5,6]\n",
    "all_train_idx = []\n",
    "for dataset in datasets:\n",
    "    all_train_idx.extend(dataset_train_idx[dataset])\n",
    "    \n",
    "all_test_idx = []\n",
    "for dataset in datasets:\n",
    "    all_test_idx.extend(dataset_test_idx[dataset])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from signal_processing import myfilter, main_QRST\n",
    "# Codes = []\n",
    "# Q_locs = []\n",
    "# idxes = []\n",
    "# for idx in tqdm(range(0, len(Datas))): \n",
    "#     codes = get_classes_from_header(Header_datas[idx])\n",
    "#     names = ', '.join([get_name(int(code), Dx_map, Dx_map_unscored) for code in codes])\n",
    "\n",
    "#     filtered_Data = myfilter(Datas[idx], 500, vis=False)\n",
    "\n",
    "#     # get the lead to apply Pan Tomkins\n",
    "#     Q_loc = main_QRST(filtered_Data, idx, '', '', names, fig2=False)\n",
    "\n",
    "#     # store\n",
    "#     Codes.append(codes)\n",
    "#     Q_locs.append(Q_loc)\n",
    "\n",
    "#     idxes.append(idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from signal_processing import myfilter, extract_QRST, main_QRST\n",
    "# idx = 1511#37581#2932#6884 #6884 #6951 #227#8074 #325#158#325#34#17#8659#4#18#61#10927#6886# 8659\n",
    "# codes = get_classes_from_header(Header_datas[idx])\n",
    "# names = ', '.join([get_name(int(code), Dx_map, Dx_map_unscored) for code in codes])\n",
    "# filtered_Data = myfilter(Datas[idx], 500, vis=False)\n",
    "# #filtered_Data = myfilter(Datas[idx][:,-4000:-1000], 500, vis=False)\n",
    "# Q_loc = main_QRST(filtered_Data, idx, '', '', names, fig2=False, verbose=True, vis=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# with open('../saved/newData_idxes_1000_7000_peakdist100.pkl', 'wb') as idxes_file:\n",
    "#     pickle.dump(idxes, idxes_file)\n",
    "\n",
    "# with open('../saved/newData_Codes_1000_7000_peakdist100.pkl', 'wb') as Codes_file:\n",
    "#     pickle.dump(Codes, Codes_file)\n",
    "\n",
    "# with open('../saved/newData_Q_locs_1000_7000_peakdist100.pkl', 'wb') as Q_locs_file:\n",
    "#     pickle.dump(Q_locs, Q_locs_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "Q_locs = None\n",
    "with open('../saved/newData_Q_locs_1000_7000_peakdist100.pkl', 'rb') as Q_locs_file:\n",
    "    Q_locs = pickle.load(Q_locs_file)\n",
    "\n",
    "Codes = None\n",
    "with open('../saved/newData_Codes_1000_7000_peakdist100.pkl', 'rb') as Codes_file:\n",
    "    Codes = pickle.load(Codes_file)\n",
    "\n",
    "idxes = None\n",
    "with open('../saved/newData_idxes_1000_7000_peakdist100.pkl', 'rb') as idxes_file:\n",
    "    idxes = pickle.load(idxes_file)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43101"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Codes = []\n",
    "# for idx in tqdm(range(0, len(Datas))): \n",
    "#     codes = get_classes_from_header(Header_datas[idx])\n",
    "#     # store\n",
    "#     Codes.append(codes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_labels = np.array([get_scored_class(codes, labels) for codes in Codes])\n",
    "# print(data_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# key_idxes = []\n",
    "# for key in equivalent_mapping.keys():\n",
    "#     print(key)\n",
    "#     key_idx = np.argwhere(labels==int(key)).flatten()[0]\n",
    "#     key_idxes.append(key_idx)\n",
    "#     val_idx = np.argwhere(labels==int(equivalent_mapping[key])).flatten()[0]\n",
    "#     key_pos = np.argwhere(data_labels[:,key_idx]==1).flatten()\n",
    "#     val_pos = np.argwhere(data_labels[:,val_idx]==1).flatten()\n",
    "#     data_labels[key_pos,val_idx] = 1\n",
    "#     data_labels[val_pos,key_idx] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# non_empty_idx = np.argwhere(np.sum(data_labels, axis=1)!=0).flatten()\n",
    "# empty_idx = np.argwhere(np.sum(data_labels, axis=1)==0).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff45cdf9946f42c99a2ef095cb15d12f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=34481.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yfeng/anaconda3/envs/physioNet/lib/python3.6/site-packages/numpy/core/fromnumeric.py:3335: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/home/yfeng/anaconda3/envs/physioNet/lib/python3.6/site-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "X_train = []\n",
    "Data_labels_train = []\n",
    "Idxes_train = []\n",
    "Idxes_dict_train = {}\n",
    "ct = 0\n",
    "for i in tqdm(all_train_idx):\n",
    "    Q_loc = Q_locs[i]\n",
    "    RR_avg = np.median([Q_loc[k+1] - Q_loc[k] for k in range(len(Q_loc)-1)])\n",
    "    RR_th = (0.3 * RR_avg, 3 * RR_avg)\n",
    "    \n",
    "    ks = [k for k in range(len(Q_loc)-1) if Q_loc[k+1] - Q_loc[k] > RR_th[0] \n",
    "              and Q_loc[k+1] - Q_loc[k] < RR_th[1]]\n",
    "    \n",
    "    \n",
    "    Idxes_dict_train[i] = []\n",
    "    for k in ks:\n",
    "        Data_labels_train.append(get_scored_class(Codes[i], labels))\n",
    "        X_train.append(Datas[i][:,1000+Q_loc[k]:1000+Q_loc[k+1]])\n",
    "        Idxes_train.append(i)\n",
    "        Idxes_dict_train[i].append(ct)\n",
    "        \n",
    "    ct += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49c499a852b5456291cd110fb79e13cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=8620.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "X_test = []\n",
    "Data_labels_test = []\n",
    "Idxes_test = []\n",
    "Idxes_dict_test = {}\n",
    "ct = 0\n",
    "for i in tqdm(all_test_idx):\n",
    "        \n",
    "    Q_loc = Q_locs[i]\n",
    "    RR_avg = np.median([Q_loc[k+1] - Q_loc[k] for k in range(len(Q_loc)-1)])\n",
    "    RR_th = (0.3 * RR_avg, 3 * RR_avg)\n",
    "    \n",
    "    ks = [k for k in range(len(Q_loc)-1) if Q_loc[k+1] - Q_loc[k] > RR_th[0] \n",
    "              and Q_loc[k+1] - Q_loc[k] < RR_th[1]]\n",
    "    \n",
    "    \n",
    "    Idxes_dict_test[i] = []\n",
    "    for k in ks:\n",
    "        Data_labels_test.append(get_scored_class(Codes[i], labels))\n",
    "        X_test.append(Datas[i][:,1000+Q_loc[k]:1000+Q_loc[k+1]])\n",
    "        Idxes_test.append(i)\n",
    "        Idxes_dict_test[i].append(ct)\n",
    "        \n",
    "    ct += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "Signals_train = np.zeros((len(X_train),12,MAX_RR))\n",
    "for i in range(len(X_train)):\n",
    "    Signals_train[i,:,:min(len(X_train[i][0]),MAX_RR)] = X_train[i][:,:MAX_RR]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "Signals_test = np.zeros((len(X_test),12,MAX_RR))\n",
    "for i in range(len(X_test)):\n",
    "    Signals_test[i,:,:min(len(X_test[i][0]),MAX_RR)] = X_test[i][:,:MAX_RR]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('../saved/Signals_train_stratified.npy', Signals_train)\n",
    "np.save('../saved/Signals_test_stratified.npy', Signals_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('../saved/Data_labels_train_stratified.pkl', 'wb') as Data_labels_train_file:\n",
    "    pickle.dump(Data_labels_train, Data_labels_train_file)\n",
    "\n",
    "with open('../saved/Data_labels_test_stratified.pkl', 'wb') as Data_labels_test_file:\n",
    "    pickle.dump(Data_labels_test, Data_labels_test_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# On y va"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "Signals_train = np.load('../saved/Signals_train_stratified.npy')\n",
    "Signals_test = np.load('../saved/Signals_test_stratified.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('../saved/Data_labels_train_stratified.pkl', 'rb') as Data_labels_train_file:\n",
    "    Data_labels_train = pickle.load(Data_labels_train_file)\n",
    "\n",
    "with open('../saved/Data_labels_test_stratified.pkl', 'rb') as Data_labels_test_file:\n",
    "    Data_labels_test = pickle.load(Data_labels_test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_name = 'resnet50_minibatch_adam_batch512_0.01_stratified'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# default `log_dir` is \"runs\" - we'll be more specific here\n",
    "writer = SummaryWriter('runs/{}'.format(run_name))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:0')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def weighted_binary_cross_entropy2(sigmoid_x, y, weighted_matrix, weight=None, reduction=None):\n",
    "    \"\"\"\n",
    "    Aha this is correct!\n",
    "    sigmoid_x = nn.Sigmoid()(x)\n",
    "    Args:\n",
    "        sigmoid_x: predicted probability of size [N,C], N sample and C Class. Eg. Must be in range of [0,1]\n",
    "        targets: true value, one-hot-like vector of size [N,C]\n",
    "        pos_weight: Weight for postive sample\n",
    "    \"\"\"\n",
    "    if not (y.size() == sigmoid_x.size()):\n",
    "        raise ValueError(\"Target size ({}) must be the same as input size ({})\".format(y.size(), sigmoid_x.size()))\n",
    "   \n",
    "    #print(\"y.size(), sigmoid_x.size()\", y.size(), sigmoid_x.size())\n",
    "    sigmoid_x = torch.clamp(sigmoid_x,min=1e-7,max=1-1e-7) \n",
    "    loss = - torch.matmul(y*sigmoid_x.log() + (1-y)*(1-sigmoid_x).log(), weighted_matrix)\n",
    "    \n",
    "    if weight is not None:\n",
    "        loss = loss * weight\n",
    "        \n",
    "    if reduction is None:\n",
    "        return loss\n",
    "    elif reduction == 'mean':\n",
    "        return loss.mean()\n",
    "    elif reduction == 'sum':\n",
    "        return loss.sum()\n",
    "    return None\n",
    "    \n",
    "class WeightedBCELoss(nn.Module):\n",
    "    def __init__(self, weights, PosWeightIsDynamic= False, WeightIsDynamic= False, \n",
    "                 reduction='mean'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            pos_weight = Weight for postive samples. Size [1,C]\n",
    "            weight = Weight for Each class. Size [1,C]\n",
    "            PosWeightIsDynamic: If True, the pos_weight is computed on each batch. If pos_weight is None, then it remains None.\n",
    "            WeightIsDynamic: If True, the weight is computed on each batch. If weight is None, then it remains None.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.register_buffer('weights', weights)\n",
    "        self.reduction = reduction\n",
    "        self.PosWeightIsDynamic = PosWeightIsDynamic\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        if self.PosWeightIsDynamic:\n",
    "            positive_counts = target.sum(dim=0)\n",
    "            nBatch = len(target)\n",
    "            self.pos_weight = (nBatch - positive_counts)/(positive_counts +1e-5)\n",
    "\n",
    "\n",
    "        return weighted_binary_cross_entropy2(input, target,\n",
    "                                             weighted_matrix=self.weights,\n",
    "                                             reduction=self.reduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "Signal_12leads_train = np.transpose(Signals_train, (1,0,2))\n",
    "Signal_12leads_test= np.transpose(Signals_test, (1,0,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "\n",
    "import os\n",
    "class SignalDataset(Dataset):\n",
    "\n",
    "    def __init__(self, signals, labels):\n",
    "        self.signals = signals\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.signals)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        sample =(torch.cat([torch.Tensor(np.array([self.signals[channel,idx]]).transpose()) for channel in range(12)], axis=1), \n",
    "                  torch.Tensor(self.labels[idx]))\n",
    "\n",
    "        return sample\n",
    "    \n",
    "signal_datasets_train = SignalDataset(Signal_12leads_train, np.array(Data_labels_train)[:,:N_CLASS])\n",
    "signal_datasets_test = SignalDataset(Signal_12leads_test, np.array(Data_labels_test)[:,:N_CLASS])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.array(Data_labels_train).shape[1] == N_CLASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([750, 12])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "signal_datasets_train[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from global_vars import weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_tensor = torch.Tensor(weights).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation.evaluate_12ECG_score import compute_modified_confusion_matrix, compute_challenge_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_score(labels, outputs, weights, normal_index=normal_idx):\n",
    "    num_recordings, num_classes = np.shape(labels)\n",
    "    # Compute the observed score.\n",
    "    A = compute_modified_confusion_matrix(labels, outputs)\n",
    "    observed_score = np.nansum(weights * A)\n",
    "\n",
    "    # Compute the score for the model that always chooses the correct label(s).\n",
    "    correct_outputs = labels\n",
    "    A = compute_modified_confusion_matrix(labels, correct_outputs)\n",
    "    correct_score = np.nansum(weights * A)\n",
    "\n",
    "    # Compute the score for the model that always chooses the normal class.\n",
    "    inactive_outputs = np.zeros((num_recordings, num_classes), dtype=np.bool)\n",
    "    inactive_outputs[:, normal_index] = 1\n",
    "    A = compute_modified_confusion_matrix(labels, inactive_outputs)\n",
    "    inactive_score = np.nansum(weights * A)\n",
    "\n",
    "    if correct_score != inactive_score:\n",
    "        normalized_score = float(observed_score - inactive_score) / float(correct_score - inactive_score)\n",
    "    else:\n",
    "        normalized_score = float('nan')\n",
    "\n",
    "    return normalized_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_pr_curve_tensorboard(class_index, test_probs, test_preds, names, global_step, prefix):\n",
    "    '''\n",
    "    Takes in a \"class_index\" from 0 to 9 and plots the corresponding\n",
    "    precision-recall curve\n",
    "    '''\n",
    "\n",
    "    writer.add_pr_curve(prefix + '_' + names[class_index],\n",
    "                        test_preds[:,class_index],\n",
    "                        test_probs[:,class_index],\n",
    "                        global_step=global_step)\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_weight = np.ones(N_CLASS) * 2\n",
    "pos_weight = torch.Tensor(pos_weight).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "dummy = torch.ones((10, 750, 48))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ECGResNet50(\n",
      "  (encoder): ResNetEncoder(\n",
      "    (gate): Sequential(\n",
      "      (0): Conv1d(750, 64, kernel_size=(9,), stride=(1,), bias=False)\n",
      "      (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): MaxPool1d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (blocks): ModuleList(\n",
      "      (0): ResNetLayer(\n",
      "        (blocks): Sequential(\n",
      "          (0): ResNet753Block(\n",
      "            (blocks): Sequential(\n",
      "              (0): Sequential(\n",
      "                (0): Conv1dAuto(64, 64, kernel_size=(7,), stride=(1,), padding=(3,), bias=False)\n",
      "                (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              )\n",
      "              (1): ReLU(inplace=True)\n",
      "              (2): Sequential(\n",
      "                (0): Conv1dAuto(64, 64, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
      "                (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              )\n",
      "              (3): ReLU(inplace=True)\n",
      "              (4): Sequential(\n",
      "                (0): Conv1dAuto(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
      "                (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              )\n",
      "            )\n",
      "            (activate): ReLU(inplace=True)\n",
      "            (shortcut): Sequential(\n",
      "              (0): Conv1d(64, 64, kernel_size=(1,), stride=(1,), bias=False)\n",
      "              (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (1): ResNet753Block(\n",
      "            (blocks): Sequential(\n",
      "              (0): Sequential(\n",
      "                (0): Conv1dAuto(64, 64, kernel_size=(7,), stride=(1,), padding=(3,), bias=False)\n",
      "                (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              )\n",
      "              (1): ReLU(inplace=True)\n",
      "              (2): Sequential(\n",
      "                (0): Conv1dAuto(64, 64, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
      "                (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              )\n",
      "              (3): ReLU(inplace=True)\n",
      "              (4): Sequential(\n",
      "                (0): Conv1dAuto(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
      "                (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              )\n",
      "            )\n",
      "            (activate): ReLU(inplace=True)\n",
      "            (shortcut): Sequential(\n",
      "              (0): Conv1d(64, 64, kernel_size=(1,), stride=(1,), bias=False)\n",
      "              (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): ResNetLayer(\n",
      "        (blocks): Sequential(\n",
      "          (0): ResNet753Block(\n",
      "            (blocks): Sequential(\n",
      "              (0): Sequential(\n",
      "                (0): Conv1dAuto(64, 64, kernel_size=(7,), stride=(1,), padding=(3,), bias=False)\n",
      "                (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              )\n",
      "              (1): ReLU(inplace=True)\n",
      "              (2): Sequential(\n",
      "                (0): Conv1dAuto(64, 64, kernel_size=(5,), stride=(2,), padding=(2,), bias=False)\n",
      "                (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              )\n",
      "              (3): ReLU(inplace=True)\n",
      "              (4): Sequential(\n",
      "                (0): Conv1dAuto(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
      "                (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              )\n",
      "            )\n",
      "            (activate): ReLU(inplace=True)\n",
      "            (shortcut): Sequential(\n",
      "              (0): Conv1d(64, 64, kernel_size=(1,), stride=(2,), bias=False)\n",
      "              (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (1): ResNet753Block(\n",
      "            (blocks): Sequential(\n",
      "              (0): Sequential(\n",
      "                (0): Conv1dAuto(64, 64, kernel_size=(7,), stride=(1,), padding=(3,), bias=False)\n",
      "                (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              )\n",
      "              (1): ReLU(inplace=True)\n",
      "              (2): Sequential(\n",
      "                (0): Conv1dAuto(64, 64, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
      "                (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              )\n",
      "              (3): ReLU(inplace=True)\n",
      "              (4): Sequential(\n",
      "                (0): Conv1dAuto(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
      "                (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              )\n",
      "            )\n",
      "            (activate): ReLU(inplace=True)\n",
      "            (shortcut): Sequential(\n",
      "              (0): Conv1d(64, 64, kernel_size=(1,), stride=(1,), bias=False)\n",
      "              (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (2): ResNet753Block(\n",
      "            (blocks): Sequential(\n",
      "              (0): Sequential(\n",
      "                (0): Conv1dAuto(64, 64, kernel_size=(7,), stride=(1,), padding=(3,), bias=False)\n",
      "                (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              )\n",
      "              (1): ReLU(inplace=True)\n",
      "              (2): Sequential(\n",
      "                (0): Conv1dAuto(64, 64, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
      "                (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              )\n",
      "              (3): ReLU(inplace=True)\n",
      "              (4): Sequential(\n",
      "                (0): Conv1dAuto(64, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
      "                (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              )\n",
      "            )\n",
      "            (activate): ReLU(inplace=True)\n",
      "            (shortcut): Sequential(\n",
      "              (0): Conv1d(64, 128, kernel_size=(1,), stride=(1,), bias=False)\n",
      "              (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (2): ResNetLayer(\n",
      "        (blocks): Sequential(\n",
      "          (0): ResNet753Block(\n",
      "            (blocks): Sequential(\n",
      "              (0): Sequential(\n",
      "                (0): Conv1dAuto(128, 128, kernel_size=(7,), stride=(1,), padding=(3,), bias=False)\n",
      "                (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              )\n",
      "              (1): ReLU(inplace=True)\n",
      "              (2): Sequential(\n",
      "                (0): Conv1dAuto(128, 128, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
      "                (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              )\n",
      "              (3): ReLU(inplace=True)\n",
      "              (4): Sequential(\n",
      "                (0): Conv1dAuto(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
      "                (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              )\n",
      "            )\n",
      "            (activate): ReLU(inplace=True)\n",
      "            (shortcut): Sequential(\n",
      "              (0): Conv1d(128, 128, kernel_size=(1,), stride=(1,), bias=False)\n",
      "              (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (1): ResNet753Block(\n",
      "            (blocks): Sequential(\n",
      "              (0): Sequential(\n",
      "                (0): Conv1dAuto(128, 128, kernel_size=(7,), stride=(1,), padding=(3,), bias=False)\n",
      "                (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              )\n",
      "              (1): ReLU(inplace=True)\n",
      "              (2): Sequential(\n",
      "                (0): Conv1dAuto(128, 128, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
      "                (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              )\n",
      "              (3): ReLU(inplace=True)\n",
      "              (4): Sequential(\n",
      "                (0): Conv1dAuto(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
      "                (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              )\n",
      "            )\n",
      "            (activate): ReLU(inplace=True)\n",
      "            (shortcut): Sequential(\n",
      "              (0): Conv1d(128, 128, kernel_size=(1,), stride=(1,), bias=False)\n",
      "              (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (2): ResNet753Block(\n",
      "            (blocks): Sequential(\n",
      "              (0): Sequential(\n",
      "                (0): Conv1dAuto(128, 128, kernel_size=(7,), stride=(1,), padding=(3,), bias=False)\n",
      "                (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              )\n",
      "              (1): ReLU(inplace=True)\n",
      "              (2): Sequential(\n",
      "                (0): Conv1dAuto(128, 128, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
      "                (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              )\n",
      "              (3): ReLU(inplace=True)\n",
      "              (4): Sequential(\n",
      "                (0): Conv1dAuto(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
      "                (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              )\n",
      "            )\n",
      "            (activate): ReLU(inplace=True)\n",
      "            (shortcut): Sequential(\n",
      "              (0): Conv1d(128, 128, kernel_size=(1,), stride=(1,), bias=False)\n",
      "              (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): ResnetDecoder(\n",
      "    (avg): AdaptiveAvgPool1d(output_size=(1,))\n",
      "    (decoder): Linear(in_features=128, out_features=27, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from resnet1d import ECGResNet50\n",
    "model = ECGResNet50(750, 27)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0026, -0.0440,  0.0116, -0.0607,  0.0306, -0.0446,  0.0832,  0.0496,\n",
       "          0.0587,  0.0717, -0.0498, -0.0656,  0.0074,  0.0578,  0.0398, -0.0377,\n",
       "         -0.0816, -0.0247, -0.0144, -0.0115, -0.0459, -0.0561, -0.0869,  0.0067,\n",
       "          0.0416, -0.0080,  0.0497],\n",
       "        [-0.0026, -0.0440,  0.0116, -0.0607,  0.0306, -0.0446,  0.0832,  0.0496,\n",
       "          0.0587,  0.0717, -0.0498, -0.0656,  0.0074,  0.0578,  0.0398, -0.0377,\n",
       "         -0.0816, -0.0247, -0.0144, -0.0115, -0.0459, -0.0561, -0.0869,  0.0067,\n",
       "          0.0416, -0.0080,  0.0497],\n",
       "        [-0.0026, -0.0440,  0.0116, -0.0607,  0.0306, -0.0446,  0.0832,  0.0496,\n",
       "          0.0587,  0.0717, -0.0498, -0.0656,  0.0074,  0.0578,  0.0398, -0.0377,\n",
       "         -0.0816, -0.0247, -0.0144, -0.0115, -0.0459, -0.0561, -0.0869,  0.0067,\n",
       "          0.0416, -0.0080,  0.0497],\n",
       "        [-0.0026, -0.0440,  0.0116, -0.0607,  0.0306, -0.0446,  0.0832,  0.0496,\n",
       "          0.0587,  0.0717, -0.0498, -0.0656,  0.0074,  0.0578,  0.0398, -0.0377,\n",
       "         -0.0816, -0.0247, -0.0144, -0.0115, -0.0459, -0.0561, -0.0869,  0.0067,\n",
       "          0.0416, -0.0080,  0.0497],\n",
       "        [-0.0026, -0.0440,  0.0116, -0.0607,  0.0306, -0.0446,  0.0832,  0.0496,\n",
       "          0.0587,  0.0717, -0.0498, -0.0656,  0.0074,  0.0578,  0.0398, -0.0377,\n",
       "         -0.0816, -0.0247, -0.0144, -0.0115, -0.0459, -0.0561, -0.0869,  0.0067,\n",
       "          0.0416, -0.0080,  0.0497],\n",
       "        [-0.0026, -0.0440,  0.0116, -0.0607,  0.0306, -0.0446,  0.0832,  0.0496,\n",
       "          0.0587,  0.0717, -0.0498, -0.0656,  0.0074,  0.0578,  0.0398, -0.0377,\n",
       "         -0.0816, -0.0247, -0.0144, -0.0115, -0.0459, -0.0561, -0.0869,  0.0067,\n",
       "          0.0416, -0.0080,  0.0497],\n",
       "        [-0.0026, -0.0440,  0.0116, -0.0607,  0.0306, -0.0446,  0.0832,  0.0496,\n",
       "          0.0587,  0.0717, -0.0498, -0.0656,  0.0074,  0.0578,  0.0398, -0.0377,\n",
       "         -0.0816, -0.0247, -0.0144, -0.0115, -0.0459, -0.0561, -0.0869,  0.0067,\n",
       "          0.0416, -0.0080,  0.0497],\n",
       "        [-0.0026, -0.0440,  0.0116, -0.0607,  0.0306, -0.0446,  0.0832,  0.0496,\n",
       "          0.0587,  0.0717, -0.0498, -0.0656,  0.0074,  0.0578,  0.0398, -0.0377,\n",
       "         -0.0816, -0.0247, -0.0144, -0.0115, -0.0459, -0.0561, -0.0869,  0.0067,\n",
       "          0.0416, -0.0080,  0.0497],\n",
       "        [-0.0026, -0.0440,  0.0116, -0.0607,  0.0306, -0.0446,  0.0832,  0.0496,\n",
       "          0.0587,  0.0717, -0.0498, -0.0656,  0.0074,  0.0578,  0.0398, -0.0377,\n",
       "         -0.0816, -0.0247, -0.0144, -0.0115, -0.0459, -0.0561, -0.0869,  0.0067,\n",
       "          0.0416, -0.0080,  0.0497],\n",
       "        [-0.0026, -0.0440,  0.0116, -0.0607,  0.0306, -0.0446,  0.0832,  0.0496,\n",
       "          0.0587,  0.0717, -0.0498, -0.0656,  0.0074,  0.0578,  0.0398, -0.0377,\n",
       "         -0.0816, -0.0247, -0.0144, -0.0115, -0.0459, -0.0561, -0.0869,  0.0067,\n",
       "          0.0416, -0.0080,  0.0497]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(dummy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from torchvision import datasets, models, transforms\n",
    "from myeval import agg_y_preds_bags, binary_acc, geometry_loss\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from snippets.pytorchtools import EarlyStopping\n",
    "from sklearn.model_selection import GroupKFold\n",
    "import time\n",
    "\n",
    "st = time.time()\n",
    "patience = 50\n",
    "batch_size= 512#65000\n",
    "\n",
    "saved_dir = '../saved/resnet50/'\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "trainDataset = torch.utils.data.Subset(signal_datasets_train, range(0,len(Signals_train), 1))\n",
    "testDataset = torch.utils.data.Subset(signal_datasets_test, range(0,len(Signals_test), 1))\n",
    "\n",
    "trainLoader = torch.utils.data.DataLoader(trainDataset, batch_size=batch_size, shuffle = True, pin_memory=True)#sampler = sampler)\n",
    "testLoader = torch.utils.data.DataLoader(testDataset, batch_size=65000, shuffle = False, pin_memory=True)\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01) \n",
    "#optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9) #\n",
    "# Decay LR by a factor of 0.1 every 100 epochs\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.1)\n",
    "#scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.1, patient=10)\n",
    "\n",
    "#criterion_train = WeightedBCELoss(weights=weights_tensor, reduction='mean')\n",
    "#criterion_test = WeightedBCELoss(weights=weights_tensor, reduction='mean')\n",
    "\n",
    "class_weights_train = 1.0/np.sum(Data_labels_train, axis=0)\n",
    "class_weights_train = class_weights_train / np.sum(class_weights_train)\n",
    "class_weights_train = torch.Tensor(class_weights_train).to(device)\n",
    "criterion_train = nn.BCEWithLogitsLoss(pos_weight=pos_weight, reduction='mean') \n",
    "criterion_test = nn.BCEWithLogitsLoss(pos_weight=pos_weight, reduction='mean') \n",
    "\n",
    "losses_train = []\n",
    "losses_test = []\n",
    "\n",
    "avg_losses_train = []\n",
    "avg_losses_test = []\n",
    "\n",
    "\n",
    "early_stopping = EarlyStopping(patience, verbose=False, \n",
    "                              saved_dir=saved_dir, \n",
    "                              save_name='resnet50_'+run_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [get_name(label, Dx_map, Dx_map_unscored) for label in labels]\n",
    "\n",
    "assert len(labels) == 27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d605994f888450390b3854dcba30b92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "S41 198.40 min |\n",
      " Train Loss: 0.107318, Acc: 0.978, F: 0.255, Fbeta: 0.282, gbeta: 0.145, geo: 0.202, score: 0.435 |\n",
      " Valid Loss: 0.126813, Acc: 0.975, F: 0.201, Fbeta: 0.220, gbeta: 0.112, geo: 0.157, score: 0.392\n",
      " \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06d9372051e44132a824fee9606139c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "S42 203.23 min |\n",
      " Train Loss: 0.107189, Acc: 0.978, F: 0.258, Fbeta: 0.284, gbeta: 0.147, geo: 0.205, score: 0.445 |\n",
      " Valid Loss: 0.126679, Acc: 0.976, F: 0.204, Fbeta: 0.223, gbeta: 0.115, geo: 0.160, score: 0.383\n",
      " \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf6f1ebc6d7c41c58bba9a261165a5a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "S43 207.86 min |\n",
      " Train Loss: 0.107079, Acc: 0.978, F: 0.258, Fbeta: 0.285, gbeta: 0.147, geo: 0.205, score: 0.439 |\n",
      " Valid Loss: 0.126470, Acc: 0.976, F: 0.200, Fbeta: 0.221, gbeta: 0.112, geo: 0.157, score: 0.394\n",
      " \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de0e5d1ac4c94505be191dcf357f0864",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "S44 212.45 min |\n",
      " Train Loss: 0.106939, Acc: 0.978, F: 0.264, Fbeta: 0.293, gbeta: 0.151, geo: 0.210, score: 0.454 |\n",
      " Valid Loss: 0.126273, Acc: 0.976, F: 0.198, Fbeta: 0.222, gbeta: 0.112, geo: 0.157, score: 0.395\n",
      " \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d996a4edf1b14323817ab9153aa76ef3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "S45 217.11 min |\n",
      " Train Loss: 0.106869, Acc: 0.978, F: 0.253, Fbeta: 0.280, gbeta: 0.143, geo: 0.200, score: 0.431 |\n",
      " Valid Loss: 0.126142, Acc: 0.975, F: 0.199, Fbeta: 0.219, gbeta: 0.111, geo: 0.156, score: 0.372\n",
      " \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ae427ae720647b3a3e30ff397e5477c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "S46 221.84 min |\n",
      " Train Loss: 0.106794, Acc: 0.978, F: 0.252, Fbeta: 0.280, gbeta: 0.143, geo: 0.200, score: 0.431 |\n",
      " Valid Loss: 0.125937, Acc: 0.976, F: 0.206, Fbeta: 0.226, gbeta: 0.117, geo: 0.162, score: 0.377\n",
      " \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36d505689c7047189d6a3f15ecac8f20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "S47 226.60 min |\n",
      " Train Loss: 0.106650, Acc: 0.978, F: 0.268, Fbeta: 0.297, gbeta: 0.154, geo: 0.213, score: 0.460 |\n",
      " Valid Loss: 0.125763, Acc: 0.976, F: 0.210, Fbeta: 0.231, gbeta: 0.120, geo: 0.166, score: 0.416\n",
      " \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a22a8f89fa744832a008a5e86261ab9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "S48 231.21 min |\n",
      " Train Loss: 0.106475, Acc: 0.979, F: 0.278, Fbeta: 0.308, gbeta: 0.160, geo: 0.222, score: 0.472 |\n",
      " Valid Loss: 0.125535, Acc: 0.977, F: 0.214, Fbeta: 0.237, gbeta: 0.121, geo: 0.169, score: 0.403\n",
      " \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4778d12811547c3997c2b09aea24792",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "S49 235.83 min |\n",
      " Train Loss: 0.106298, Acc: 0.979, F: 0.279, Fbeta: 0.309, gbeta: 0.161, geo: 0.223, score: 0.474 |\n",
      " Valid Loss: 0.125348, Acc: 0.976, F: 0.210, Fbeta: 0.232, gbeta: 0.118, geo: 0.166, score: 0.384\n",
      " \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6635eace7c34d8da515260b3b6631eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "S50 240.32 min |\n",
      " Train Loss: 0.106228, Acc: 0.978, F: 0.258, Fbeta: 0.286, gbeta: 0.146, geo: 0.204, score: 0.436 |\n",
      " Valid Loss: 0.125218, Acc: 0.976, F: 0.213, Fbeta: 0.235, gbeta: 0.120, geo: 0.168, score: 0.403\n",
      " \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6034d5eeaa9d43e7b08c6af341e7730d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "S51 244.81 min |\n",
      " Train Loss: 0.106054, Acc: 0.979, F: 0.282, Fbeta: 0.313, gbeta: 0.162, geo: 0.225, score: 0.478 |\n",
      " Valid Loss: 0.125045, Acc: 0.977, F: 0.219, Fbeta: 0.241, gbeta: 0.123, geo: 0.172, score: 0.446\n",
      " \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7312a1ad696d43848272e9610203c99b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "S52 249.49 min |\n",
      " Train Loss: 0.105874, Acc: 0.979, F: 0.286, Fbeta: 0.316, gbeta: 0.165, geo: 0.228, score: 0.484 |\n",
      " Valid Loss: 0.124880, Acc: 0.977, F: 0.214, Fbeta: 0.235, gbeta: 0.121, geo: 0.169, score: 0.456\n",
      " \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1e7195edc28455fb9da7bbda6660f1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "S53 254.09 min |\n",
      " Train Loss: 0.105687, Acc: 0.979, F: 0.290, Fbeta: 0.321, gbeta: 0.167, geo: 0.232, score: 0.489 |\n",
      " Valid Loss: 0.124804, Acc: 0.970, F: 0.221, Fbeta: 0.235, gbeta: 0.119, geo: 0.167, score: 0.313\n",
      " \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60d8f9e21cc1478ca44ddb0abc1af358",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "S54 258.61 min |\n",
      " Train Loss: 0.105500, Acc: 0.979, F: 0.292, Fbeta: 0.324, gbeta: 0.169, geo: 0.234, score: 0.490 |\n",
      " Valid Loss: 0.124686, Acc: 0.976, F: 0.216, Fbeta: 0.238, gbeta: 0.122, geo: 0.170, score: 0.433\n",
      " \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39112fa0f9aa4b1ab4e35fd92ef531a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "S55 263.30 min |\n",
      " Train Loss: 0.105323, Acc: 0.979, F: 0.293, Fbeta: 0.324, gbeta: 0.168, geo: 0.234, score: 0.490 |\n",
      " Valid Loss: 0.124524, Acc: 0.977, F: 0.217, Fbeta: 0.239, gbeta: 0.124, geo: 0.172, score: 0.460\n",
      " \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6380f84fac65437b9a0c0f691456cdc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "S56 267.93 min |\n",
      " Train Loss: 0.105131, Acc: 0.980, F: 0.298, Fbeta: 0.330, gbeta: 0.172, geo: 0.238, score: 0.499 |\n",
      " Valid Loss: 0.124317, Acc: 0.977, F: 0.230, Fbeta: 0.254, gbeta: 0.131, geo: 0.183, score: 0.406\n",
      " \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9620c931efbd455db51af31f030b2a38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "S57 272.69 min |\n",
      " Train Loss: 0.104941, Acc: 0.980, F: 0.299, Fbeta: 0.331, gbeta: 0.173, geo: 0.239, score: 0.500 |\n",
      " Valid Loss: 0.124188, Acc: 0.977, F: 0.222, Fbeta: 0.244, gbeta: 0.126, geo: 0.175, score: 0.458\n",
      " \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "803cae7de2bd4d9fbc7412ddce2cfbed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "S58 277.41 min |\n",
      " Train Loss: 0.104748, Acc: 0.980, F: 0.301, Fbeta: 0.333, gbeta: 0.174, geo: 0.241, score: 0.504 |\n",
      " Valid Loss: 0.123999, Acc: 0.977, F: 0.236, Fbeta: 0.260, gbeta: 0.133, geo: 0.186, score: 0.437\n",
      " \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5cd3d1e5a864b2bb7fd09af3987e4b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "S59 282.28 min |\n",
      " Train Loss: 0.104556, Acc: 0.980, F: 0.304, Fbeta: 0.337, gbeta: 0.176, geo: 0.243, score: 0.506 |\n",
      " Valid Loss: 0.123808, Acc: 0.978, F: 0.232, Fbeta: 0.257, gbeta: 0.133, geo: 0.185, score: 0.484\n",
      " \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cf01e3501a64b3ab1a51bf95f59c4af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "S60 287.04 min |\n",
      " Train Loss: 0.104366, Acc: 0.980, F: 0.305, Fbeta: 0.339, gbeta: 0.176, geo: 0.244, score: 0.507 |\n",
      " Valid Loss: 0.123655, Acc: 0.977, F: 0.219, Fbeta: 0.240, gbeta: 0.125, geo: 0.173, score: 0.434\n",
      " \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07c81bc9f0cb473797a34a377dbde684",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "S61 291.67 min |\n",
      " Train Loss: 0.104173, Acc: 0.980, F: 0.306, Fbeta: 0.339, gbeta: 0.178, geo: 0.246, score: 0.513 |\n",
      " Valid Loss: 0.123495, Acc: 0.977, F: 0.229, Fbeta: 0.250, gbeta: 0.130, geo: 0.180, score: 0.454\n",
      " \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c1876fd9f2d4c3eb13299dbd7408521",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "S62 296.29 min |\n",
      " Train Loss: 0.103978, Acc: 0.980, F: 0.311, Fbeta: 0.345, gbeta: 0.180, geo: 0.249, score: 0.516 |\n",
      " Valid Loss: 0.123408, Acc: 0.971, F: 0.230, Fbeta: 0.244, gbeta: 0.125, geo: 0.174, score: 0.314\n",
      " \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0924c42d3d44e77a77857fdffeb4317",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "S63 300.92 min |\n",
      " Train Loss: 0.103784, Acc: 0.980, F: 0.310, Fbeta: 0.343, gbeta: 0.180, geo: 0.249, score: 0.518 |\n",
      " Valid Loss: 0.123235, Acc: 0.977, F: 0.233, Fbeta: 0.254, gbeta: 0.133, geo: 0.184, score: 0.449\n",
      " \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "761c9b50f3a546c99a7f5ae4261b8ba8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "S64 305.53 min |\n",
      " Train Loss: 0.103594, Acc: 0.980, F: 0.314, Fbeta: 0.349, gbeta: 0.182, geo: 0.252, score: 0.518 |\n",
      " Valid Loss: 0.123132, Acc: 0.977, F: 0.227, Fbeta: 0.248, gbeta: 0.129, geo: 0.179, score: 0.457\n",
      " \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6bd48abda2e470b8f452e5cca583630",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "S65 310.16 min |\n",
      " Train Loss: 0.103401, Acc: 0.980, F: 0.316, Fbeta: 0.351, gbeta: 0.184, geo: 0.254, score: 0.522 |\n",
      " Valid Loss: 0.122993, Acc: 0.977, F: 0.240, Fbeta: 0.258, gbeta: 0.136, geo: 0.187, score: 0.457\n",
      " \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92273adf54cc4c088c21b2435d9cf7f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "S66 314.76 min |\n",
      " Train Loss: 0.103209, Acc: 0.980, F: 0.317, Fbeta: 0.352, gbeta: 0.185, geo: 0.255, score: 0.525 |\n",
      " Valid Loss: 0.122876, Acc: 0.977, F: 0.223, Fbeta: 0.245, gbeta: 0.128, geo: 0.177, score: 0.470\n",
      " \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e325115e1fc4bc292a6ef859378995d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "S67 319.31 min |\n",
      " Train Loss: 0.103017, Acc: 0.980, F: 0.318, Fbeta: 0.352, gbeta: 0.186, geo: 0.256, score: 0.525 |\n",
      " Valid Loss: 0.122760, Acc: 0.977, F: 0.230, Fbeta: 0.254, gbeta: 0.131, geo: 0.183, score: 0.459\n",
      " \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5525d3667334bd4971d96a84c110bdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "S68 324.04 min |\n",
      " Train Loss: 0.102825, Acc: 0.981, F: 0.321, Fbeta: 0.357, gbeta: 0.188, geo: 0.259, score: 0.529 |\n",
      " Valid Loss: 0.122637, Acc: 0.978, F: 0.233, Fbeta: 0.257, gbeta: 0.133, geo: 0.185, score: 0.478\n",
      " \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63376ced164647a89bbe90c095347145",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "S69 328.58 min |\n",
      " Train Loss: 0.102634, Acc: 0.981, F: 0.322, Fbeta: 0.357, gbeta: 0.188, geo: 0.259, score: 0.531 |\n",
      " Valid Loss: 0.122506, Acc: 0.978, F: 0.237, Fbeta: 0.262, gbeta: 0.136, geo: 0.189, score: 0.491\n",
      " \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "277d335e784149cf8d6050dd68e14285",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "S70 333.13 min |\n",
      " Train Loss: 0.102480, Acc: 0.980, F: 0.312, Fbeta: 0.347, gbeta: 0.180, geo: 0.250, score: 0.516 |\n",
      " Valid Loss: 0.122411, Acc: 0.977, F: 0.233, Fbeta: 0.257, gbeta: 0.133, geo: 0.185, score: 0.480\n",
      " \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d437c50275e40ab840985f330e4552e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "S71 337.66 min |\n",
      " Train Loss: 0.102291, Acc: 0.981, F: 0.327, Fbeta: 0.363, gbeta: 0.191, geo: 0.263, score: 0.537 |\n",
      " Valid Loss: 0.122269, Acc: 0.978, F: 0.247, Fbeta: 0.268, gbeta: 0.141, geo: 0.194, score: 0.462\n",
      " \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "321f403947784ef8a4c8cd15475dfc2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "S72 342.17 min |\n",
      " Train Loss: 0.102238, Acc: 0.979, F: 0.285, Fbeta: 0.318, gbeta: 0.161, geo: 0.226, score: 0.466 |\n",
      " Valid Loss: 0.122158, Acc: 0.977, F: 0.238, Fbeta: 0.260, gbeta: 0.135, geo: 0.187, score: 0.464\n",
      " \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe42fae4cb9b4663bf4fa00b5497c306",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "S73 346.74 min |\n",
      " Train Loss: 0.102068, Acc: 0.981, F: 0.324, Fbeta: 0.360, gbeta: 0.189, geo: 0.261, score: 0.530 |\n",
      " Valid Loss: 0.122052, Acc: 0.978, F: 0.229, Fbeta: 0.255, gbeta: 0.132, geo: 0.184, score: 0.521\n",
      " \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9364b39acf048249d77850e4a87d7be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "S74 351.28 min |\n",
      " Train Loss: 0.101885, Acc: 0.981, F: 0.330, Fbeta: 0.366, gbeta: 0.193, geo: 0.266, score: 0.540 |\n",
      " Valid Loss: 0.121952, Acc: 0.978, F: 0.228, Fbeta: 0.250, gbeta: 0.132, geo: 0.181, score: 0.474\n",
      " \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcef704d25b047dfaba39f772d30a4b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_trains_tensor = None\n",
    "y_tests_tensor = None\n",
    "for epoch in range(41, 5000):\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    running_loss_train = 0.0\n",
    "\n",
    "    y_trains = [] # ground truth\n",
    "    output_trains = [] # output\n",
    "    for k, (X_train, y_train) in tqdm(enumerate(trainLoader)):\n",
    "        y_train = y_train.to(device)\n",
    "        X_train = X_train.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output_train = model(X_train)\n",
    "        output_trains.append(output_train.cpu())\n",
    "        \n",
    "        loss_train = criterion_train(output_train, y_train)\n",
    "        losses_train.append(loss_train.item())\n",
    "        \n",
    "        avg_loss_train = np.average(losses_train)\n",
    "        avg_losses_train.append(avg_loss_train)\n",
    "    \n",
    "        \n",
    "        if np.mod(k, 100) == 0:\n",
    "            writer.add_scalar('training loss',\n",
    "            avg_loss_train,\n",
    "            epoch * (len(Data_labels_train)//batch_size//100+1) + k//100)\n",
    "        \n",
    "        y_trains.append(y_train.cpu())\n",
    "            \n",
    "        loss_train.backward()\n",
    "        optimizer.step()\n",
    "            \n",
    "    scheduler.step()\n",
    "    \n",
    "\n",
    "    y_tests = [] # ground truth\n",
    "    output_tests = [] # output\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        \n",
    "        for X_test, y_test in testLoader:  \n",
    "            y_test = y_test.to(device)\n",
    "            X_test = X_test.to(device)\n",
    "            output_test = model(X_test)\n",
    "\n",
    "            loss_test = criterion_test(output_test, y_test)\n",
    "            losses_test.append(loss_test.item())\n",
    "\n",
    "            output_tests.append(output_test.cpu())\n",
    "            y_tests.append(y_test.cpu())\n",
    "            \n",
    "            \n",
    "            \n",
    "        avg_loss_test = np.average(losses_test)\n",
    "        avg_losses_test.append(avg_loss_test)\n",
    "        \n",
    "        #scheduler.step(avg_loss_test)\n",
    "\n",
    "        writer.add_scalar('testing loss',\n",
    "                avg_loss_test,\n",
    "                epoch)\n",
    "\n",
    "\n",
    "\n",
    "    y_trains_tensor = torch.cat(y_trains, axis=0) # ground truth\n",
    "    y_tests_tensor = torch.cat(y_tests, axis=0) # ground truth\n",
    "\n",
    "    output_trains = torch.cat(output_trains, axis=0) \n",
    "    y_train_preds = torch.sigmoid(output_trains)\n",
    "\n",
    "    output_tests = torch.cat(output_tests, axis=0)\n",
    "    y_test_preds = torch.sigmoid(output_tests)\n",
    "\n",
    "    #output_trains = torch.cat(output_trains, axis=0)\n",
    "#     y_train_preds_max, y_train_preds_mean, _ = agg_y_preds_bags(y_train_preds, bag_size=n_segments)\n",
    "#     y_test_preds_max, y_test_preds_mean, _ = agg_y_preds_bags(y_test_preds, bag_size=n_segments)\n",
    "#     _, _, y_trains = agg_y_preds_bags(y_trains, bag_size=n_segments)\n",
    "#     _, _, y_tests = agg_y_preds_bags(y_tests, bag_size=n_segments)\n",
    "\n",
    "\n",
    "    for class_idx in range(N_CLASS):\n",
    "        add_pr_curve_tensorboard(class_idx, y_trains_tensor, y_train_preds, names, global_step=epoch, prefix='train')\n",
    "        add_pr_curve_tensorboard(class_idx, y_tests_tensor, y_test_preds, names, global_step=epoch, prefix='test')\n",
    "\n",
    "\n",
    "\n",
    "    acc, fmeasure, fbeta, gbeta = binary_acc(y_train_preds, y_trains_tensor)           \n",
    "    acc2, fmeasure2, fbeta2, gbeta2 = binary_acc(y_test_preds, y_tests_tensor)\n",
    "    geometry = geometry_loss(fbeta, gbeta)\n",
    "    geometry2 = geometry_loss(fbeta2, gbeta2)\n",
    "    \n",
    "#     output_str = 'S{} {:.2f} min |\\n Train Loss: {:.6f}, Acc: {:.3f}, F: {:.3f}, Fbeta: {:.3f}, gbeta: {:.3f}, geo: {:.3f}, score: {:.3f} |\\n Valid Loss: {:.6f}, Acc: {:.3f}, F: {:.3f}, Fbeta: {:.3f}, gbeta: {:.3f}, geo: {:.3f}, score: {:.3f}\\n '.format(\n",
    "#         epoch, (time.time()-st)/60,\n",
    "#         avg_loss_train, acc, fmeasure, fbeta, gbeta, geometry, np.nan,\n",
    "#         avg_loss_test, acc2, fmeasure2, fbeta2, gbeta2, geometry2, np.nan)\n",
    "#     print(output_str)\n",
    "    score = compute_score(np.round(y_train_preds.data.numpy()), np.round(y_trains_tensor.data.numpy()), weights)\n",
    "    score2 = compute_score(np.round(y_test_preds.data.numpy()), np.round(y_tests_tensor.data.numpy()), weights)\n",
    "    output_str = 'S{} {:.2f} min |\\n Train Loss: {:.6f}, Acc: {:.3f}, F: {:.3f}, Fbeta: {:.3f}, gbeta: {:.3f}, geo: {:.3f}, score: {:.3f} |\\n Valid Loss: {:.6f}, Acc: {:.3f}, F: {:.3f}, Fbeta: {:.3f}, gbeta: {:.3f}, geo: {:.3f}, score: {:.3f}\\n '.format(\n",
    "        epoch, (time.time()-st)/60,\n",
    "        avg_loss_train, acc, fmeasure, fbeta, gbeta, geometry, score,\n",
    "        avg_loss_test, acc2, fmeasure2, fbeta2, gbeta2, geometry2, score2)\n",
    "    \n",
    "    print(output_str)\n",
    "\n",
    "    with open(saved_dir+'loss_{}.txt'.format(run_name), 'a') as f:\n",
    "        print(output_str, file=f)\n",
    "\n",
    "    early_stopping(avg_loss_test, model)\n",
    "\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping\")\n",
    "        break\n",
    "\n",
    "\n",
    "#     output_string = 'AUROC|AUPRC|Accuracy|F-measure|Fbeta-measure|Gbeta-measure|Geomotry\\n{:.3f}|{:.3f}|{:.3f}|{:.3f}|{:.3f}|{:.3f}|{:.3f}'.format(auroc2,auprc2,acc2,fmeasure2,fbeta2,gbeta2,geometry2)\n",
    "#     print(output_string)     \n",
    "#     with open(saved_dir+'score'+ str(i)+ '_epoch' + str(epoch) + '.txt', 'w') as f:\n",
    "#         f.write(output_string)\n",
    "\n",
    "#     avg_losses_train = np.array(avg_losses_train)\n",
    "#     avg_losses_test = np.array(avg_losses_test)\n",
    "\n",
    "#     np.save(saved_dir + 'avg_losses_train' + str(i) + '_epoch' + str(epoch), avg_losses_train)\n",
    "#     np.save(saved_dir + 'avg_losses_test' + str(i) + '_epoch' + str(epoch), avg_losses_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import multilabel_confusion_matrix, confusion_matrix\n",
    "from global_vars import labels\n",
    "\n",
    "cf_matrices = multilabel_confusion_matrix(y_trains_tensor.data.numpy(), np.round(y_train_preds.data.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label, cf_matrix in zip(labels, cf_matrices):\n",
    "    print(get_name(label, Dx_map, Dx_map_unscored))\n",
    "    print(cf_matrix)\n",
    "    print('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_trains_tensor.data.numpy()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 (physioNet)",
   "language": "python",
   "name": "physionet"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
