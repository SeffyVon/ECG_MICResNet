{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset  1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12ebf3c21735491094651587e36af69e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Done.\n",
      "Dataset  2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b7e2912072e44b0ba85f1a5ba131c95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Done.\n",
      "Dataset  3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3c5fbd44dfb41f3b70fd8be08d02c75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Done.\n",
      "Dataset  4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ed92d7520b74f6989c05cd483eec3c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Done.\n",
      "Dataset  5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "621bbc22a8bf4d1fb32847cead72df6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Done.\n",
      "Dataset  6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbad3733e87d4bd4b54f2159cc988543",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np, os, sys\n",
    "from tqdm.notebook import tqdm    \n",
    "from manipulations import get_classes, get_classes_from_header, get_Fs_from_header, load_challenge_data\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    Datas = []\n",
    "    Header_datas = []\n",
    "    Classes = []\n",
    "    for dataset in range(1,7):\n",
    "        print('Dataset ', dataset)\n",
    "        # Parse arguments.\n",
    "        if len(sys.argv) != 3:\n",
    "            raise Exception('Include the input and output directories as arguments, e.g., python driver.py input output.')\n",
    "\n",
    "        input_directory = '../Data/Training{}/'.format(dataset)\n",
    "        output_directory = '../Output/'\n",
    "\n",
    "        # Find files.\n",
    "        input_files = []\n",
    "        for f in os.listdir(input_directory):\n",
    "            if os.path.isfile(os.path.join(input_directory, f)) and not f.lower().startswith('.') and f.lower().endswith('mat'):\n",
    "                input_files.append(f)\n",
    "\n",
    "        if not os.path.isdir(output_directory):\n",
    "            os.mkdir(output_directory)\n",
    "\n",
    "        classes=get_classes(input_directory,input_files)\n",
    "\n",
    "        num_files = len(input_files)\n",
    "        datas = []\n",
    "        header_datas = []\n",
    "        for i, f in tqdm(enumerate(input_files)):\n",
    "            #print('    {}/{}...'.format(i+1, num_files), f)\n",
    "            tmp_input_file = os.path.join(input_directory,f)\n",
    "            data,header_data = load_challenge_data(tmp_input_file)\n",
    "            datas.append(data)\n",
    "            header_datas.append(header_data)\n",
    "\n",
    "        Datas += datas\n",
    "        Header_datas += header_datas\n",
    "        Classes += classes\n",
    "        print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from manipulations import get_abbr, get_name\n",
    "from global_vars import labels, Dx_map, Dx_map_unscored\n",
    "first_idx = {scored_code: None for scored_code in list(Dx_map['SNOMED CT Code'])}\n",
    "first_idx_unscored = {unscored_code: None for unscored_code in list(Dx_map_unscored['SNOMED CT Code'])}\n",
    "\n",
    "# for i, Header_data in tqdm(enumerate(Header_datas)):\n",
    "#     codes = get_classes_from_header(Header_data)\n",
    "#     abbrs = ' '.join([get_abbr(int(code), Dx_map, Dx_map_unscored) for code in codes])\n",
    "#     for code, abbr in zip(codes, abbrs):\n",
    "#         if code in first_idx and first_idx[code] is None:\n",
    "#             first_idx[code] = i\n",
    "#         if code in first_idx_unscored and first_idx_unscored[code] is None:\n",
    "#             first_idx_unscored[code] = i\n",
    "#     encore = False\n",
    "#     for code in first_idx.keys():\n",
    "#         if first_idx[code] is None:\n",
    "#             encore = True\n",
    "#             break\n",
    "# #     for code in first_idx_unscored.keys():\n",
    "# #         if first_idx_unscored[code] is None:\n",
    "# #             encore = True\n",
    "# #             break\n",
    "#     if not encore:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1322180e317f472194618d698f8377e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=11186.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from signal_processing import myfilter, main_QRST\n",
    "Codes = []\n",
    "Q_locs = []\n",
    "for idx in tqdm(range(0, 11186)): \n",
    "    if np.sum(Datas[idx]) != 0: # not all zeros\n",
    "        codes = get_classes_from_header(Header_datas[idx])\n",
    "        names = ', '.join([get_name(int(code), Dx_map, Dx_map_unscored) for code in codes])\n",
    "\n",
    "        filtered_Data = myfilter(Datas[idx][:,1000:4000], 500, vis=False)\n",
    "\n",
    "        # get the lead to apply Pan Tomkins\n",
    "        Q_loc = main_QRST(filtered_Data, idx, '', '', names, fig2=False)\n",
    "\n",
    "        # store\n",
    "        Codes.append(codes)\n",
    "        Q_locs.append(Q_loc)\n",
    "    else:\n",
    "        Codes.append([])\n",
    "        Q_locs.append([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from global_vars import labels\n",
    "def get_scored_class(code, labels):\n",
    "    return [1 if label in code else 0 for label in labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_labels = np.array([get_scored_class(codes, labels) for codes in Codes])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_RR = 750 # 60 beats/min => 60 beats/60 s ==> beat/1s ==> 500 samples / beat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bb9735ddd2d485fb58ace35f2dd24dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "X_trains = []\n",
    "Data_labels = []\n",
    "\n",
    "for i in tqdm(range(100)):\n",
    "    Q_loc = Q_locs[i]\n",
    "    RR_avg = np.median([Q_loc[k+1] - Q_loc[k] for k in range(len(Q_loc)-1)])\n",
    "    RR_th = (0.3 * RR_avg, 3 * RR_avg)\n",
    "    \n",
    "    ks = [k for k in range(len(Q_loc)-1) if Q_loc[k+1] - Q_loc[k] > RR_th[0] \n",
    "              and Q_loc[k+1] - Q_loc[k] < RR_th[1]]\n",
    "        \n",
    "    for k in ks:\n",
    "        Data_labels.append(get_scored_class(Codes[i], labels))\n",
    "        X_trains.append(Datas[i][:,1000+Q_loc[k]:1000+Q_loc[k+1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Signals = np.zeros((len(X_trains),12,MAX_RR))\n",
    "for i in range(len(X_trains)):\n",
    "    Signals[i,:,:min(len(X_trains[i][0]),MAX_RR)] = X_trains[i][:,:MAX_RR]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# On y va"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torchvision import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation_func(activation):\n",
    "    return  nn.ModuleDict([\n",
    "        ['relu', nn.ReLU(inplace=True)],\n",
    "        ['leaky_relu', nn.LeakyReLU(negative_slope=0.01, inplace=True)],\n",
    "        ['selu', nn.SELU(inplace=True)],\n",
    "        ['none', nn.Identity()]\n",
    "    ])[activation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "class Conv1dAuto(nn.Conv1d):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.padding =  (self.kernel_size[0] // 2,) # dynamic add padding based on the kernel_size\n",
    "       # print(self.kernel_size, self.padding)\n",
    "        \n",
    "conv3 = partial(Conv1dAuto, kernel_size=3, bias=False)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv1dAuto(64, 32, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv3(64,32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, activation='relu'):\n",
    "        super().__init__()\n",
    "        self.in_channels, self.out_channels, self.activation = in_channels, out_channels, activation\n",
    "        self.blocks = nn.Identity()\n",
    "        self.activate = activation_func(activation)\n",
    "        self.shortcut = nn.Identity()   \n",
    "    \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        if self.should_apply_shortcut: residual = self.shortcut(x)\n",
    "        x = self.blocks(x)\n",
    "        x += residual\n",
    "        x = self.activate(x)\n",
    "        return x\n",
    "    \n",
    "    @property\n",
    "    def should_apply_shortcut(self):\n",
    "        return self.in_channels != self.out_channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResidualBlock(\n",
       "  (blocks): Identity()\n",
       "  (activate): ReLU(inplace=True)\n",
       "  (shortcut): Identity()\n",
       ")"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ResidualBlock(32, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[2.]]])"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy = torch.ones((1, 1, 1))\n",
    "\n",
    "block = ResidualBlock(1, 64)\n",
    "block(dummy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetResidualBlock(ResidualBlock):\n",
    "    def __init__(self, in_channels, out_channels, expansion=1, downsampling=1, conv=conv3, *args, **kwargs):\n",
    "        super().__init__(in_channels, out_channels, *args, **kwargs)\n",
    "        self.expansion, self.downsampling, self.conv = expansion, downsampling, conv\n",
    "        self.shortcut = nn.Sequential(\n",
    "            nn.Conv1d(self.in_channels, self.expanded_channels, kernel_size=1,\n",
    "                      stride=self.downsampling, bias=False),\n",
    "            nn.BatchNorm1d(self.expanded_channels)) if self.should_apply_shortcut else None\n",
    "        \n",
    "        \n",
    "    @property\n",
    "    def expanded_channels(self):\n",
    "        return self.out_channels * self.expansion\n",
    "    \n",
    "    @property\n",
    "    def should_apply_shortcut(self):\n",
    "        return self.in_channels != self.expanded_channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "block = ResNetResidualBlock(32, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_bn(in_channels, out_channels, conv, *args, **kwargs):\n",
    "    return nn.Sequential(conv(in_channels, out_channels, *args, **kwargs), nn.BatchNorm1d(out_channels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetBasicBlock(ResNetResidualBlock):\n",
    "    \"\"\"\n",
    "    Basic ResNet block composed by two layers of 3conv/batchnorm/activation\n",
    "    \"\"\"\n",
    "    expansion = 1\n",
    "    def __init__(self, in_channels, out_channels, *args, **kwargs):\n",
    "        super().__init__(in_channels, out_channels, *args, **kwargs)\n",
    "        self.blocks = nn.Sequential(\n",
    "            conv_bn(self.in_channels, self.out_channels, conv=self.conv, bias=False, stride=self.downsampling),\n",
    "            activation_func(self.activation),\n",
    "            conv_bn(self.out_channels, self.expanded_channels, conv=self.conv, bias=False),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64, 224])"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy = torch.ones((1, 32, 224))# in channel 1, out channel 32, length 224 (in 2d there is another dim)\n",
    "\n",
    "block = ResNetBasicBlock(32, 64)\n",
    "block(dummy).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetBottleNeckBlock(ResNetResidualBlock):\n",
    "    expansion = 4\n",
    "    def __init__(self, in_channels, out_channels, *args, **kwargs):\n",
    "        super().__init__(in_channels, out_channels, expansion=4, *args, **kwargs)\n",
    "        self.blocks = nn.Sequential(\n",
    "           conv_bn(self.in_channels, self.out_channels, self.conv, kernel_size=1),\n",
    "             activation_func(self.activation),\n",
    "             conv_bn(self.out_channels, self.out_channels, self.conv, kernel_size=3, stride=self.downsampling),\n",
    "             activation_func(self.activation),\n",
    "             conv_bn(self.out_channels, self.expanded_channels, self.conv, kernel_size=1),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNetBottleNeckBlock(\n",
      "  (blocks): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): Conv1dAuto(32, 64, kernel_size=(1,), stride=(1,), bias=False)\n",
      "      (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Sequential(\n",
      "      (0): Conv1dAuto(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
      "      (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): Sequential(\n",
      "      (0): Conv1dAuto(64, 256, kernel_size=(1,), stride=(1,), bias=False)\n",
      "      (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (activate): ReLU(inplace=True)\n",
      "  (shortcut): Sequential(\n",
      "    (0): Conv1d(32, 256, kernel_size=(1,), stride=(1,), bias=False)\n",
      "    (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "dummy = torch.ones((1, 32, 10))\n",
    "\n",
    "block = ResNetBottleNeckBlock(32, 64)\n",
    "block(dummy).shape\n",
    "print(block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    A ResNet layer composed by `n` blocks stacked one after the other\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, block=ResNetBasicBlock, n=1, *args, **kwargs):\n",
    "        super().__init__()\n",
    "        # 'We perform downsampling directly by convolutional layers that have a stride of 2.'\n",
    "        downsampling = 2 if in_channels != out_channels else 1\n",
    "        self.blocks = nn.Sequential(\n",
    "            block(in_channels , out_channels, *args, **kwargs, downsampling=downsampling),\n",
    "            *[block(out_channels * block.expansion, out_channels, downsampling=1, *args, **kwargs) for _ in range(n - 1)]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.blocks(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128, 24])"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "dummy = torch.ones((1, 64, 48))\n",
    "\n",
    "layer = ResNetLayer(64, 128, block=ResNetBasicBlock, n=3)\n",
    "layer(dummy).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    ResNet encoder composed by layers with increasing features.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels=3, blocks_sizes=[64, 128, 256, 512], deepths=[2,2,2,2], \n",
    "                 activation='relu', block=ResNetBasicBlock, *args, **kwargs):\n",
    "        super().__init__()\n",
    "        self.blocks_sizes = blocks_sizes\n",
    "        \n",
    "        self.gate = nn.Sequential(\n",
    "            nn.Conv1d(in_channels, self.blocks_sizes[0], kernel_size=7, stride=2, padding=3, bias=False),\n",
    "            nn.BatchNorm1d(self.blocks_sizes[0]),\n",
    "            activation_func(activation),\n",
    "            nn.MaxPool1d(kernel_size=3, stride=2, padding=1)\n",
    "        )\n",
    "        \n",
    "        self.in_out_block_sizes = list(zip(blocks_sizes, blocks_sizes[1:]))\n",
    "        self.blocks = nn.ModuleList([ \n",
    "            ResNetLayer(blocks_sizes[0], blocks_sizes[0], n=deepths[0], activation=activation, \n",
    "                        block=block,*args, **kwargs),\n",
    "            *[ResNetLayer(in_channels * block.expansion, \n",
    "                          out_channels, n=n, activation=activation, \n",
    "                          block=block, *args, **kwargs) \n",
    "              for (in_channels, out_channels), n in zip(self.in_out_block_sizes, deepths[1:])]       \n",
    "        ])\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.gate(x)\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        return x\n",
    "\n",
    "class ResnetDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    This class represents the tail of ResNet. It performs a global pooling and maps the output to the\n",
    "    correct class by using a fully connected layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, n_classes):\n",
    "        super().__init__()\n",
    "        self.avg = nn.AdaptiveAvgPool1d((1,))\n",
    "        self.decoder = nn.Linear(in_features, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.avg(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "    \n",
    "class ResNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels, n_classes, *args, **kwargs):\n",
    "        super().__init__()\n",
    "        self.encoder = ResNetEncoder(in_channels, *args, **kwargs)\n",
    "        self.decoder = ResnetDecoder(self.encoder.blocks[-1].blocks[-1].expanded_channels, n_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resnet18(in_channels, n_classes, block=ResNetBasicBlock, *args, **kwargs):\n",
    "    return ResNet(in_channels, n_classes, block=block, deepths=[2, 2, 2, 2], *args, **kwargs)\n",
    "\n",
    "resnet = resnet18(64, len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3010, -0.4552, -0.8846, -0.9471,  0.0468,  0.0338,  0.4211, -0.6411,\n",
       "          0.1713, -0.8693,  0.2292,  0.1981,  0.1014, -0.1080,  0.6074, -0.6452,\n",
       "         -0.2831,  0.2506,  0.9028, -0.5637,  0.4700, -0.7781,  1.5727, -0.7719,\n",
       "         -0.0652, -0.3378,  0.8847]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet(dummy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# default `log_dir` is \"runs\" - we'll be more specific here\n",
    "writer = SummaryWriter('runs/resnet1d')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phew.. let's do some toy examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:0')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from resnet1d import resnet18\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from global_vars import weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def weighted_binary_cross_entropy2(sigmoid_x, y, weighted_matrix, weight=None, reduction=None):\n",
    "    \"\"\"\n",
    "    Aha this is correct!\n",
    "    sigmoid_x = nn.Sigmoid()(x)\n",
    "    Args:\n",
    "        sigmoid_x: predicted probability of size [N,C], N sample and C Class. Eg. Must be in range of [0,1]\n",
    "        targets: true value, one-hot-like vector of size [N,C]\n",
    "        pos_weight: Weight for postive sample\n",
    "    \"\"\"\n",
    "    if not (y.size() == sigmoid_x.size()):\n",
    "        raise ValueError(\"Target size ({}) must be the same as input size ({})\".format(y.size(), sigmoid_x.size()))\n",
    "   \n",
    "    #print(\"y.size(), sigmoid_x.size()\", y.size(), sigmoid_x.size())\n",
    "    sigmoid_x = torch.clamp(sigmoid_x,min=1e-7,max=1-1e-7) \n",
    "    loss = - torch.matmul(y*sigmoid_x.log() + (1-y)*(1-sigmoid_x).log(), weighted_matrix)\n",
    "    \n",
    "    if weight is not None:\n",
    "        loss = loss * weight\n",
    "        \n",
    "    if reduction is None:\n",
    "        return loss\n",
    "    elif reduction == 'mean':\n",
    "        return loss.mean()\n",
    "    elif reduction == 'sum':\n",
    "        return loss.sum()\n",
    "    return None\n",
    "    \n",
    "class WeightedBCELoss(nn.Module):\n",
    "    def __init__(self, weights, PosWeightIsDynamic= False, WeightIsDynamic= False, \n",
    "                 reduction='mean'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            pos_weight = Weight for postive samples. Size [1,C]\n",
    "            weight = Weight for Each class. Size [1,C]\n",
    "            PosWeightIsDynamic: If True, the pos_weight is computed on each batch. If pos_weight is None, then it remains None.\n",
    "            WeightIsDynamic: If True, the weight is computed on each batch. If weight is None, then it remains None.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.register_buffer('weights', weights)\n",
    "        self.reduction = reduction\n",
    "        self.PosWeightIsDynamic = PosWeightIsDynamic\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        if self.PosWeightIsDynamic:\n",
    "            positive_counts = target.sum(dim=0)\n",
    "            nBatch = len(target)\n",
    "            self.pos_weight = (nBatch - positive_counts)/(positive_counts +1e-5)\n",
    "\n",
    "\n",
    "        return weighted_binary_cross_entropy2(input, target,\n",
    "                                             weighted_matrix=self.weights,\n",
    "                                             reduction=self.reduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "Signal_12leads = np.transpose(Signals, (1,0,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from global_vars import labels\n",
    "import os\n",
    "class SignalDataset(Dataset):\n",
    "\n",
    "    def __init__(self, signals, labels):\n",
    "        self.signals = signals\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.signals)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        sample =(torch.Tensor(np.array([self.signals[idx]]).transpose()), torch.Tensor(self.labels[idx]))\n",
    "\n",
    "        return sample\n",
    "    \n",
    "signal_datasets = SignalDataset(Signal_12leads[0,:,:], Data_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([27])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "signal_datasets[0][1].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (encoder): ResNetEncoder(\n",
       "    (gate): Sequential(\n",
       "      (0): Conv1d(750, 64, kernel_size=(7,), stride=(2,), padding=(3,), bias=False)\n",
       "      (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): MaxPool1d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (blocks): ModuleList(\n",
       "      (0): ResNetLayer(\n",
       "        (blocks): Sequential(\n",
       "          (0): ResNetBasicBlock(\n",
       "            (blocks): Sequential(\n",
       "              (0): Sequential(\n",
       "                (0): Conv1dAuto(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "                (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Sequential(\n",
       "                (0): Conv1dAuto(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "                (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "            (activate): ReLU(inplace=True)\n",
       "            (shortcut): None\n",
       "          )\n",
       "          (1): ResNetBasicBlock(\n",
       "            (blocks): Sequential(\n",
       "              (0): Sequential(\n",
       "                (0): Conv1dAuto(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "                (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Sequential(\n",
       "                (0): Conv1dAuto(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "                (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "            (activate): ReLU(inplace=True)\n",
       "            (shortcut): None\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): ResNetLayer(\n",
       "        (blocks): Sequential(\n",
       "          (0): ResNetBasicBlock(\n",
       "            (blocks): Sequential(\n",
       "              (0): Sequential(\n",
       "                (0): Conv1dAuto(64, 128, kernel_size=(3,), stride=(2,), padding=(1,), bias=False)\n",
       "                (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Sequential(\n",
       "                (0): Conv1dAuto(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "                (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "            (activate): ReLU(inplace=True)\n",
       "            (shortcut): Sequential(\n",
       "              (0): Conv1d(64, 128, kernel_size=(1,), stride=(2,), bias=False)\n",
       "              (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (1): ResNetBasicBlock(\n",
       "            (blocks): Sequential(\n",
       "              (0): Sequential(\n",
       "                (0): Conv1dAuto(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "                (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Sequential(\n",
       "                (0): Conv1dAuto(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "                (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "            (activate): ReLU(inplace=True)\n",
       "            (shortcut): None\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): ResNetLayer(\n",
       "        (blocks): Sequential(\n",
       "          (0): ResNetBasicBlock(\n",
       "            (blocks): Sequential(\n",
       "              (0): Sequential(\n",
       "                (0): Conv1dAuto(128, 256, kernel_size=(3,), stride=(2,), padding=(1,), bias=False)\n",
       "                (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Sequential(\n",
       "                (0): Conv1dAuto(256, 256, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "                (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "            (activate): ReLU(inplace=True)\n",
       "            (shortcut): Sequential(\n",
       "              (0): Conv1d(128, 256, kernel_size=(1,), stride=(2,), bias=False)\n",
       "              (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (1): ResNetBasicBlock(\n",
       "            (blocks): Sequential(\n",
       "              (0): Sequential(\n",
       "                (0): Conv1dAuto(256, 256, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "                (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Sequential(\n",
       "                (0): Conv1dAuto(256, 256, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "                (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "            (activate): ReLU(inplace=True)\n",
       "            (shortcut): None\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): ResNetLayer(\n",
       "        (blocks): Sequential(\n",
       "          (0): ResNetBasicBlock(\n",
       "            (blocks): Sequential(\n",
       "              (0): Sequential(\n",
       "                (0): Conv1dAuto(256, 512, kernel_size=(3,), stride=(2,), padding=(1,), bias=False)\n",
       "                (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Sequential(\n",
       "                (0): Conv1dAuto(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "                (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "            (activate): ReLU(inplace=True)\n",
       "            (shortcut): Sequential(\n",
       "              (0): Conv1d(256, 512, kernel_size=(1,), stride=(2,), bias=False)\n",
       "              (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (1): ResNetBasicBlock(\n",
       "            (blocks): Sequential(\n",
       "              (0): Sequential(\n",
       "                (0): Conv1dAuto(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "                (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Sequential(\n",
       "                (0): Conv1dAuto(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "                (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "            (activate): ReLU(inplace=True)\n",
       "            (shortcut): None\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): ResnetDecoder(\n",
       "    (avg): AdaptiveAvgPool1d(output_size=(1,))\n",
       "    (decoder): Linear(in_features=512, out_features=27, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = resnet18(MAX_RR, len(labels))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = torch.Tensor(weights).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, models, transforms\n",
    "from myeval import agg_y_preds_bags, binary_acc, geometry_loss\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from snippets.pytorchtools import EarlyStopping\n",
    "from sklearn.model_selection import KFold\n",
    "import time\n",
    "\n",
    "st = time.time()\n",
    "patience = 50\n",
    "kf = KFold(5)\n",
    "batch_size=10000\n",
    "\n",
    "saved_dir = '../saved/ResNet1d/'\n",
    "X = Signal_12leads[0,:,:]\n",
    "y = Data_labels\n",
    "\n",
    "for i, (train_idx, test_idx) in enumerate(kf.split(X, y)):\n",
    "    \n",
    "    trainDataset = torch.utils.data.Subset(signal_datasets, train_idx)\n",
    "    testDataset = torch.utils.data.Subset(signal_datasets, test_idx)\n",
    "    \n",
    "    trainLoader = torch.utils.data.DataLoader(trainDataset, batch_size=batch_size, shuffle = True, pin_memory=True)#sampler = sampler)\n",
    "    trainLoader2 = torch.utils.data.DataLoader(trainDataset, batch_size=batch_size, shuffle = False, pin_memory=True)#sampler = sampler)\n",
    "    testLoader = torch.utils.data.DataLoader(testDataset, batch_size = batch_size, shuffle = False, pin_memory=True)\n",
    "\n",
    "    \n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9) #\n",
    "    # Decay LR by a factor of 0.1 every 100 epochs\n",
    "    scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "    \n",
    "    criterion_train = WeightedBCELoss(weights=weights, reduction='mean')\n",
    "    criterion_test = WeightedBCELoss(weights=weights, reduction='mean')\n",
    "\n",
    "    losses_train = []\n",
    "    losses_test = []\n",
    "\n",
    "    avg_losses_train = []\n",
    "    avg_losses_test = []\n",
    "\n",
    "\n",
    "    early_stopping = EarlyStopping(patience, verbose=False, \n",
    "                                  saved_dir=saved_dir, \n",
    "                                   save_name='MutliCWTNetFull11'+str(i))\n",
    "    epoch = 0\n",
    "    auroc = 0\n",
    "    auprc = 0\n",
    "    accuracy = 0\n",
    "    fmeasure = 0\n",
    "    fbeta = 0\n",
    "    gbeta = 0\n",
    "    for epoch in range(50000):\n",
    "        \n",
    "        model.train()\n",
    "        output_trains = []\n",
    "        \n",
    "        for X_train, y_train in trainLoader:\n",
    "            y_train = y_train.to(device)\n",
    "            X_train = X_train.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output_train = model(X_train)\n",
    "            loss_train = criterion_train(output_train, y_train)\n",
    "            #print(loss_train)\n",
    "            losses_train.append(loss_train.item())\n",
    "            loss_train.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        scheduler.step()\n",
    "        \n",
    "        avg_loss_train = np.average(losses_train)\n",
    "        avg_losses_train.append(avg_loss_train)\n",
    "        \n",
    "        output_tests = []\n",
    "        y_tests = []\n",
    "        y_trains = []\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "\n",
    "            for X_train, y_train in trainLoader2:  \n",
    "                X_train = X_train.to(device)\n",
    "                output_train = model(X_train)\n",
    "                output_trains.append(output_train.cpu())\n",
    "                y_trains.append(y_train.cpu())\n",
    "    \n",
    "                output_trains.append(output_train.cpu())\n",
    "                y_trains.append(y_train)\n",
    "                \n",
    "            for X_test, y_test in testLoader:  \n",
    "                y_test = y_test.to(device)\n",
    "                X_test = X_test.to(device)\n",
    "                output_test = model(X_test)\n",
    "                \n",
    "                loss_test = criterion_test(output_test, y_test)\n",
    "                losses_test.append(loss_test.item())\n",
    "                \n",
    "                output_tests.append(output_test.cpu())\n",
    "                y_tests.append(y_test.cpu())\n",
    "                \n",
    "            avg_loss_test = np.average(losses_test)\n",
    "            avg_losses_test.append(avg_loss_test)\n",
    "        \n",
    "        y_trains = torch.cat(y_trains, axis=0)\n",
    "        y_tests = torch.cat(y_tests, axis=0)\n",
    "    \n",
    "        output_trains = torch.cat(output_trains, axis=0)\n",
    "        y_train_preds = torch.sigmoid(output_trains)\n",
    "        \n",
    "        output_tests = torch.cat(output_tests, axis=0)\n",
    "        y_test_preds = torch.sigmoid(output_tests)\n",
    "        \n",
    "        #output_trains = torch.cat(output_trains, axis=0)\n",
    "#         y_train_preds_max, y_train_preds_mean, _ = agg_y_preds_bags(y_train_preds, bag_size=n_segments)\n",
    "#         y_test_preds_max, y_test_preds_mean, _ = agg_y_preds_bags(y_test_preds, bag_size=n_segments)\n",
    "#         _, _, y_trains = agg_y_preds_bags(y_trains, bag_size=n_segments)\n",
    "#         _, _, y_tests = agg_y_preds_bags(y_tests, bag_size=n_segments)\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        if epoch % 500 == 0:\n",
    "            \n",
    "            acc, fmeasure, fbeta, gbeta = binary_acc(y_train_preds, y_trains)           \n",
    "            acc2, fmeasure2, fbeta2, gbeta2 = binary_acc(y_test_preds, y_tests)\n",
    "            geometry = geometry_loss(fbeta, gbeta)\n",
    "            geometry2 = geometry_loss(fbeta2, gbeta2)\n",
    "            output_str = 'S{}/{} {:.2f} min |\\n Train Loss: {:.6f}, Acc: {:.3f}, F: {:.3f}, Fbeta: {:.3f}, gbeta: {:.3f}, geo: {:.3f} |\\n Valid Loss: {:.6f}, Acc: {:.3f}, F: {:.3f}, Fbeta: {:.3f}, gbeta: {:.3f}, geo: {:.3f}\\n '.format(\n",
    "                i, epoch, (time.time()-st)/60,\n",
    "                avg_loss_train, acc, fmeasure, fbeta, gbeta, geometry,\n",
    "                avg_loss_test, acc2, fmeasure2, fbeta2, gbeta2, geometry2)\n",
    "            print(output_str)\n",
    "        \n",
    "#             output_str = 'S{}/{} {:.2f} min Train Loss: {:.6f} Valid Loss: {:.6f}'.format(i, epoch, (time.time()-st)/60, avg_loss_train, avg_loss_test)\n",
    "#             print(output_str)\n",
    "#             with open(saved_dir+'loss11_{}.txt'.format(i), 'a') as f:\n",
    "#                 print(output_str, file=f)\n",
    "\n",
    "#             early_stopping(avg_loss_test, model)\n",
    "\n",
    "            if early_stopping.early_stop:\n",
    "                print(\"Early stopping\")\n",
    "                break\n",
    "            \n",
    "#     output_string = 'AUROC|AUPRC|Accuracy|F-measure|Fbeta-measure|Gbeta-measure|Geomotry\\n{:.3f}|{:.3f}|{:.3f}|{:.3f}|{:.3f}|{:.3f}|{:.3f}'.format(auroc2,auprc2,acc2,fmeasure2,fbeta2,gbeta2,geometry2)\n",
    "#     print(output_string)     \n",
    "#     with open(saved_dir+'score'+ str(i)+ '_epoch' + str(epoch) + '.txt', 'w') as f:\n",
    "#         f.write(output_string)\n",
    "\n",
    "#     avg_losses_train = np.array(avg_losses_train)\n",
    "#     avg_losses_test = np.array(avg_losses_test)\n",
    "    \n",
    "#     np.save(saved_dir + 'avg_losses_train' + str(i) + '_epoch' + str(epoch), avg_losses_train)\n",
    "#     np.save(saved_dir + 'avg_losses_test' + str(i) + '_epoch' + str(epoch), avg_losses_test)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 (physioNet)",
   "language": "python",
   "name": "physionet"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
