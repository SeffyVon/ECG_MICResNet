{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np, os, sys\n",
    "from tqdm.notebook import tqdm    \n",
    "from manipulations import get_classes, get_classes_from_header, get_Fs_from_header, load_challenge_data\n",
    "from sklearn.model_selection import KFold, StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_CLASS = 27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_RR = 500 # 60 beats/min => 60 beats/60 s ==> beat/1s ==> 500 samples / beat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from manipulations import get_abbr, get_name\n",
    "from global_vars import labels, Dx_map, Dx_map_unscored\n",
    "first_idx = {scored_code: None for scored_code in list(Dx_map['SNOMED CT Code'])}\n",
    "first_idx_unscored = {unscored_code: None for unscored_code in list(Dx_map_unscored['SNOMED CT Code'])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from global_vars import labels, normal_class, equivalent_mapping\n",
    "normal_idx = np.argwhere(labels==int(normal_class))\n",
    "def get_scored_class(code, labels):\n",
    "    return [1 if label in code else 0 for label in labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset  1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0b2c98ddf804b28961ef5894a11159d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-6fe3ae238910>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;31m#print('    {}/{}...'.format(i+1, num_files), f)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0mtmp_input_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_directory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mheader_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_challenge_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp_input_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0mcodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_classes_from_header\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheader_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Project/CinC2020/source/manipulations.py\u001b[0m in \u001b[0;36mload_challenge_data\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloadmat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/physioNet/lib/python3.6/site-packages/scipy/io/matlab/mio.py\u001b[0m in \u001b[0;36mloadmat\u001b[0;34m(file_name, mdict, appendmat, **kwargs)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0mvariable_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'variable_names'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mappendmat\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m         \u001b[0mMR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmat_reader_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0mmatfile_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMR\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariable_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/physioNet/lib/python3.6/site-packages/scipy/io/matlab/mio.py\u001b[0m in \u001b[0;36mmat_reader_factory\u001b[0;34m(file_name, appendmat, **kwargs)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \"\"\"\n\u001b[1;32m     71\u001b[0m     \u001b[0mbyte_stream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_opened\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mappendmat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m     \u001b[0mmjv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmnv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_matfile_version\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbyte_stream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmjv\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mMatFile4Reader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbyte_stream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_opened\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/physioNet/lib/python3.6/site-packages/scipy/io/matlab/miobase.py\u001b[0m in \u001b[0;36mget_matfile_version\u001b[0;34m(fileobj)\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[0;31m# Mat4 files have a zero somewhere in first 4 bytes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m     \u001b[0mfileobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseek\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m     \u001b[0mmopt_bytes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfileobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmopt_bytes\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mMatReadError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Mat file appears to be empty\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    Datas = []\n",
    "    Header_datas = []\n",
    "    Classes = []\n",
    "    Codes = []\n",
    "    \n",
    "    dataset_idx = {}\n",
    "    dataset_data_labels = {}\n",
    "    dataset_train_idx = {}\n",
    "    dataset_test_idx = {}\n",
    "    \n",
    "    global_idx = 0\n",
    "    datasets = [1,2,3,4,5,6]\n",
    "    for dataset in datasets:\n",
    "        print('Dataset ', dataset)\n",
    "        # Parse arguments.\n",
    "        if len(sys.argv) != 3:\n",
    "            raise Exception('Include the input and output directories as arguments, e.g., python driver.py input output.')\n",
    "\n",
    "        input_directory = '../NewData/{}/'.format(dataset)\n",
    "        output_directory = '../Output/'\n",
    "\n",
    "        # Find files.\n",
    "        input_files = []\n",
    "        for f in os.listdir(input_directory):\n",
    "            if os.path.isfile(os.path.join(input_directory, f)) and not f.lower().startswith('.') and f.lower().endswith('mat'):\n",
    "                input_files.append(f)\n",
    "\n",
    "        if not os.path.isdir(output_directory):\n",
    "            os.mkdir(output_directory)\n",
    "\n",
    "        classes=get_classes(input_directory,input_files)\n",
    "\n",
    "        num_files = len(input_files)\n",
    "        datas = []\n",
    "        header_datas = []\n",
    "        dataset_data_labels[dataset] = []\n",
    "        dataset_idx[dataset] = []\n",
    "        for i, f in tqdm(enumerate(input_files)):\n",
    "            #print('    {}/{}...'.format(i+1, num_files), f)\n",
    "            tmp_input_file = os.path.join(input_directory,f)\n",
    "            data,header_data = load_challenge_data(tmp_input_file)\n",
    "            \n",
    "            codes = get_classes_from_header(header_data)\n",
    "            data_labels = get_scored_class(codes, labels)\n",
    "            Codes.append(codes)\n",
    "            \n",
    "            datas.append(data[:,1000:7000])\n",
    "            header_datas.append(header_data)\n",
    "            dataset_data_labels[dataset].append(data_labels)\n",
    "            dataset_idx[dataset].append(global_idx)\n",
    "            global_idx += 1\n",
    "\n",
    "        Datas += datas\n",
    "        Header_datas += header_datas\n",
    "        Classes += classes\n",
    "        \n",
    "        kf = MultilabelStratifiedKFold(5, random_state=0)\n",
    "        train_idx, test_idx = next(kf.split(datas, np.array(dataset_data_labels[dataset])))\n",
    "\n",
    "\n",
    "        dataset_train_idx[dataset] = train_idx +  dataset_idx[dataset][0]\n",
    "        dataset_test_idx[dataset] = test_idx + dataset_idx[dataset][0]\n",
    "        \n",
    "        \n",
    "        print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datasets = [1,2,3,4,5,6]\n",
    "# all_train_idx2 = []\n",
    "# for dataset in datasets:\n",
    "#     all_train_idx2.extend(dataset_train_idx[dataset])\n",
    "    \n",
    "# all_test_idx2= []\n",
    "# for dataset in datasets:\n",
    "#     all_test_idx2.extend(dataset_test_idx[dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('../saved/all_test_idx_stratified.pkl', 'wb') as all_test_idx_file:\n",
    "#     pickle.dump(all_test_idx, all_test_idx_file)\n",
    "# with open('../saved/all_train_idx_stratified.pkl', 'wb') as all_train_idx_file:\n",
    "#     pickle.dump(all_train_idx, all_train_idx_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from signal_processing import myfilter, main_QRST\n",
    "# Codes = []\n",
    "# Q_locs = []\n",
    "# idxes = []\n",
    "# for idx in tqdm(range(0, len(Datas))): \n",
    "#     codes = get_classes_from_header(Header_datas[idx])\n",
    "#     names = ', '.join([get_name(int(code), Dx_map, Dx_map_unscored) for code in codes])\n",
    "\n",
    "#     filtered_Data = myfilter(Datas[idx], 500, vis=False)\n",
    "\n",
    "#     # get the lead to apply Pan Tomkins\n",
    "#     Q_loc = main_QRST(filtered_Data, idx, '', '', names, fig2=False)\n",
    "\n",
    "#     # store\n",
    "#     Codes.append(codes)\n",
    "#     Q_locs.append(Q_loc)\n",
    "\n",
    "#     idxes.append(idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from signal_processing import myfilter, extract_QRST, main_QRST\n",
    "# idx = 1511#37581#2932#6884 #6884 #6951 #227#8074 #325#158#325#34#17#8659#4#18#61#10927#6886# 8659\n",
    "# codes = get_classes_from_header(Header_datas[idx])\n",
    "# names = ', '.join([get_name(int(code), Dx_map, Dx_map_unscored) for code in codes])\n",
    "# filtered_Data = myfilter(Datas[idx], 500, vis=False)\n",
    "# #filtered_Data = myfilter(Datas[idx][:,-4000:-1000], 500, vis=False)\n",
    "# Q_loc = main_QRST(filtered_Data, idx, '', '', names, fig2=False, verbose=True, vis=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# with open('../saved/newData_idxes_1000_7000_peakdist100.pkl', 'wb') as idxes_file:\n",
    "#     pickle.dump(idxes, idxes_file)\n",
    "\n",
    "# with open('../saved/newData_Codes_1000_7000_peakdist100.pkl', 'wb') as Codes_file:\n",
    "#     pickle.dump(Codes, Codes_file)\n",
    "\n",
    "# with open('../saved/newData_Q_locs_1000_7000_peakdist100.pkl', 'wb') as Q_locs_file:\n",
    "#     pickle.dump(Q_locs, Q_locs_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "Q_locs = None\n",
    "with open('../saved/newData_Q_locs_1000_7000_peakdist100.pkl', 'rb') as Q_locs_file:\n",
    "    Q_locs = pickle.load(Q_locs_file)\n",
    "\n",
    "Codes = None\n",
    "with open('../saved/newData_Codes_1000_7000_peakdist100.pkl', 'rb') as Codes_file:\n",
    "    Codes = pickle.load(Codes_file)\n",
    "\n",
    "idxes = None\n",
    "with open('../saved/newData_idxes_1000_7000_peakdist100.pkl', 'rb') as idxes_file:\n",
    "    idxes = pickle.load(idxes_file)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('../saved/all_train_idx_stratified.pkl', 'rb') as all_train_idx_file:\n",
    "    all_train_idx = pickle.load(all_train_idx_file)\n",
    "with open('../saved/all_test_idx_stratified.pkl', 'rb') as all_test_idx_file:\n",
    "    all_test_idx = pickle.load(all_test_idx_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43101"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(43101, 27)\n"
     ]
    }
   ],
   "source": [
    "data_labels = np.array([get_scored_class(codes, labels) for codes in Codes])\n",
    "print(data_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "713427006\n",
      "284470004\n",
      "427172004\n"
     ]
    }
   ],
   "source": [
    "key_idxes = []\n",
    "for key in equivalent_mapping.keys():\n",
    "    print(key)\n",
    "    key_idx = np.argwhere(labels==int(key)).flatten()[0]\n",
    "    key_idxes.append(key_idx)\n",
    "    val_idx = np.argwhere(labels==int(equivalent_mapping[key])).flatten()[0]\n",
    "    key_pos = np.argwhere(data_labels[:,key_idx]==1).flatten()\n",
    "    val_pos = np.argwhere(data_labels[:,val_idx]==1).flatten()\n",
    "    data_labels[key_pos,val_idx] = 1\n",
    "    data_labels[val_pos,key_idx] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# non_empty_idx = np.argwhere(np.sum(data_labels, axis=1)!=0).flatten()\n",
    "# empty_idx = np.argwhere(np.sum(data_labels, axis=1)==0).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86337f7fb2514d8bb8abecee2d05717f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=34482.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yfeng/anaconda3/envs/physioNet/lib/python3.6/site-packages/numpy/core/fromnumeric.py:3335: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/home/yfeng/anaconda3/envs/physioNet/lib/python3.6/site-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "X_train = []\n",
    "Data_labels_train = []\n",
    "Idxes_train = []\n",
    "Idxes_dict_train = {}\n",
    "ct = 0\n",
    "for i in tqdm(all_train_idx):\n",
    "    Q_loc = Q_locs[i]\n",
    "    RR_avg = np.median([Q_loc[k+1] - Q_loc[k] for k in range(len(Q_loc)-1)])\n",
    "    RR_th = (0.3 * RR_avg, 3 * RR_avg)\n",
    "    \n",
    "    ks = [k for k in range(len(Q_loc)-1) if Q_loc[k+1] - Q_loc[k] > RR_th[0] \n",
    "              and Q_loc[k+1] - Q_loc[k] < RR_th[1]]\n",
    "    \n",
    "    \n",
    "    Idxes_dict_train[i] = []\n",
    "    for k in ks:\n",
    "        Data_labels_train.append(data_labels[i])\n",
    "        X_train.append(Datas[i][:,Q_loc[k]:Q_loc[k+1]])\n",
    "        Idxes_train.append(i)\n",
    "        Idxes_dict_train[i].append(ct)\n",
    "        \n",
    "    ct += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc003d5a68114cab9247945d6519455b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=8619.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#X_test = []\n",
    "Data_labels_test = []\n",
    "Idxes_test = []\n",
    "Idxes_dict_test = {}\n",
    "ct = 0\n",
    "for i in tqdm(all_test_idx):\n",
    "        \n",
    "    Q_loc = Q_locs[i]\n",
    "    RR_avg = np.median([Q_loc[k+1] - Q_loc[k] for k in range(len(Q_loc)-1)])\n",
    "    RR_th = (0.3 * RR_avg, 3 * RR_avg)\n",
    "    \n",
    "    ks = [k for k in range(len(Q_loc)-1) if Q_loc[k+1] - Q_loc[k] > RR_th[0] \n",
    "              and Q_loc[k+1] - Q_loc[k] < RR_th[1]]\n",
    "    \n",
    "    \n",
    "    Idxes_dict_test[i] = []\n",
    "    for k in ks:\n",
    "        Data_labels_test.append(data_labels[i])\n",
    "        #X_test.append(Datas[i][:,1000+Q_loc[k]:1000+Q_loc[k+1]])\n",
    "        Idxes_test.append(i)\n",
    "        Idxes_dict_test[i].append(ct)\n",
    "        \n",
    "    ct += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Signals_train = np.zeros((len(X_train),12,MAX_RR))\n",
    "# for i in range(len(X_train)):\n",
    "#     Signals_train[i,:,:min(len(X_train[i][0]),MAX_RR)] = X_train[i][:,:MAX_RR]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Signals_test = np.zeros((len(X_test),12,MAX_RR))\n",
    "# for i in range(len(X_test)):\n",
    "#     Signals_test[i,:,:min(len(X_test[i][0]),MAX_RR)] = X_test[i][:,:MAX_RR]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save('../saved/Signals_train_stratified.npy', Signals_train)\n",
    "# np.save('../saved/Signals_test_stratified.npy', Signals_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# with open('../saved/Data_labels_train_stratified.pkl', 'wb') as Data_labels_train_file:\n",
    "#     pickle.dump(Data_labels_train, Data_labels_train_file)\n",
    "\n",
    "# with open('../saved/Data_labels_test_stratified.pkl', 'wb') as Data_labels_test_file:\n",
    "#     pickle.dump(Data_labels_test, Data_labels_test_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# On y va"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Signals_train = np.load('../saved/Signals_train_stratified.npy')\n",
    "Signals_test = np.load('../saved/Signals_test_stratified.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('../saved/Data_labels_train_stratified.pkl', 'rb') as Data_labels_train_file:\n",
    "    Data_labels_train = pickle.load(Data_labels_train_file)\n",
    "\n",
    "with open('../saved/Data_labels_test_stratified.pkl', 'rb') as Data_labels_test_file:\n",
    "    Data_labels_test = pickle.load(Data_labels_test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_name = 'ECGResNet_balanced_eqvclasses_no_posweight_no_weightedloss_500chn_n1_500'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# default `log_dir` is \"runs\" - we'll be more specific here\n",
    "writer = SummaryWriter('runs/{}'.format(run_name))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:0')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def weighted_binary_cross_entropy2(sigmoid_x, y, weighted_matrix, weight=None, reduction=None):\n",
    "    \"\"\"\n",
    "    Aha this is correct!\n",
    "    sigmoid_x = nn.Sigmoid()(x)\n",
    "    Args:\n",
    "        sigmoid_x: predicted probability of size [N,C], N sample and C Class. Eg. Must be in range of [0,1]\n",
    "        targets: true value, one-hot-like vector of size [N,C]\n",
    "        pos_weight: Weight for postive sample\n",
    "    \"\"\"\n",
    "    if not (y.size() == sigmoid_x.size()):\n",
    "        raise ValueError(\"Target size ({}) must be the same as input size ({})\".format(y.size(), sigmoid_x.size()))\n",
    "   \n",
    "    #print(\"y.size(), sigmoid_x.size()\", y.size(), sigmoid_x.size())\n",
    "    sigmoid_x = torch.clamp(sigmoid_x,min=1e-7,max=1-1e-7) \n",
    "    loss = - torch.matmul(y*sigmoid_x.log() + (1-y)*(1-sigmoid_x).log(), weighted_matrix)\n",
    "    \n",
    "    if weight is not None:\n",
    "        loss = loss * weight\n",
    "        \n",
    "    if reduction is None:\n",
    "        return loss\n",
    "    elif reduction == 'mean':\n",
    "        return loss.mean()\n",
    "    elif reduction == 'sum':\n",
    "        return loss.sum()\n",
    "    return None\n",
    "    \n",
    "class WeightedBCELoss(nn.Module):\n",
    "    def __init__(self, weights, PosWeightIsDynamic= False, WeightIsDynamic= False, \n",
    "                 reduction='mean'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            pos_weight = Weight for postive samples. Size [1,C]\n",
    "            weight = Weight for Each class. Size [1,C]\n",
    "            PosWeightIsDynamic: If True, the pos_weight is computed on each batch. If pos_weight is None, then it remains None.\n",
    "            WeightIsDynamic: If True, the weight is computed on each batch. If weight is None, then it remains None.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.register_buffer('weights', weights)\n",
    "        self.reduction = reduction\n",
    "        self.PosWeightIsDynamic = PosWeightIsDynamic\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        if self.PosWeightIsDynamic:\n",
    "            positive_counts = target.sum(dim=0)\n",
    "            nBatch = len(target)\n",
    "            self.pos_weight = (nBatch - positive_counts)/(positive_counts +1e-5)\n",
    "\n",
    "\n",
    "        return weighted_binary_cross_entropy2(input, target,\n",
    "                                             weighted_matrix=self.weights,\n",
    "                                             reduction=self.reduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "Signal_12leads_train = np.transpose(Signals_train, (1,0,2))\n",
    "Signal_12leads_test= np.transpose(Signals_test, (1,0,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "\n",
    "import os\n",
    "class SignalDataset(Dataset):\n",
    "\n",
    "    def __init__(self, signals, labels):\n",
    "        self.signals = signals\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.signals)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        sample =(torch.cat([torch.Tensor(np.array([self.signals[idx,channel]])) for channel in range(12)], axis=0), \n",
    "                  torch.Tensor(self.labels[idx]))\n",
    "\n",
    "        return sample\n",
    "    \n",
    "signal_datasets_train = SignalDataset(Signal_12leads_train[:,:,:MAX_RR], np.array(Data_labels_train)[:,:N_CLASS])\n",
    "signal_datasets_test = SignalDataset(Signal_12leads_test[:,:,:MAX_RR], np.array(Data_labels_test)[:,:N_CLASS])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12, 321263, 750)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Signal_12leads_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.array(Data_labels_train).shape[1] == N_CLASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert signal_datasets_train[0][0].shape[1] == MAX_RR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from global_vars import weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_tensor = torch.Tensor(weights).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation.evaluate_12ECG_score import compute_modified_confusion_matrix, compute_challenge_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_score(labels, outputs, weights, normal_index=normal_idx):\n",
    "    num_recordings, num_classes = np.shape(labels)\n",
    "    # Compute the observed score.\n",
    "    A = compute_modified_confusion_matrix(labels, outputs)\n",
    "    observed_score = np.nansum(weights * A)\n",
    "\n",
    "    # Compute the score for the model that always chooses the correct label(s).\n",
    "    correct_outputs = labels\n",
    "    A = compute_modified_confusion_matrix(labels, correct_outputs)\n",
    "    correct_score = np.nansum(weights * A)\n",
    "\n",
    "    # Compute the score for the model that always chooses the normal class.\n",
    "    inactive_outputs = np.zeros((num_recordings, num_classes), dtype=np.bool)\n",
    "    inactive_outputs[:, normal_index] = 1\n",
    "    A = compute_modified_confusion_matrix(labels, inactive_outputs)\n",
    "    inactive_score = np.nansum(weights * A)\n",
    "\n",
    "    if correct_score != inactive_score:\n",
    "        normalized_score = float(observed_score - inactive_score) / float(correct_score - inactive_score)\n",
    "    else:\n",
    "        normalized_score = float('nan')\n",
    "\n",
    "    return normalized_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_pr_curve_tensorboard(class_index, test_probs, test_preds, names, global_step, prefix):\n",
    "    '''\n",
    "    Takes in a \"class_index\" from 0 to 9 and plots the corresponding\n",
    "    precision-recall curve\n",
    "    '''\n",
    "\n",
    "    writer.add_pr_curve(prefix + '_' + names[class_index],\n",
    "                        test_preds[:,class_index],\n",
    "                        test_probs[:,class_index],\n",
    "                        global_step=global_step,\n",
    "                        num_thresholds=127)\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "dummy = torch.ones((10, MAX_RR, 12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from resnet1d import ECGResNet\n",
    "model = ECGResNet(MAX_RR, 27, n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 27])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(dummy).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ECGResNet(\n",
       "  (encoder): ResNetEncoder(\n",
       "    (gate): Sequential(\n",
       "      (0): Conv1d(500, 64, kernel_size=(7,), stride=(2,), padding=(3,), bias=False)\n",
       "      (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): MaxPool1d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (blocks): ModuleList(\n",
       "      (0): ResNetLayer(\n",
       "        (blocks): Sequential(\n",
       "          (0): ResNet753Block(\n",
       "            (blocks): Sequential(\n",
       "              (0): Sequential(\n",
       "                (0): Conv1dAuto(64, 64, kernel_size=(7,), stride=(1,), padding=(3,), bias=False)\n",
       "                (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Sequential(\n",
       "                (0): Conv1dAuto(64, 64, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
       "                (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (3): ReLU(inplace=True)\n",
       "              (4): Sequential(\n",
       "                (0): Conv1dAuto(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "                (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "            (activate): ReLU(inplace=True)\n",
       "            (shortcut): Sequential(\n",
       "              (0): Conv1d(64, 64, kernel_size=(1,), stride=(1,), bias=False)\n",
       "              (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): ResNetLayer(\n",
       "        (blocks): Sequential(\n",
       "          (0): ResNet753Block(\n",
       "            (blocks): Sequential(\n",
       "              (0): Sequential(\n",
       "                (0): Conv1dAuto(64, 64, kernel_size=(7,), stride=(1,), padding=(3,), bias=False)\n",
       "                (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Sequential(\n",
       "                (0): Conv1dAuto(64, 64, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
       "                (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (3): ReLU(inplace=True)\n",
       "              (4): Sequential(\n",
       "                (0): Conv1dAuto(64, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "                (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "            (activate): ReLU(inplace=True)\n",
       "            (shortcut): Sequential(\n",
       "              (0): Conv1d(64, 128, kernel_size=(1,), stride=(1,), bias=False)\n",
       "              (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): ResNetLayer(\n",
       "        (blocks): Sequential(\n",
       "          (0): ResNet753Block(\n",
       "            (blocks): Sequential(\n",
       "              (0): Sequential(\n",
       "                (0): Conv1dAuto(128, 128, kernel_size=(7,), stride=(1,), padding=(3,), bias=False)\n",
       "                (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Sequential(\n",
       "                (0): Conv1dAuto(128, 128, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
       "                (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (3): ReLU(inplace=True)\n",
       "              (4): Sequential(\n",
       "                (0): Conv1dAuto(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "                (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "            (activate): ReLU(inplace=True)\n",
       "            (shortcut): Sequential(\n",
       "              (0): Conv1d(128, 128, kernel_size=(1,), stride=(1,), bias=False)\n",
       "              (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): ResnetDecoder(\n",
       "    (avg): AdaptiveAvgPool1d(output_size=(1,))\n",
       "    (decoder): Linear(in_features=128, out_features=27, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision import datasets, models, transforms\n",
    "from myeval import agg_y_preds_bags, binary_acc, geometry_loss\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from snippets.pytorchtools import EarlyStopping\n",
    "from sklearn.model_selection import GroupKFold\n",
    "import time\n",
    "from imbalanced_sampler import ImbalancedDatasetSampler\n",
    "\n",
    "st = time.time()\n",
    "patience = 50\n",
    "batch_size= 512#65000\n",
    "\n",
    "saved_dir = '../saved/ECGResNet/'\n",
    "model.to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Encode old label (multi-hot encoding) to new label (number) with a threshold (>=80-percentile length)\n",
    "\"\"\"\n",
    "Code_label_train = [get_scored_class(Data_labels_train[i], labels) for i in range(len(Data_labels_train)) ]\n",
    "\n",
    "\n",
    "# key: multi-hot encoding label in str\n",
    "# val: idx\n",
    "labels_dict = {}\n",
    "for idx, data_label in enumerate(Code_label_train):\n",
    "    key = ''.join([str(l) for l in data_label])\n",
    "    if key not in labels_dict:\n",
    "        labels_dict[key] = [idx]\n",
    "    else:\n",
    "        labels_dict[key].append(idx)\n",
    "\n",
    "# for each label (multi-hot encoding), their length\n",
    "labels_dict_len = []\n",
    "labels_dict_key = []\n",
    "for key in labels_dict.keys():\n",
    "    labels_dict_len.append(len(labels_dict[key]))\n",
    "    labels_dict_key.append(key)\n",
    "labels_dict_len = np.array(labels_dict_len)\n",
    "labels_dict_key = np.array(labels_dict_key)\n",
    "\n",
    "threshold = np.percentile(labels_dict_len, 80)\n",
    "\n",
    "labels_dict_len_threshold = labels_dict_len[labels_dict_len>threshold]\n",
    "labels_dict_key_threshold = labels_dict_key[labels_dict_len>threshold]\n",
    "\n",
    "# old label (multi-hot encoding) => new label (number)\n",
    "new_index_dict = {}\n",
    "for i in range(len(labels_dict_key_threshold)):\n",
    "    new_index_dict[labels_dict_key_threshold[i]] = i\n",
    "\n",
    "# change label to new label\n",
    "Code_label_train_new = np.zeros((len(Code_label_train),), dtype = int)\n",
    "special_class = -1\n",
    "for key, vals in labels_dict.items():\n",
    "    for val in vals:\n",
    "        if key not in labels_dict_key_threshold:\n",
    "            Code_label_train_new[val] = special_class\n",
    "        else:\n",
    "            Code_label_train_new[val] = new_index_dict[key]\n",
    "        \n",
    "\n",
    "# distribution of classes in the dataset \n",
    "label_to_count = {}\n",
    "for label in Code_label_train_new:\n",
    "    if label in label_to_count:\n",
    "        label_to_count[label] += 1\n",
    "    else:\n",
    "        label_to_count[label] = 1\n",
    "\n",
    "# weight for each sample\n",
    "sample_weights = [1.0 / label_to_count[label]\n",
    "           for label in Code_label_train_new]\n",
    "import torch\n",
    "sample_weights = torch.DoubleTensor(sample_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imbalanced_sampler import  WeightedImbalancedDatasetSampler\n",
    "trainDataset = torch.utils.data.Subset(signal_datasets_train, range(0,len(Signals_train), 1))\n",
    "testDataset = torch.utils.data.Subset(signal_datasets_test, range(0,len(Signals_test), 1))\n",
    "\n",
    "trainLoader = torch.utils.data.DataLoader(trainDataset, batch_size=batch_size, pin_memory=True, \n",
    "                                       sampler=WeightedImbalancedDatasetSampler(trainDataset, weights=sample_weights))\n",
    "testLoader = torch.utils.data.DataLoader(testDataset, batch_size=3000, shuffle = False, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#criterion_train = WeightedBCELoss(weights=weights_tensor, reduction='mean')\n",
    "#criterion_test = WeightedBCELoss(weights=weights_tensor, reduction='mean')\n",
    "\n",
    "class_weights_train = 1.0/np.sum(Data_labels_train, axis=0)\n",
    "class_weights_train = class_weights_train / np.sum(class_weights_train)\n",
    "class_weights_train = torch.Tensor(class_weights_train).to(device)\n",
    "\n",
    "\n",
    "class_weights_test = 1.0/np.sum(Data_labels_test, axis=0)\n",
    "class_weights_test = class_weights_test / np.sum(class_weights_test)\n",
    "class_weights_test = torch.Tensor(class_weights_test).to(device)\n",
    "\n",
    "pos_weight = np.ones(N_CLASS) * 5\n",
    "pos_weight = torch.Tensor(pos_weight).to(device)\n",
    "\n",
    "criterion_train = nn.BCEWithLogitsLoss(reduction='mean') \n",
    "criterion_test = nn.BCEWithLogitsLoss(reduction='mean') \n",
    "\n",
    "losses_train = []\n",
    "losses_test = []\n",
    "\n",
    "avg_losses_train = []\n",
    "avg_losses_test = []\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(patience, verbose=False, \n",
    "                              saved_dir=saved_dir, \n",
    "                              save_name='resnet50_'+run_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.01) \n",
    "#optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9) #\n",
    "# Decay LR by a factor of 0.1 every 100 epochs\n",
    "#scheduler = lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.1)\n",
    "scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.1, patience=20, mode='max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [get_name(label, Dx_map, Dx_map_unscored) for label in labels]\n",
    "\n",
    "assert len(labels) == 27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce32358cb73f431eb93584c6964afff5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "IndexError",
     "evalue": "index 179976 is out of bounds for axis 0 with size 12",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-76-7823765597ef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0my_trains\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# ground truth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0moutput_trains\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainLoader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/physioNet/lib/python3.6/site-packages/tqdm/notebook.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    216\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtqdm_notebook\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m                 \u001b[0;31m# return super(tqdm...) will not catch exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/physioNet/lib/python3.6/site-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1117\u001b[0m                 fp_write=getattr(self.fp, 'write', sys.stderr.write))\n\u001b[1;32m   1118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1119\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1120\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1121\u001b[0m             \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/physioNet/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/physioNet/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    383\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/physioNet/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/physioNet/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/physioNet/lib/python3.6/site-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-50-a04c5a142f2a>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         sample =(torch.cat([torch.Tensor(np.array([self.signals[idx,channel]])) for channel in range(12)], axis=0), \n\u001b[0m\u001b[1;32m     18\u001b[0m                   torch.Tensor(self.labels[idx]))\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-50-a04c5a142f2a>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         sample =(torch.cat([torch.Tensor(np.array([self.signals[idx,channel]])) for channel in range(12)], axis=0), \n\u001b[0m\u001b[1;32m     18\u001b[0m                   torch.Tensor(self.labels[idx]))\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 179976 is out of bounds for axis 0 with size 12"
     ]
    }
   ],
   "source": [
    "y_trains_tensor = None\n",
    "y_tests_tensor = None\n",
    "for epoch in range(0, 5000):\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    running_loss_train = 0.0\n",
    "\n",
    "    y_trains = [] # ground truth\n",
    "    output_trains = [] # output\n",
    "    for k, (X_train, y_train) in tqdm(enumerate(trainLoader)):\n",
    "        y_train = y_train.to(device)\n",
    "        X_train = X_train.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output_train = model(X_train)\n",
    "        output_trains.append(output_train.cpu())\n",
    "        \n",
    "        loss_train = criterion_train(output_train, y_train)\n",
    "        losses_train.append(loss_train.item())\n",
    "        \n",
    "        avg_loss_train = np.average(losses_train)\n",
    "        avg_losses_train.append(avg_loss_train)\n",
    "    \n",
    "        \n",
    "        if np.mod(k, 100) == 0:\n",
    "            writer.add_scalar('training loss',\n",
    "            avg_loss_train,\n",
    "            epoch * (len(Data_labels_train)//batch_size//100+1) + k//100)\n",
    "        \n",
    "        y_trains.append(y_train.cpu())\n",
    "            \n",
    "        loss_train.backward()\n",
    "        optimizer.step()\n",
    "            \n",
    "    #scheduler.step()\n",
    "    \n",
    "\n",
    "    y_tests = [] # ground truth\n",
    "    output_tests = [] # output\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        \n",
    "        for X_test, y_test in testLoader:  \n",
    "            y_test = y_test.to(device)\n",
    "            X_test = X_test.to(device)\n",
    "            output_test = model(X_test)\n",
    "\n",
    "            loss_test = criterion_test(output_test, y_test)\n",
    "            losses_test.append(loss_test.item())\n",
    "\n",
    "            output_tests.append(output_test.cpu())\n",
    "            y_tests.append(y_test.cpu())\n",
    "            \n",
    "            \n",
    "            \n",
    "        avg_loss_test = np.average(losses_test)\n",
    "        avg_losses_test.append(avg_loss_test)\n",
    "\n",
    "        writer.add_scalar('testing loss',\n",
    "                avg_loss_test,\n",
    "                epoch)\n",
    "\n",
    "\n",
    "\n",
    "    y_trains_tensor = torch.cat(y_trains, axis=0) # ground truth\n",
    "    y_tests_tensor = torch.cat(y_tests, axis=0) # ground truth\n",
    "\n",
    "    output_trains = torch.cat(output_trains, axis=0) \n",
    "    y_train_preds = torch.sigmoid(output_trains)\n",
    "\n",
    "    output_tests = torch.cat(output_tests, axis=0)\n",
    "    y_test_preds = torch.sigmoid(output_tests)\n",
    "\n",
    "    #output_trains = torch.cat(output_trains, axis=0)\n",
    "#     y_train_preds_max, y_train_preds_mean, _ = agg_y_preds_bags(y_train_preds, bag_size=n_segments)\n",
    "#     y_test_preds_max, y_test_preds_mean, _ = agg_y_preds_bags(y_test_preds, bag_size=n_segments)\n",
    "#     _, _, y_trains = agg_y_preds_bags(y_trains, bag_size=n_segments)\n",
    "#     _, _, y_tests = agg_y_preds_bags(y_tests, bag_size=n_segments)\n",
    "\n",
    "\n",
    "    for class_idx in range(N_CLASS):\n",
    "        add_pr_curve_tensorboard(class_idx, y_trains_tensor, y_train_preds, names, global_step=epoch, prefix='train')\n",
    "        add_pr_curve_tensorboard(class_idx, y_tests_tensor, y_test_preds, names, global_step=epoch, prefix='test')\n",
    "\n",
    "\n",
    "\n",
    "    acc, fmeasure, fbeta, gbeta = binary_acc(y_train_preds, y_trains_tensor)           \n",
    "    acc2, fmeasure2, fbeta2, gbeta2 = binary_acc(y_test_preds, y_tests_tensor)\n",
    "    geometry = geometry_loss(fbeta, gbeta)\n",
    "    geometry2 = geometry_loss(fbeta2, gbeta2)\n",
    "    \n",
    "#     output_str = 'S{} {:.2f} min |\\n Train Loss: {:.6f}, Acc: {:.3f}, F: {:.3f}, Fbeta: {:.3f}, gbeta: {:.3f}, geo: {:.3f}, score: {:.3f} |\\n Valid Loss: {:.6f}, Acc: {:.3f}, F: {:.3f}, Fbeta: {:.3f}, gbeta: {:.3f}, geo: {:.3f}, score: {:.3f}\\n '.format(\n",
    "#         epoch, (time.time()-st)/60,\n",
    "#         avg_loss_train, acc, fmeasure, fbeta, gbeta, geometry, np.nan,\n",
    "#         avg_loss_test, acc2, fmeasure2, fbeta2, gbeta2, geometry2, np.nan)\n",
    "#     print(output_str)\n",
    "    score = compute_score(np.round(y_train_preds.data.numpy()), np.round(y_trains_tensor.data.numpy()), weights)\n",
    "    score2 = compute_score(np.round(y_test_preds.data.numpy()), np.round(y_tests_tensor.data.numpy()), weights)\n",
    "    output_str = 'S{} {:.2f} min |\\n Train Loss: {:.6f}, Acc: {:.3f}, F: {:.3f}, Fbeta: {:.3f}, gbeta: {:.3f}, geo: {:.3f}, score: {:.3f} |\\n Valid Loss: {:.6f}, Acc: {:.3f}, F: {:.3f}, Fbeta: {:.3f}, gbeta: {:.3f}, geo: {:.3f}, score: {:.3f}\\n '.format(\n",
    "        epoch, (time.time()-st)/60,\n",
    "        avg_loss_train, acc, fmeasure, fbeta, gbeta, geometry, score,\n",
    "        avg_loss_test, acc2, fmeasure2, fbeta2, gbeta2, geometry2, score2)\n",
    "    scheduler.step(score2)\n",
    "    \n",
    "    writer.add_scalar('training score',\n",
    "            score,\n",
    "            epoch)\n",
    "\n",
    "    writer.add_scalar('testing score',\n",
    "            score2,\n",
    "            epoch)\n",
    "    \n",
    "    print(output_str)\n",
    "\n",
    "    with open(saved_dir+'loss_{}.txt'.format(run_name), 'a') as f:\n",
    "        print(output_str, file=f)\n",
    "\n",
    "    early_stopping(-score2, model)\n",
    "\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping\")\n",
    "        break\n",
    "\n",
    "\n",
    "#     output_string = 'AUROC|AUPRC|Accuracy|F-measure|Fbeta-measure|Gbeta-measure|Geomotry\\n{:.3f}|{:.3f}|{:.3f}|{:.3f}|{:.3f}|{:.3f}|{:.3f}'.format(auroc2,auprc2,acc2,fmeasure2,fbeta2,gbeta2,geometry2)\n",
    "#     print(output_string)     \n",
    "#     with open(saved_dir+'score'+ str(i)+ '_epoch' + str(epoch) + '.txt', 'w') as f:\n",
    "#         f.write(output_string)\n",
    "\n",
    "#     avg_losses_train = np.array(avg_losses_train)\n",
    "#     avg_losses_test = np.array(avg_losses_test)\n",
    "\n",
    "#     np.save(saved_dir + 'avg_losses_train' + str(i) + '_epoch' + str(epoch), avg_losses_train)\n",
    "#     np.save(saved_dir + 'avg_losses_test' + str(i) + '_epoch' + str(epoch), avg_losses_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import multilabel_confusion_matrix, confusion_matrix\n",
    "from global_vars import labels\n",
    "\n",
    "cf_matrices = multilabel_confusion_matrix(y_trains_tensor.data.numpy(), np.round(y_train_preds.data.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label, cf_matrix in zip(labels, cf_matrices):\n",
    "    print(get_name(label, Dx_map, Dx_map_unscored))\n",
    "    print(cf_matrix)\n",
    "    print('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_trains_tensor.data.numpy()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 (physioNet)",
   "language": "python",
   "name": "physionet"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
