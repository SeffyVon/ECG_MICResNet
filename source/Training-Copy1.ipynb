{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np, os, sys\n",
    "from tqdm.notebook import tqdm    \n",
    "from manipulations import get_classes, get_classes_from_header, get_Fs_from_header, load_challenge_data\n",
    "from sklearn.model_selection import KFold, StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_CLASS = 27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_RR = 750 # 60 beats/min => 60 beats/60 s ==> beat/1s ==> 500 samples / beat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from manipulations import get_abbr, get_name\n",
    "from global_vars import labels, Dx_map, Dx_map_unscored\n",
    "first_idx = {scored_code: None for scored_code in list(Dx_map['SNOMED CT Code'])}\n",
    "first_idx_unscored = {unscored_code: None for unscored_code in list(Dx_map_unscored['SNOMED CT Code'])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from global_vars import labels, normal_class, equivalent_mapping\n",
    "normal_idx = np.argwhere(labels==int(normal_class))\n",
    "def get_scored_class(code, labels):\n",
    "    return [1 if label in code else 0 for label in labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "    \n",
    "#     Datas = []\n",
    "#     Header_datas = []\n",
    "#     Classes = []\n",
    "#     dataset_idx = {}\n",
    "#     dataset_train_idx = {}\n",
    "#     dataset_test_idx = {}\n",
    "#     global_idx = 0\n",
    "#     datasets = [1,2,3,4,5,6]\n",
    "#     for dataset in datasets:\n",
    "#         print('Dataset ', dataset)\n",
    "#         # Parse arguments.\n",
    "#         if len(sys.argv) != 3:\n",
    "#             raise Exception('Include the input and output directories as arguments, e.g., python driver.py input output.')\n",
    "\n",
    "#         input_directory = '../NewData/{}/'.format(dataset)\n",
    "#         output_directory = '../Output/'\n",
    "\n",
    "#         # Find files.\n",
    "#         input_files = []\n",
    "#         for f in os.listdir(input_directory):\n",
    "#             if os.path.isfile(os.path.join(input_directory, f)) and not f.lower().startswith('.') and f.lower().endswith('mat'):\n",
    "#                 input_files.append(f)\n",
    "\n",
    "#         if not os.path.isdir(output_directory):\n",
    "#             os.mkdir(output_directory)\n",
    "\n",
    "#         classes=get_classes(input_directory,input_files)\n",
    "\n",
    "#         num_files = len(input_files)\n",
    "#         datas = []\n",
    "#         header_datas = []\n",
    "#         dataset_idx[dataset] = []\n",
    "#         for i, f in tqdm(enumerate(input_files)):\n",
    "#             #print('    {}/{}...'.format(i+1, num_files), f)\n",
    "#             tmp_input_file = os.path.join(input_directory,f)\n",
    "#             data,header_data = load_challenge_data(tmp_input_file)\n",
    "#             datas.append(data[:,1000:7000])\n",
    "#             header_datas.append(header_data)\n",
    "#             dataset_idx[dataset].append(global_idx)\n",
    "#             global_idx += 1\n",
    "\n",
    "#         Datas += datas\n",
    "#         Header_datas += header_datas\n",
    "#         Classes += classes\n",
    "        \n",
    "#         kf = KFold(5)\n",
    "#         train_idx, test_idx = next(kf.split(datas))\n",
    "        \n",
    "#         dataset_train_idx[dataset] = train_idx\n",
    "#         dataset_test_idx[dataset] = test_idx\n",
    "#         print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datasets = [1,2,3,4,5,6]\n",
    "# all_train_idx = []\n",
    "# for dataset in datasets:\n",
    "#     all_train_idx.extend(dataset_train_idx[dataset])\n",
    "    \n",
    "# all_test_idx = []\n",
    "# for dataset in datasets:\n",
    "#     all_test_idx.extend(dataset_test_idx[dataset])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from signal_processing import myfilter, main_QRST\n",
    "# Codes = []\n",
    "# Q_locs = []\n",
    "# idxes = []\n",
    "# for idx in tqdm(range(0, len(Datas))): \n",
    "#     codes = get_classes_from_header(Header_datas[idx])\n",
    "#     names = ', '.join([get_name(int(code), Dx_map, Dx_map_unscored) for code in codes])\n",
    "\n",
    "#     filtered_Data = myfilter(Datas[idx], 500, vis=False)\n",
    "\n",
    "#     # get the lead to apply Pan Tomkins\n",
    "#     Q_loc = main_QRST(filtered_Data, idx, '', '', names, fig2=False)\n",
    "\n",
    "#     # store\n",
    "#     Codes.append(codes)\n",
    "#     Q_locs.append(Q_loc)\n",
    "\n",
    "#     idxes.append(idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from signal_processing import myfilter, extract_QRST, main_QRST\n",
    "# idx = 1511#37581#2932#6884 #6884 #6951 #227#8074 #325#158#325#34#17#8659#4#18#61#10927#6886# 8659\n",
    "# codes = get_classes_from_header(Header_datas[idx])\n",
    "# names = ', '.join([get_name(int(code), Dx_map, Dx_map_unscored) for code in codes])\n",
    "# filtered_Data = myfilter(Datas[idx], 500, vis=False)\n",
    "# #filtered_Data = myfilter(Datas[idx][:,-4000:-1000], 500, vis=False)\n",
    "# Q_loc = main_QRST(filtered_Data, idx, '', '', names, fig2=False, verbose=True, vis=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# with open('../saved/newData_idxes_1000_7000_peakdist100.pkl', 'wb') as idxes_file:\n",
    "#     pickle.dump(idxes, idxes_file)\n",
    "\n",
    "# with open('../saved/newData_Codes_1000_7000_peakdist100.pkl', 'wb') as Codes_file:\n",
    "#     pickle.dump(Codes, Codes_file)\n",
    "\n",
    "# with open('../saved/newData_Q_locs_1000_7000_peakdist100.pkl', 'wb') as Q_locs_file:\n",
    "#     pickle.dump(Q_locs, Q_locs_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# Q_locs = None\n",
    "# with open('../saved/newData_Q_locs_1000_7000_peakdist100.pkl', 'rb') as Q_locs_file:\n",
    "#     Q_locs = pickle.load(Q_locs_file)\n",
    "\n",
    "# Codes = None\n",
    "# with open('../saved/newData_Codes_1000_7000_peakdist100.pkl', 'rb') as Codes_file:\n",
    "#     Codes = pickle.load(Codes_file)\n",
    "\n",
    "# idxes = None\n",
    "# with open('../saved/newData_idxes_1000_7000_peakdist100.pkl', 'rb') as idxes_file:\n",
    "#     idxes = pickle.load(idxes_file)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Codes = []\n",
    "# for idx in tqdm(range(0, len(Datas))): \n",
    "#     codes = get_classes_from_header(Header_datas[idx])\n",
    "#     # store\n",
    "#     Codes.append(codes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_labels = np.array([get_scored_class(codes, labels) for codes in Codes])\n",
    "# print(data_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# key_idxes = []\n",
    "# for key in equivalent_mapping.keys():\n",
    "#     print(key)\n",
    "#     key_idx = np.argwhere(labels==int(key)).flatten()[0]\n",
    "#     key_idxes.append(key_idx)\n",
    "#     val_idx = np.argwhere(labels==int(equivalent_mapping[key])).flatten()[0]\n",
    "#     key_pos = np.argwhere(data_labels[:,key_idx]==1).flatten()\n",
    "#     val_pos = np.argwhere(data_labels[:,val_idx]==1).flatten()\n",
    "#     data_labels[key_pos,val_idx] = 1\n",
    "#     data_labels[val_pos,key_idx] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# non_empty_idx = np.argwhere(np.sum(data_labels, axis=1)!=0).flatten()\n",
    "# empty_idx = np.argwhere(np.sum(data_labels, axis=1)==0).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = []\n",
    "# Data_labels_train = []\n",
    "# Idxes_train = []\n",
    "# Idxes_dict_train = {}\n",
    "# ct = 0\n",
    "# for i in tqdm(all_train_idx):\n",
    "#     Q_loc = Q_locs[i]\n",
    "#     RR_avg = np.median([Q_loc[k+1] - Q_loc[k] for k in range(len(Q_loc)-1)])\n",
    "#     RR_th = (0.3 * RR_avg, 3 * RR_avg)\n",
    "    \n",
    "#     ks = [k for k in range(len(Q_loc)-1) if Q_loc[k+1] - Q_loc[k] > RR_th[0] \n",
    "#               and Q_loc[k+1] - Q_loc[k] < RR_th[1]]\n",
    "    \n",
    "    \n",
    "#     Idxes_dict_train[i] = []\n",
    "#     for k in ks:\n",
    "#         Data_labels_train.append(get_scored_class(Codes[i], labels))\n",
    "#         X_train.append(Datas[i][:,1000+Q_loc[k]:1000+Q_loc[k+1]])\n",
    "#         Idxes_train.append(i)\n",
    "#         Idxes_dict_train[i].append(ct)\n",
    "        \n",
    "#     ct += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test = []\n",
    "# Data_labels_test = []\n",
    "# Idxes_test = []\n",
    "# Idxes_dict_test = {}\n",
    "# ct = 0\n",
    "# for i in tqdm(all_test_idx):\n",
    "        \n",
    "#     Q_loc = Q_locs[i]\n",
    "#     RR_avg = np.median([Q_loc[k+1] - Q_loc[k] for k in range(len(Q_loc)-1)])\n",
    "#     RR_th = (0.3 * RR_avg, 3 * RR_avg)\n",
    "    \n",
    "#     ks = [k for k in range(len(Q_loc)-1) if Q_loc[k+1] - Q_loc[k] > RR_th[0] \n",
    "#               and Q_loc[k+1] - Q_loc[k] < RR_th[1]]\n",
    "    \n",
    "    \n",
    "#     Idxes_dict_test[i] = []\n",
    "#     for k in ks:\n",
    "#         Data_labels_test.append(get_scored_class(Codes[i], labels))\n",
    "#         X_test.append(Datas[i][:,1000+Q_loc[k]:1000+Q_loc[k+1]])\n",
    "#         Idxes_test.append(i)\n",
    "#         Idxes_dict_test[i].append(ct)\n",
    "        \n",
    "#     ct += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Signals_train = np.zeros((len(X_train),12,MAX_RR))\n",
    "# for i in range(len(X_train)):\n",
    "#     Signals_train[i,:,:min(len(X_train[i][0]),MAX_RR)] = X_train[i][:,:MAX_RR]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Signals_test = np.zeros((len(X_test),12,MAX_RR))\n",
    "# for i in range(len(X_test)):\n",
    "#     Signals_test[i,:,:min(len(X_test[i][0]),MAX_RR)] = X_test[i][:,:MAX_RR]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save('../saved/Signals_train.npy', Signals_train)\n",
    "# np.save('../saved/Signals_test.npy', Signals_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# with open('../saved/Data_labels_train.pkl', 'wb') as Data_labels_train_file:\n",
    "#     pickle.dump(Data_labels_train, Data_labels_train_file)\n",
    "\n",
    "# with open('../saved/Data_labels_test.pkl', 'wb') as Data_labels_test_file:\n",
    "#     pickle.dump(Data_labels_test, Data_labels_test_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# On y va"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "Signals_train = np.load('../saved/Signals_train.npy')\n",
    "Signals_test = np.load('../saved/Signals_test.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('../saved/Data_labels_train.pkl', 'rb') as Data_labels_train_file:\n",
    "    Data_labels_train = pickle.load(Data_labels_train_file)\n",
    "\n",
    "with open('../saved/Data_labels_test.pkl', 'rb') as Data_labels_test_file:\n",
    "    Data_labels_test = pickle.load(Data_labels_test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_name = 'resnet50_minibatch_adam'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# default `log_dir` is \"runs\" - we'll be more specific here\n",
    "writer = SummaryWriter('runs/{}'.format(run_name))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:0')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def weighted_binary_cross_entropy2(sigmoid_x, y, weighted_matrix, weight=None, reduction=None):\n",
    "    \"\"\"\n",
    "    Aha this is correct!\n",
    "    sigmoid_x = nn.Sigmoid()(x)\n",
    "    Args:\n",
    "        sigmoid_x: predicted probability of size [N,C], N sample and C Class. Eg. Must be in range of [0,1]\n",
    "        targets: true value, one-hot-like vector of size [N,C]\n",
    "        pos_weight: Weight for postive sample\n",
    "    \"\"\"\n",
    "    if not (y.size() == sigmoid_x.size()):\n",
    "        raise ValueError(\"Target size ({}) must be the same as input size ({})\".format(y.size(), sigmoid_x.size()))\n",
    "   \n",
    "    #print(\"y.size(), sigmoid_x.size()\", y.size(), sigmoid_x.size())\n",
    "    sigmoid_x = torch.clamp(sigmoid_x,min=1e-7,max=1-1e-7) \n",
    "    loss = - torch.matmul(y*sigmoid_x.log() + (1-y)*(1-sigmoid_x).log(), weighted_matrix)\n",
    "    \n",
    "    if weight is not None:\n",
    "        loss = loss * weight\n",
    "        \n",
    "    if reduction is None:\n",
    "        return loss\n",
    "    elif reduction == 'mean':\n",
    "        return loss.mean()\n",
    "    elif reduction == 'sum':\n",
    "        return loss.sum()\n",
    "    return None\n",
    "    \n",
    "class WeightedBCELoss(nn.Module):\n",
    "    def __init__(self, weights, PosWeightIsDynamic= False, WeightIsDynamic= False, \n",
    "                 reduction='mean'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            pos_weight = Weight for postive samples. Size [1,C]\n",
    "            weight = Weight for Each class. Size [1,C]\n",
    "            PosWeightIsDynamic: If True, the pos_weight is computed on each batch. If pos_weight is None, then it remains None.\n",
    "            WeightIsDynamic: If True, the weight is computed on each batch. If weight is None, then it remains None.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.register_buffer('weights', weights)\n",
    "        self.reduction = reduction\n",
    "        self.PosWeightIsDynamic = PosWeightIsDynamic\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        if self.PosWeightIsDynamic:\n",
    "            positive_counts = target.sum(dim=0)\n",
    "            nBatch = len(target)\n",
    "            self.pos_weight = (nBatch - positive_counts)/(positive_counts +1e-5)\n",
    "\n",
    "\n",
    "        return weighted_binary_cross_entropy2(input, target,\n",
    "                                             weighted_matrix=self.weights,\n",
    "                                             reduction=self.reduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "Signal_12leads_train = np.transpose(Signals_train, (1,0,2))\n",
    "Signal_12leads_test= np.transpose(Signals_test, (1,0,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "\n",
    "import os\n",
    "class SignalDataset(Dataset):\n",
    "\n",
    "    def __init__(self, signals, labels):\n",
    "        self.signals = signals\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.signals)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        sample =(torch.cat([torch.Tensor(np.array([self.signals[channel,idx]]).transpose()) for channel in range(12)], axis=1), \n",
    "                  torch.Tensor(self.labels[idx]))\n",
    "\n",
    "        return sample\n",
    "    \n",
    "signal_datasets_train = SignalDataset(Signal_12leads_train, np.array(Data_labels_train)[:,:N_CLASS])\n",
    "signal_datasets_test = SignalDataset(Signal_12leads_test, np.array(Data_labels_test)[:,:N_CLASS])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.array(Data_labels_train).shape[1] == N_CLASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([750, 12])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "signal_datasets_train[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from global_vars import weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_tensor = torch.Tensor(weights).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation.evaluate_12ECG_score import compute_modified_confusion_matrix, compute_challenge_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_score(labels, outputs, weights, normal_index=normal_idx):\n",
    "    num_recordings, num_classes = np.shape(labels)\n",
    "    # Compute the observed score.\n",
    "    A = compute_modified_confusion_matrix(labels, outputs)\n",
    "    observed_score = np.nansum(weights * A)\n",
    "\n",
    "    # Compute the score for the model that always chooses the correct label(s).\n",
    "    correct_outputs = labels\n",
    "    A = compute_modified_confusion_matrix(labels, correct_outputs)\n",
    "    correct_score = np.nansum(weights * A)\n",
    "\n",
    "    # Compute the score for the model that always chooses the normal class.\n",
    "    inactive_outputs = np.zeros((num_recordings, num_classes), dtype=np.bool)\n",
    "    inactive_outputs[:, normal_index] = 1\n",
    "    A = compute_modified_confusion_matrix(labels, inactive_outputs)\n",
    "    inactive_score = np.nansum(weights * A)\n",
    "\n",
    "    if correct_score != inactive_score:\n",
    "        normalized_score = float(observed_score - inactive_score) / float(correct_score - inactive_score)\n",
    "    else:\n",
    "        normalized_score = float('nan')\n",
    "\n",
    "    return normalized_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_pr_curve_tensorboard(class_index, test_probs, test_preds, names, global_step, prefix):\n",
    "    '''\n",
    "    Takes in a \"class_index\" from 0 to 9 and plots the corresponding\n",
    "    precision-recall curve\n",
    "    '''\n",
    "\n",
    "    writer.add_pr_curve(prefix + '_' + names[class_index],\n",
    "                        test_preds[:,class_index],\n",
    "                        test_probs[:,class_index],\n",
    "                        global_step=global_step)\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_weight = np.ones(N_CLASS) * 2\n",
    "pos_weight = torch.Tensor(pos_weight).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "dummy = torch.ones((10, 750, 48))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ECGResNet50(\n",
      "  (encoder): ResNetEncoder(\n",
      "    (gate): Sequential(\n",
      "      (0): Conv1d(750, 64, kernel_size=(9,), stride=(1,), bias=False)\n",
      "      (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): MaxPool1d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (blocks): ModuleList(\n",
      "      (0): ResNetLayer(\n",
      "        (blocks): Sequential(\n",
      "          (0): ResNet753Block(\n",
      "            (blocks): Sequential(\n",
      "              (0): Sequential(\n",
      "                (0): Conv1dAuto(64, 64, kernel_size=(7,), stride=(1,), padding=(3,), bias=False)\n",
      "                (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              )\n",
      "              (1): ReLU(inplace=True)\n",
      "              (2): Sequential(\n",
      "                (0): Conv1dAuto(64, 64, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
      "                (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              )\n",
      "              (3): ReLU(inplace=True)\n",
      "              (4): Sequential(\n",
      "                (0): Conv1dAuto(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
      "                (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              )\n",
      "            )\n",
      "            (activate): ReLU(inplace=True)\n",
      "            (shortcut): Sequential(\n",
      "              (0): Conv1d(64, 64, kernel_size=(1,), stride=(1,), bias=False)\n",
      "              (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (1): ResNet753Block(\n",
      "            (blocks): Sequential(\n",
      "              (0): Sequential(\n",
      "                (0): Conv1dAuto(64, 64, kernel_size=(7,), stride=(1,), padding=(3,), bias=False)\n",
      "                (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              )\n",
      "              (1): ReLU(inplace=True)\n",
      "              (2): Sequential(\n",
      "                (0): Conv1dAuto(64, 64, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
      "                (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              )\n",
      "              (3): ReLU(inplace=True)\n",
      "              (4): Sequential(\n",
      "                (0): Conv1dAuto(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
      "                (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              )\n",
      "            )\n",
      "            (activate): ReLU(inplace=True)\n",
      "            (shortcut): Sequential(\n",
      "              (0): Conv1d(64, 64, kernel_size=(1,), stride=(1,), bias=False)\n",
      "              (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): ResNetLayer(\n",
      "        (blocks): Sequential(\n",
      "          (0): ResNet753Block(\n",
      "            (blocks): Sequential(\n",
      "              (0): Sequential(\n",
      "                (0): Conv1dAuto(64, 64, kernel_size=(7,), stride=(1,), padding=(3,), bias=False)\n",
      "                (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              )\n",
      "              (1): ReLU(inplace=True)\n",
      "              (2): Sequential(\n",
      "                (0): Conv1dAuto(64, 64, kernel_size=(5,), stride=(2,), padding=(2,), bias=False)\n",
      "                (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              )\n",
      "              (3): ReLU(inplace=True)\n",
      "              (4): Sequential(\n",
      "                (0): Conv1dAuto(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
      "                (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              )\n",
      "            )\n",
      "            (activate): ReLU(inplace=True)\n",
      "            (shortcut): Sequential(\n",
      "              (0): Conv1d(64, 64, kernel_size=(1,), stride=(2,), bias=False)\n",
      "              (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (1): ResNet753Block(\n",
      "            (blocks): Sequential(\n",
      "              (0): Sequential(\n",
      "                (0): Conv1dAuto(64, 64, kernel_size=(7,), stride=(1,), padding=(3,), bias=False)\n",
      "                (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              )\n",
      "              (1): ReLU(inplace=True)\n",
      "              (2): Sequential(\n",
      "                (0): Conv1dAuto(64, 64, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
      "                (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              )\n",
      "              (3): ReLU(inplace=True)\n",
      "              (4): Sequential(\n",
      "                (0): Conv1dAuto(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
      "                (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              )\n",
      "            )\n",
      "            (activate): ReLU(inplace=True)\n",
      "            (shortcut): Sequential(\n",
      "              (0): Conv1d(64, 64, kernel_size=(1,), stride=(1,), bias=False)\n",
      "              (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (2): ResNet753Block(\n",
      "            (blocks): Sequential(\n",
      "              (0): Sequential(\n",
      "                (0): Conv1dAuto(64, 64, kernel_size=(7,), stride=(1,), padding=(3,), bias=False)\n",
      "                (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              )\n",
      "              (1): ReLU(inplace=True)\n",
      "              (2): Sequential(\n",
      "                (0): Conv1dAuto(64, 64, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
      "                (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              )\n",
      "              (3): ReLU(inplace=True)\n",
      "              (4): Sequential(\n",
      "                (0): Conv1dAuto(64, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
      "                (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              )\n",
      "            )\n",
      "            (activate): ReLU(inplace=True)\n",
      "            (shortcut): Sequential(\n",
      "              (0): Conv1d(64, 128, kernel_size=(1,), stride=(1,), bias=False)\n",
      "              (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (2): ResNetLayer(\n",
      "        (blocks): Sequential(\n",
      "          (0): ResNet753Block(\n",
      "            (blocks): Sequential(\n",
      "              (0): Sequential(\n",
      "                (0): Conv1dAuto(128, 128, kernel_size=(7,), stride=(1,), padding=(3,), bias=False)\n",
      "                (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              )\n",
      "              (1): ReLU(inplace=True)\n",
      "              (2): Sequential(\n",
      "                (0): Conv1dAuto(128, 128, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
      "                (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              )\n",
      "              (3): ReLU(inplace=True)\n",
      "              (4): Sequential(\n",
      "                (0): Conv1dAuto(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
      "                (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              )\n",
      "            )\n",
      "            (activate): ReLU(inplace=True)\n",
      "            (shortcut): Sequential(\n",
      "              (0): Conv1d(128, 128, kernel_size=(1,), stride=(1,), bias=False)\n",
      "              (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (1): ResNet753Block(\n",
      "            (blocks): Sequential(\n",
      "              (0): Sequential(\n",
      "                (0): Conv1dAuto(128, 128, kernel_size=(7,), stride=(1,), padding=(3,), bias=False)\n",
      "                (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              )\n",
      "              (1): ReLU(inplace=True)\n",
      "              (2): Sequential(\n",
      "                (0): Conv1dAuto(128, 128, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
      "                (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              )\n",
      "              (3): ReLU(inplace=True)\n",
      "              (4): Sequential(\n",
      "                (0): Conv1dAuto(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
      "                (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              )\n",
      "            )\n",
      "            (activate): ReLU(inplace=True)\n",
      "            (shortcut): Sequential(\n",
      "              (0): Conv1d(128, 128, kernel_size=(1,), stride=(1,), bias=False)\n",
      "              (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (2): ResNet753Block(\n",
      "            (blocks): Sequential(\n",
      "              (0): Sequential(\n",
      "                (0): Conv1dAuto(128, 128, kernel_size=(7,), stride=(1,), padding=(3,), bias=False)\n",
      "                (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              )\n",
      "              (1): ReLU(inplace=True)\n",
      "              (2): Sequential(\n",
      "                (0): Conv1dAuto(128, 128, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
      "                (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              )\n",
      "              (3): ReLU(inplace=True)\n",
      "              (4): Sequential(\n",
      "                (0): Conv1dAuto(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
      "                (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              )\n",
      "            )\n",
      "            (activate): ReLU(inplace=True)\n",
      "            (shortcut): Sequential(\n",
      "              (0): Conv1d(128, 128, kernel_size=(1,), stride=(1,), bias=False)\n",
      "              (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): ResnetDecoder(\n",
      "    (avg): AdaptiveAvgPool1d(output_size=(1,))\n",
      "    (decoder): Linear(in_features=128, out_features=27, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from resnet1d import ECGResNet50\n",
    "model = ECGResNet50(750, 27)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0741, -0.0539, -0.0556,  0.0414, -0.0480,  0.0719,  0.0662,  0.0294,\n",
       "          0.0207,  0.0109,  0.0641, -0.0077,  0.0524,  0.0706, -0.0367, -0.0092,\n",
       "          0.0353,  0.0855,  0.0653,  0.0440, -0.0372, -0.0086, -0.0448, -0.0148,\n",
       "          0.0657,  0.0576, -0.0504],\n",
       "        [-0.0741, -0.0539, -0.0556,  0.0414, -0.0480,  0.0719,  0.0662,  0.0294,\n",
       "          0.0207,  0.0109,  0.0641, -0.0077,  0.0524,  0.0706, -0.0367, -0.0092,\n",
       "          0.0353,  0.0855,  0.0653,  0.0440, -0.0372, -0.0086, -0.0448, -0.0148,\n",
       "          0.0657,  0.0576, -0.0504],\n",
       "        [-0.0741, -0.0539, -0.0556,  0.0414, -0.0480,  0.0719,  0.0662,  0.0294,\n",
       "          0.0207,  0.0109,  0.0641, -0.0077,  0.0524,  0.0706, -0.0367, -0.0092,\n",
       "          0.0353,  0.0855,  0.0653,  0.0440, -0.0372, -0.0086, -0.0448, -0.0148,\n",
       "          0.0657,  0.0576, -0.0504],\n",
       "        [-0.0741, -0.0539, -0.0556,  0.0414, -0.0480,  0.0719,  0.0662,  0.0294,\n",
       "          0.0207,  0.0109,  0.0641, -0.0077,  0.0524,  0.0706, -0.0367, -0.0092,\n",
       "          0.0353,  0.0855,  0.0653,  0.0440, -0.0372, -0.0086, -0.0448, -0.0148,\n",
       "          0.0657,  0.0576, -0.0504],\n",
       "        [-0.0741, -0.0539, -0.0556,  0.0414, -0.0480,  0.0719,  0.0662,  0.0294,\n",
       "          0.0207,  0.0109,  0.0641, -0.0077,  0.0524,  0.0706, -0.0367, -0.0092,\n",
       "          0.0353,  0.0855,  0.0653,  0.0440, -0.0372, -0.0086, -0.0448, -0.0148,\n",
       "          0.0657,  0.0576, -0.0504],\n",
       "        [-0.0741, -0.0539, -0.0556,  0.0414, -0.0480,  0.0719,  0.0662,  0.0294,\n",
       "          0.0207,  0.0109,  0.0641, -0.0077,  0.0524,  0.0706, -0.0367, -0.0092,\n",
       "          0.0353,  0.0855,  0.0653,  0.0440, -0.0372, -0.0086, -0.0448, -0.0148,\n",
       "          0.0657,  0.0576, -0.0504],\n",
       "        [-0.0741, -0.0539, -0.0556,  0.0414, -0.0480,  0.0719,  0.0662,  0.0294,\n",
       "          0.0207,  0.0109,  0.0641, -0.0077,  0.0524,  0.0706, -0.0367, -0.0092,\n",
       "          0.0353,  0.0855,  0.0653,  0.0440, -0.0372, -0.0086, -0.0448, -0.0148,\n",
       "          0.0657,  0.0576, -0.0504],\n",
       "        [-0.0741, -0.0539, -0.0556,  0.0414, -0.0480,  0.0719,  0.0662,  0.0294,\n",
       "          0.0207,  0.0109,  0.0641, -0.0077,  0.0524,  0.0706, -0.0367, -0.0092,\n",
       "          0.0353,  0.0855,  0.0653,  0.0440, -0.0372, -0.0086, -0.0448, -0.0148,\n",
       "          0.0657,  0.0576, -0.0504],\n",
       "        [-0.0741, -0.0539, -0.0556,  0.0414, -0.0480,  0.0719,  0.0662,  0.0294,\n",
       "          0.0207,  0.0109,  0.0641, -0.0077,  0.0524,  0.0706, -0.0367, -0.0092,\n",
       "          0.0353,  0.0855,  0.0653,  0.0440, -0.0372, -0.0086, -0.0448, -0.0148,\n",
       "          0.0657,  0.0576, -0.0504],\n",
       "        [-0.0741, -0.0539, -0.0556,  0.0414, -0.0480,  0.0719,  0.0662,  0.0294,\n",
       "          0.0207,  0.0109,  0.0641, -0.0077,  0.0524,  0.0706, -0.0367, -0.0092,\n",
       "          0.0353,  0.0855,  0.0653,  0.0440, -0.0372, -0.0086, -0.0448, -0.0148,\n",
       "          0.0657,  0.0576, -0.0504]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(dummy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from torchvision import datasets, models, transforms\n",
    "from myeval import agg_y_preds_bags, binary_acc, geometry_loss\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from snippets.pytorchtools import EarlyStopping\n",
    "from sklearn.model_selection import GroupKFold\n",
    "import time\n",
    "\n",
    "st = time.time()\n",
    "patience = 50\n",
    "batch_size= 512#65000\n",
    "\n",
    "saved_dir = '../saved/resnet50/'\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "trainDataset = torch.utils.data.Subset(signal_datasets_train, range(0,len(Signals_train), 1))\n",
    "testDataset = torch.utils.data.Subset(signal_datasets_test, range(0,len(Signals_test), 1))\n",
    "\n",
    "trainLoader = torch.utils.data.DataLoader(trainDataset, batch_size=batch_size, shuffle = True, pin_memory=True)#sampler = sampler)\n",
    "testLoader = torch.utils.data.DataLoader(testDataset, batch_size=65000, shuffle = False, pin_memory=True)\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01) \n",
    "#optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9) #\n",
    "# Decay LR by a factor of 0.1 every 100 epochs\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.1)\n",
    "\n",
    "#criterion_train = WeightedBCELoss(weights=weights_tensor, reduction='mean')\n",
    "#criterion_test = WeightedBCELoss(weights=weights_tensor, reduction='mean')\n",
    "\n",
    "class_weights_train = 1.0/np.sum(Data_labels_train, axis=0)\n",
    "class_weights_train = class_weights_train / np.sum(class_weights_train)\n",
    "class_weights_train = torch.Tensor(class_weights_train).to(device)\n",
    "criterion_train = nn.BCEWithLogitsLoss(pos_weight=pos_weight, reduction='mean') \n",
    "criterion_test = nn.BCEWithLogitsLoss(pos_weight=pos_weight, reduction='mean') \n",
    "\n",
    "losses_train = []\n",
    "losses_test = []\n",
    "\n",
    "avg_losses_train = []\n",
    "avg_losses_test = []\n",
    "\n",
    "\n",
    "early_stopping = EarlyStopping(patience, verbose=False, \n",
    "                              saved_dir=saved_dir, \n",
    "                              save_name='resnet50_'+run_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.01) \n",
    "#optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9) #\n",
    "# Decay LR by a factor of 0.1 every 100 epochs\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [get_name(label, Dx_map, Dx_map_unscored) for label in labels]\n",
    "\n",
    "assert len(labels) == 27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "684e725ccf6f4d4aa04c14984caca292",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "S59 266.72 min |\n",
      " Train Loss: 0.112691, Acc: 0.975, F: 0.265, Fbeta: 0.294, gbeta: 0.147, geo: 0.208, score: 0.338 |\n",
      " Valid Loss: 0.082327, Acc: 0.980, F: 0.384, Fbeta: 0.393, gbeta: 0.337, geo: 0.364, score: 0.457\n",
      " \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e851d12c25fe41f3958303957d63e0b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "S60 271.07 min |\n",
      " Train Loss: 0.112709, Acc: 0.975, F: 0.260, Fbeta: 0.289, gbeta: 0.144, geo: 0.204, score: 0.335 |\n",
      " Valid Loss: 0.082356, Acc: 0.980, F: 0.463, Fbeta: 0.471, gbeta: 0.414, geo: 0.442, score: 0.512\n",
      " \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef3ad4be9f34478f926301dcaeab1af6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "S61 275.43 min |\n",
      " Train Loss: 0.112701, Acc: 0.976, F: 0.269, Fbeta: 0.298, gbeta: 0.149, geo: 0.211, score: 0.350 |\n",
      " Valid Loss: 0.082391, Acc: 0.980, F: 0.464, Fbeta: 0.472, gbeta: 0.415, geo: 0.443, score: 0.506\n",
      " \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0af85d60af6a4e58a3bcbda1d826c83c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "S62 279.88 min |\n",
      " Train Loss: 0.112684, Acc: 0.976, F: 0.271, Fbeta: 0.300, gbeta: 0.151, geo: 0.213, score: 0.358 |\n",
      " Valid Loss: 0.082350, Acc: 0.981, F: 0.392, Fbeta: 0.401, gbeta: 0.343, geo: 0.371, score: 0.548\n",
      " \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f01033dffcc843e89716f6bbc996f4db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "S63 284.34 min |\n",
      " Train Loss: 0.112665, Acc: 0.976, F: 0.275, Fbeta: 0.305, gbeta: 0.153, geo: 0.216, score: 0.357 |\n",
      " Valid Loss: 0.082323, Acc: 0.982, F: 0.427, Fbeta: 0.436, gbeta: 0.380, geo: 0.407, score: 0.571\n",
      " \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e8c326083fd4c39b625e6f4525c6c19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "S64 288.75 min |\n",
      " Train Loss: 0.112630, Acc: 0.976, F: 0.278, Fbeta: 0.308, gbeta: 0.155, geo: 0.219, score: 0.366 |\n",
      " Valid Loss: 0.082275, Acc: 0.981, F: 0.355, Fbeta: 0.364, gbeta: 0.305, geo: 0.333, score: 0.532\n",
      " \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bee190137334b45916411f3611e71e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "S65 293.16 min |\n",
      " Train Loss: 0.112591, Acc: 0.976, F: 0.280, Fbeta: 0.311, gbeta: 0.157, geo: 0.221, score: 0.368 |\n",
      " Valid Loss: 0.082277, Acc: 0.981, F: 0.504, Fbeta: 0.512, gbeta: 0.454, geo: 0.482, score: 0.499\n",
      " \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd65069ccaf9407b96f6679159842f27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "S66 297.59 min |\n",
      " Train Loss: 0.112556, Acc: 0.976, F: 0.281, Fbeta: 0.312, gbeta: 0.157, geo: 0.221, score: 0.367 |\n",
      " Valid Loss: 0.082215, Acc: 0.982, F: 0.433, Fbeta: 0.441, gbeta: 0.384, geo: 0.411, score: 0.542\n",
      " \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b2f40f551114755b526fbaf0eb39970",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "S67 302.11 min |\n",
      " Train Loss: 0.112501, Acc: 0.976, F: 0.287, Fbeta: 0.318, gbeta: 0.161, geo: 0.226, score: 0.383 |\n",
      " Valid Loss: 0.082196, Acc: 0.981, F: 0.433, Fbeta: 0.440, gbeta: 0.383, geo: 0.410, score: 0.498\n",
      " \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d71e9b9699b4e40a4e0bda2c2ae63a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "S68 306.55 min |\n",
      " Train Loss: 0.112439, Acc: 0.976, F: 0.291, Fbeta: 0.323, gbeta: 0.163, geo: 0.230, score: 0.389 |\n",
      " Valid Loss: 0.082176, Acc: 0.981, F: 0.397, Fbeta: 0.405, gbeta: 0.347, geo: 0.375, score: 0.516\n",
      " \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "718a92ae347c4608ae92b314991cbd49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "S69 311.01 min |\n",
      " Train Loss: 0.112368, Acc: 0.977, F: 0.294, Fbeta: 0.326, gbeta: 0.166, geo: 0.233, score: 0.394 |\n",
      " Valid Loss: 0.082128, Acc: 0.978, F: 0.356, Fbeta: 0.360, gbeta: 0.304, geo: 0.331, score: 0.324\n",
      " \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c71f146aa0c44a44b307f1d3a388ef8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "S70 315.47 min |\n",
      " Train Loss: 0.112296, Acc: 0.977, F: 0.296, Fbeta: 0.329, gbeta: 0.167, geo: 0.234, score: 0.396 |\n",
      " Valid Loss: 0.082063, Acc: 0.982, F: 0.433, Fbeta: 0.442, gbeta: 0.384, geo: 0.412, score: 0.573\n",
      " \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d0fb77c9e6f4b86b7919c893c632985",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "S71 319.95 min |\n",
      " Train Loss: 0.112219, Acc: 0.977, F: 0.298, Fbeta: 0.331, gbeta: 0.168, geo: 0.236, score: 0.399 |\n",
      " Valid Loss: 0.082005, Acc: 0.982, F: 0.434, Fbeta: 0.442, gbeta: 0.385, geo: 0.413, score: 0.557\n",
      " \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5748c0f9606452f80eb97ec508db67b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "S72 324.40 min |\n",
      " Train Loss: 0.112132, Acc: 0.977, F: 0.302, Fbeta: 0.335, gbeta: 0.171, geo: 0.239, score: 0.407 |\n",
      " Valid Loss: 0.081994, Acc: 0.981, F: 0.356, Fbeta: 0.366, gbeta: 0.306, geo: 0.335, score: 0.521\n",
      " \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bfe46d304d4479192fde68a2b22b835",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "S73 328.85 min |\n",
      " Train Loss: 0.112045, Acc: 0.977, F: 0.304, Fbeta: 0.338, gbeta: 0.171, geo: 0.241, score: 0.405 |\n",
      " Valid Loss: 0.081880, Acc: 0.982, F: 0.363, Fbeta: 0.370, gbeta: 0.312, geo: 0.340, score: 0.526\n",
      " \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad1f1b9a28e04d57b3932acba4b13de5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "S74 333.33 min |\n",
      " Train Loss: 0.111950, Acc: 0.977, F: 0.309, Fbeta: 0.342, gbeta: 0.174, geo: 0.244, score: 0.406 |\n",
      " Valid Loss: 0.081782, Acc: 0.983, F: 0.473, Fbeta: 0.481, gbeta: 0.423, geo: 0.451, score: 0.587\n",
      " \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf8f4dfa2c4b4689aede40f5d970b6ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "S75 337.77 min |\n",
      " Train Loss: 0.111852, Acc: 0.977, F: 0.310, Fbeta: 0.344, gbeta: 0.176, geo: 0.246, score: 0.419 |\n",
      " Valid Loss: 0.081723, Acc: 0.982, F: 0.322, Fbeta: 0.331, gbeta: 0.273, geo: 0.301, score: 0.559\n",
      " \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94868106ae7d4fbdb3bcb83583359a8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "S76 342.20 min |\n",
      " Train Loss: 0.111752, Acc: 0.977, F: 0.312, Fbeta: 0.346, gbeta: 0.177, geo: 0.247, score: 0.417 |\n",
      " Valid Loss: 0.081624, Acc: 0.983, F: 0.326, Fbeta: 0.335, gbeta: 0.276, geo: 0.304, score: 0.569\n",
      " \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecb9a2126f1a487aa77a109f4dd8139b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "S77 346.57 min |\n",
      " Train Loss: 0.111660, Acc: 0.977, F: 0.308, Fbeta: 0.341, gbeta: 0.174, geo: 0.244, score: 0.413 |\n",
      " Valid Loss: 0.081526, Acc: 0.983, F: 0.397, Fbeta: 0.406, gbeta: 0.348, geo: 0.376, score: 0.553\n",
      " \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "320f9326f8de4c1296ddb239d9e68e34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "S78 350.94 min |\n",
      " Train Loss: 0.111557, Acc: 0.977, F: 0.314, Fbeta: 0.349, gbeta: 0.178, geo: 0.249, score: 0.422 |\n",
      " Valid Loss: 0.081434, Acc: 0.982, F: 0.401, Fbeta: 0.409, gbeta: 0.351, geo: 0.379, score: 0.569\n",
      " \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9ad3dbbf7be4eb6b421e88fae66a8f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "S79 355.32 min |\n",
      " Train Loss: 0.111449, Acc: 0.977, F: 0.320, Fbeta: 0.355, gbeta: 0.182, geo: 0.254, score: 0.427 |\n",
      " Valid Loss: 0.081372, Acc: 0.982, F: 0.323, Fbeta: 0.331, gbeta: 0.273, geo: 0.300, score: 0.531\n",
      " \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6dd23bf5a414420bf4a791933707124",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "S80 359.69 min |\n",
      " Train Loss: 0.111345, Acc: 0.977, F: 0.318, Fbeta: 0.353, gbeta: 0.181, geo: 0.253, score: 0.424 |\n",
      " Valid Loss: 0.081274, Acc: 0.982, F: 0.329, Fbeta: 0.336, gbeta: 0.278, geo: 0.306, score: 0.534\n",
      " \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bafb4ef3ff43440e8b79c6b48c2ff776",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "S81 364.05 min |\n",
      " Train Loss: 0.111232, Acc: 0.978, F: 0.323, Fbeta: 0.359, gbeta: 0.184, geo: 0.257, score: 0.431 |\n",
      " Valid Loss: 0.081207, Acc: 0.982, F: 0.326, Fbeta: 0.334, gbeta: 0.276, geo: 0.304, score: 0.552\n",
      " \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38462165a39e4284aa1f19f095bd2893",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "S82 368.50 min |\n",
      " Train Loss: 0.111122, Acc: 0.978, F: 0.325, Fbeta: 0.361, gbeta: 0.185, geo: 0.258, score: 0.433 |\n",
      " Valid Loss: 0.081123, Acc: 0.983, F: 0.401, Fbeta: 0.410, gbeta: 0.351, geo: 0.379, score: 0.594\n",
      " \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6517a3d22c874c63a787a3c92766cb9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "S83 372.90 min |\n",
      " Train Loss: 0.111013, Acc: 0.978, F: 0.323, Fbeta: 0.358, gbeta: 0.184, geo: 0.257, score: 0.433 |\n",
      " Valid Loss: 0.081025, Acc: 0.983, F: 0.367, Fbeta: 0.375, gbeta: 0.317, geo: 0.345, score: 0.577\n",
      " \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a10c8d4ef1e4336acf4b58e5bbf4225",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "S84 377.26 min |\n",
      " Train Loss: 0.110899, Acc: 0.978, F: 0.327, Fbeta: 0.362, gbeta: 0.186, geo: 0.260, score: 0.441 |\n",
      " Valid Loss: 0.080927, Acc: 0.982, F: 0.257, Fbeta: 0.264, gbeta: 0.206, geo: 0.233, score: 0.547\n",
      " \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "539f24428a9f4f2899038878fc08b23d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "S85 381.67 min |\n",
      " Train Loss: 0.110782, Acc: 0.978, F: 0.332, Fbeta: 0.368, gbeta: 0.189, geo: 0.264, score: 0.438 |\n",
      " Valid Loss: 0.080820, Acc: 0.983, F: 0.293, Fbeta: 0.300, gbeta: 0.242, geo: 0.269, score: 0.584\n",
      " \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cdb37d7da864ff3bc4599ea3ea93d83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "S86 386.09 min |\n",
      " Train Loss: 0.110664, Acc: 0.978, F: 0.332, Fbeta: 0.368, gbeta: 0.190, geo: 0.264, score: 0.445 |\n",
      " Valid Loss: 0.080740, Acc: 0.983, F: 0.366, Fbeta: 0.374, gbeta: 0.316, geo: 0.344, score: 0.570\n",
      " \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c26fa53ac64a410fb77b443f88e07e87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "S87 390.51 min |\n",
      " Train Loss: 0.110575, Acc: 0.977, F: 0.321, Fbeta: 0.357, gbeta: 0.182, geo: 0.255, score: 0.422 |\n",
      " Valid Loss: 0.080681, Acc: 0.983, F: 0.291, Fbeta: 0.300, gbeta: 0.241, geo: 0.269, score: 0.590\n",
      " \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f19efd3e378c4a659f359ef9d6e3b515",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "S88 394.97 min |\n",
      " Train Loss: 0.110453, Acc: 0.978, F: 0.336, Fbeta: 0.372, gbeta: 0.192, geo: 0.268, score: 0.452 |\n",
      " Valid Loss: 0.080588, Acc: 0.983, F: 0.291, Fbeta: 0.300, gbeta: 0.241, geo: 0.269, score: 0.582\n",
      " \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d220066e5c848e18932ba02305696f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "S89 399.44 min |\n",
      " Train Loss: 0.110330, Acc: 0.978, F: 0.339, Fbeta: 0.376, gbeta: 0.194, geo: 0.270, score: 0.455 |\n",
      " Valid Loss: 0.080503, Acc: 0.983, F: 0.294, Fbeta: 0.302, gbeta: 0.243, geo: 0.271, score: 0.597\n",
      " \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae537a82cfd54ea98a1ab0c25ce72cfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "S90 403.85 min |\n",
      " Train Loss: 0.110210, Acc: 0.978, F: 0.339, Fbeta: 0.375, gbeta: 0.194, geo: 0.270, score: 0.452 |\n",
      " Valid Loss: 0.080449, Acc: 0.983, F: 0.296, Fbeta: 0.303, gbeta: 0.244, geo: 0.272, score: 0.559\n",
      " \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91755f124c064046a10a686ff77471e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "S91 408.33 min |\n",
      " Train Loss: 0.110088, Acc: 0.978, F: 0.342, Fbeta: 0.379, gbeta: 0.196, geo: 0.273, score: 0.457 |\n",
      " Valid Loss: 0.080383, Acc: 0.982, F: 0.295, Fbeta: 0.302, gbeta: 0.244, geo: 0.271, score: 0.530\n",
      " \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "219eef370379403e864ad308ec9c71a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "S92 412.83 min |\n",
      " Train Loss: 0.109978, Acc: 0.978, F: 0.339, Fbeta: 0.377, gbeta: 0.194, geo: 0.270, score: 0.454 |\n",
      " Valid Loss: 0.080284, Acc: 0.983, F: 0.295, Fbeta: 0.303, gbeta: 0.244, geo: 0.272, score: 0.592\n",
      " \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "805b568641c343078070a11ccc903e82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "S93 417.33 min |\n",
      " Train Loss: 0.109855, Acc: 0.978, F: 0.345, Fbeta: 0.382, gbeta: 0.197, geo: 0.275, score: 0.463 |\n",
      " Valid Loss: 0.080206, Acc: 0.983, F: 0.330, Fbeta: 0.338, gbeta: 0.279, geo: 0.307, score: 0.546\n",
      " \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ecaa0a976414c68bb7af6166c1bdb6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_trains_tensor = None\n",
    "y_tests_tensor = None\n",
    "for epoch in range(59, 5000):\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    running_loss_train = 0.0\n",
    "\n",
    "    y_trains = [] # ground truth\n",
    "    output_trains = [] # output\n",
    "    for k, (X_train, y_train) in tqdm(enumerate(trainLoader)):\n",
    "        y_train = y_train.to(device)\n",
    "        X_train = X_train.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output_train = model(X_train)\n",
    "        output_trains.append(output_train.cpu())\n",
    "        \n",
    "        loss_train = criterion_train(output_train, y_train)\n",
    "        losses_train.append(loss_train.item())\n",
    "        \n",
    "        avg_loss_train = np.average(losses_train)\n",
    "        avg_losses_train.append(avg_loss_train)\n",
    "    \n",
    "        \n",
    "        \n",
    "        if np.mod(k, 100) == 0:\n",
    "            writer.add_scalar('training loss',\n",
    "            avg_loss_train,\n",
    "            epoch * (len(Data_labels_train)//batch_size//100+1) + k//100)\n",
    "        \n",
    "        y_trains.append(y_train.cpu())\n",
    "            \n",
    "        loss_train.backward()\n",
    "        optimizer.step()\n",
    "            \n",
    "    scheduler.step()\n",
    "\n",
    "    y_tests = [] # ground truth\n",
    "    output_tests = [] # output\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        \n",
    "        for X_test, y_test in testLoader:  \n",
    "            y_test = y_test.to(device)\n",
    "            X_test = X_test.to(device)\n",
    "            output_test = model(X_test)\n",
    "\n",
    "            loss_test = criterion_test(output_test, y_test)\n",
    "            losses_test.append(loss_test.item())\n",
    "\n",
    "            output_tests.append(output_test.cpu())\n",
    "            y_tests.append(y_test.cpu())\n",
    "\n",
    "        avg_loss_test = np.average(losses_test)\n",
    "        avg_losses_test.append(avg_loss_test)\n",
    "\n",
    "        writer.add_scalar('testing loss',\n",
    "                avg_loss_test,\n",
    "                epoch* (len(Data_labels_test)//batch_size+1))\n",
    "\n",
    "\n",
    "\n",
    "    y_trains_tensor = torch.cat(y_trains, axis=0) # ground truth\n",
    "    y_tests_tensor = torch.cat(y_tests, axis=0) # ground truth\n",
    "\n",
    "    output_trains = torch.cat(output_trains, axis=0) \n",
    "    y_train_preds = torch.sigmoid(output_trains)\n",
    "\n",
    "    output_tests = torch.cat(output_tests, axis=0)\n",
    "    y_test_preds = torch.sigmoid(output_tests)\n",
    "\n",
    "    #output_trains = torch.cat(output_trains, axis=0)\n",
    "#     y_train_preds_max, y_train_preds_mean, _ = agg_y_preds_bags(y_train_preds, bag_size=n_segments)\n",
    "#     y_test_preds_max, y_test_preds_mean, _ = agg_y_preds_bags(y_test_preds, bag_size=n_segments)\n",
    "#     _, _, y_trains = agg_y_preds_bags(y_trains, bag_size=n_segments)\n",
    "#     _, _, y_tests = agg_y_preds_bags(y_tests, bag_size=n_segments)\n",
    "\n",
    "\n",
    "    for class_idx in range(N_CLASS):\n",
    "        add_pr_curve_tensorboard(class_idx, y_trains_tensor, y_train_preds, names, global_step=epoch, prefix='train')\n",
    "        add_pr_curve_tensorboard(class_idx, y_tests_tensor, y_test_preds, names, global_step=epoch, prefix='test')\n",
    "\n",
    "\n",
    "\n",
    "    acc, fmeasure, fbeta, gbeta = binary_acc(y_train_preds, y_trains_tensor)           \n",
    "    acc2, fmeasure2, fbeta2, gbeta2 = binary_acc(y_test_preds, y_tests_tensor)\n",
    "    geometry = geometry_loss(fbeta, gbeta)\n",
    "    geometry2 = geometry_loss(fbeta2, gbeta2)\n",
    "    \n",
    "#     output_str = 'S{} {:.2f} min |\\n Train Loss: {:.6f}, Acc: {:.3f}, F: {:.3f}, Fbeta: {:.3f}, gbeta: {:.3f}, geo: {:.3f}, score: {:.3f} |\\n Valid Loss: {:.6f}, Acc: {:.3f}, F: {:.3f}, Fbeta: {:.3f}, gbeta: {:.3f}, geo: {:.3f}, score: {:.3f}\\n '.format(\n",
    "#         epoch, (time.time()-st)/60,\n",
    "#         avg_loss_train, acc, fmeasure, fbeta, gbeta, geometry, np.nan,\n",
    "#         avg_loss_test, acc2, fmeasure2, fbeta2, gbeta2, geometry2, np.nan)\n",
    "#     print(output_str)\n",
    "    score = compute_score(np.round(y_train_preds.data.numpy()), np.round(y_trains_tensor.data.numpy()), weights)\n",
    "    score2 = compute_score(np.round(y_test_preds.data.numpy()), np.round(y_tests_tensor.data.numpy()), weights)\n",
    "    output_str = 'S{} {:.2f} min |\\n Train Loss: {:.6f}, Acc: {:.3f}, F: {:.3f}, Fbeta: {:.3f}, gbeta: {:.3f}, geo: {:.3f}, score: {:.3f} |\\n Valid Loss: {:.6f}, Acc: {:.3f}, F: {:.3f}, Fbeta: {:.3f}, gbeta: {:.3f}, geo: {:.3f}, score: {:.3f}\\n '.format(\n",
    "        epoch, (time.time()-st)/60,\n",
    "        avg_loss_train, acc, fmeasure, fbeta, gbeta, geometry, score,\n",
    "        avg_loss_test, acc2, fmeasure2, fbeta2, gbeta2, geometry2, score2)\n",
    "    \n",
    "    print(output_str)\n",
    "\n",
    "    with open(saved_dir+'loss_{}.txt'.format(run_name), 'a') as f:\n",
    "        print(output_str, file=f)\n",
    "\n",
    "    early_stopping(avg_loss_test, model)\n",
    "\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping\")\n",
    "        break\n",
    "\n",
    "\n",
    "#     output_string = 'AUROC|AUPRC|Accuracy|F-measure|Fbeta-measure|Gbeta-measure|Geomotry\\n{:.3f}|{:.3f}|{:.3f}|{:.3f}|{:.3f}|{:.3f}|{:.3f}'.format(auroc2,auprc2,acc2,fmeasure2,fbeta2,gbeta2,geometry2)\n",
    "#     print(output_string)     \n",
    "#     with open(saved_dir+'score'+ str(i)+ '_epoch' + str(epoch) + '.txt', 'w') as f:\n",
    "#         f.write(output_string)\n",
    "\n",
    "#     avg_losses_train = np.array(avg_losses_train)\n",
    "#     avg_losses_test = np.array(avg_losses_test)\n",
    "\n",
    "#     np.save(saved_dir + 'avg_losses_train' + str(i) + '_epoch' + str(epoch), avg_losses_train)\n",
    "#     np.save(saved_dir + 'avg_losses_test' + str(i) + '_epoch' + str(epoch), avg_losses_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import multilabel_confusion_matrix, confusion_matrix\n",
    "from global_vars import labels\n",
    "\n",
    "cf_matrices = multilabel_confusion_matrix(y_trains_tensor.data.numpy(), np.round(y_train_preds.data.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label, cf_matrix in zip(labels, cf_matrices):\n",
    "    print(get_name(label, Dx_map, Dx_map_unscored))\n",
    "    print(cf_matrix)\n",
    "    print('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_trains_tensor.data.numpy()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 (physioNet)",
   "language": "python",
   "name": "physionet"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
