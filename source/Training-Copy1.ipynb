{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np, os, sys\n",
    "from tqdm.notebook import tqdm    \n",
    "from manipulations import get_classes, get_classes_from_header, get_Fs_from_header, load_challenge_data\n",
    "from sklearn.model_selection import KFold, StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_CLASS = 27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_RR = 750 # 60 beats/min => 60 beats/60 s ==> beat/1s ==> 500 samples / beat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from manipulations import get_abbr, get_name\n",
    "from global_vars import labels, Dx_map, Dx_map_unscored\n",
    "first_idx = {scored_code: None for scored_code in list(Dx_map['SNOMED CT Code'])}\n",
    "first_idx_unscored = {unscored_code: None for unscored_code in list(Dx_map_unscored['SNOMED CT Code'])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from global_vars import labels, normal_class, equivalent_mapping\n",
    "normal_idx = np.argwhere(labels==int(normal_class))\n",
    "def get_scored_class(code, labels):\n",
    "    return [1 if label in code else 0 for label in labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "    \n",
    "#     Datas = []\n",
    "#     Header_datas = []\n",
    "#     Classes = []\n",
    "#     dataset_idx = {}\n",
    "#     dataset_train_idx = {}\n",
    "#     dataset_test_idx = {}\n",
    "#     global_idx = 0\n",
    "#     datasets = [1,2,3,4,5,6]\n",
    "#     for dataset in datasets:\n",
    "#         print('Dataset ', dataset)\n",
    "#         # Parse arguments.\n",
    "#         if len(sys.argv) != 3:\n",
    "#             raise Exception('Include the input and output directories as arguments, e.g., python driver.py input output.')\n",
    "\n",
    "#         input_directory = '../NewData/{}/'.format(dataset)\n",
    "#         output_directory = '../Output/'\n",
    "\n",
    "#         # Find files.\n",
    "#         input_files = []\n",
    "#         for f in os.listdir(input_directory):\n",
    "#             if os.path.isfile(os.path.join(input_directory, f)) and not f.lower().startswith('.') and f.lower().endswith('mat'):\n",
    "#                 input_files.append(f)\n",
    "\n",
    "#         if not os.path.isdir(output_directory):\n",
    "#             os.mkdir(output_directory)\n",
    "\n",
    "#         classes=get_classes(input_directory,input_files)\n",
    "\n",
    "#         num_files = len(input_files)\n",
    "#         datas = []\n",
    "#         header_datas = []\n",
    "#         dataset_idx[dataset] = []\n",
    "#         for i, f in tqdm(enumerate(input_files)):\n",
    "#             #print('    {}/{}...'.format(i+1, num_files), f)\n",
    "#             tmp_input_file = os.path.join(input_directory,f)\n",
    "#             data,header_data = load_challenge_data(tmp_input_file)\n",
    "#             datas.append(data[:,1000:7000])\n",
    "#             header_datas.append(header_data)\n",
    "#             dataset_idx[dataset].append(global_idx)\n",
    "#             global_idx += 1\n",
    "\n",
    "#         Datas += datas\n",
    "#         Header_datas += header_datas\n",
    "#         Classes += classes\n",
    "        \n",
    "#         kf = KFold(5)\n",
    "#         train_idx, test_idx = next(kf.split(datas))\n",
    "        \n",
    "#         dataset_train_idx[dataset] = train_idx\n",
    "#         dataset_test_idx[dataset] = test_idx\n",
    "#         print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datasets = [1,2,3,4,5,6]\n",
    "# all_train_idx = []\n",
    "# for dataset in datasets:\n",
    "#     all_train_idx.extend(dataset_train_idx[dataset])\n",
    "    \n",
    "# all_test_idx = []\n",
    "# for dataset in datasets:\n",
    "#     all_test_idx.extend(dataset_test_idx[dataset])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from signal_processing import myfilter, main_QRST\n",
    "# Codes = []\n",
    "# Q_locs = []\n",
    "# idxes = []\n",
    "# for idx in tqdm(range(0, len(Datas))): \n",
    "#     codes = get_classes_from_header(Header_datas[idx])\n",
    "#     names = ', '.join([get_name(int(code), Dx_map, Dx_map_unscored) for code in codes])\n",
    "\n",
    "#     filtered_Data = myfilter(Datas[idx], 500, vis=False)\n",
    "\n",
    "#     # get the lead to apply Pan Tomkins\n",
    "#     Q_loc = main_QRST(filtered_Data, idx, '', '', names, fig2=False)\n",
    "\n",
    "#     # store\n",
    "#     Codes.append(codes)\n",
    "#     Q_locs.append(Q_loc)\n",
    "\n",
    "#     idxes.append(idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from signal_processing import myfilter, extract_QRST, main_QRST\n",
    "# idx = 1511#37581#2932#6884 #6884 #6951 #227#8074 #325#158#325#34#17#8659#4#18#61#10927#6886# 8659\n",
    "# codes = get_classes_from_header(Header_datas[idx])\n",
    "# names = ', '.join([get_name(int(code), Dx_map, Dx_map_unscored) for code in codes])\n",
    "# filtered_Data = myfilter(Datas[idx], 500, vis=False)\n",
    "# #filtered_Data = myfilter(Datas[idx][:,-4000:-1000], 500, vis=False)\n",
    "# Q_loc = main_QRST(filtered_Data, idx, '', '', names, fig2=False, verbose=True, vis=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# with open('../saved/newData_idxes_1000_7000_peakdist100.pkl', 'wb') as idxes_file:\n",
    "#     pickle.dump(idxes, idxes_file)\n",
    "\n",
    "# with open('../saved/newData_Codes_1000_7000_peakdist100.pkl', 'wb') as Codes_file:\n",
    "#     pickle.dump(Codes, Codes_file)\n",
    "\n",
    "# with open('../saved/newData_Q_locs_1000_7000_peakdist100.pkl', 'wb') as Q_locs_file:\n",
    "#     pickle.dump(Q_locs, Q_locs_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# Q_locs = None\n",
    "# with open('../saved/newData_Q_locs_1000_7000_peakdist100.pkl', 'rb') as Q_locs_file:\n",
    "#     Q_locs = pickle.load(Q_locs_file)\n",
    "\n",
    "# Codes = None\n",
    "# with open('../saved/newData_Codes_1000_7000_peakdist100.pkl', 'rb') as Codes_file:\n",
    "#     Codes = pickle.load(Codes_file)\n",
    "\n",
    "# idxes = None\n",
    "# with open('../saved/newData_idxes_1000_7000_peakdist100.pkl', 'rb') as idxes_file:\n",
    "#     idxes = pickle.load(idxes_file)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Codes = []\n",
    "# for idx in tqdm(range(0, len(Datas))): \n",
    "#     codes = get_classes_from_header(Header_datas[idx])\n",
    "#     # store\n",
    "#     Codes.append(codes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_labels = np.array([get_scored_class(codes, labels) for codes in Codes])\n",
    "# print(data_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# key_idxes = []\n",
    "# for key in equivalent_mapping.keys():\n",
    "#     print(key)\n",
    "#     key_idx = np.argwhere(labels==int(key)).flatten()[0]\n",
    "#     key_idxes.append(key_idx)\n",
    "#     val_idx = np.argwhere(labels==int(equivalent_mapping[key])).flatten()[0]\n",
    "#     key_pos = np.argwhere(data_labels[:,key_idx]==1).flatten()\n",
    "#     val_pos = np.argwhere(data_labels[:,val_idx]==1).flatten()\n",
    "#     data_labels[key_pos,val_idx] = 1\n",
    "#     data_labels[val_pos,key_idx] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# non_empty_idx = np.argwhere(np.sum(data_labels, axis=1)!=0).flatten()\n",
    "# empty_idx = np.argwhere(np.sum(data_labels, axis=1)==0).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = []\n",
    "# Data_labels_train = []\n",
    "# Idxes_train = []\n",
    "# Idxes_dict_train = {}\n",
    "# ct = 0\n",
    "# for i in tqdm(all_train_idx):\n",
    "#     Q_loc = Q_locs[i]\n",
    "#     RR_avg = np.median([Q_loc[k+1] - Q_loc[k] for k in range(len(Q_loc)-1)])\n",
    "#     RR_th = (0.3 * RR_avg, 3 * RR_avg)\n",
    "    \n",
    "#     ks = [k for k in range(len(Q_loc)-1) if Q_loc[k+1] - Q_loc[k] > RR_th[0] \n",
    "#               and Q_loc[k+1] - Q_loc[k] < RR_th[1]]\n",
    "    \n",
    "    \n",
    "#     Idxes_dict_train[i] = []\n",
    "#     for k in ks:\n",
    "#         Data_labels_train.append(get_scored_class(Codes[i], labels))\n",
    "#         X_train.append(Datas[i][:,1000+Q_loc[k]:1000+Q_loc[k+1]])\n",
    "#         Idxes_train.append(i)\n",
    "#         Idxes_dict_train[i].append(ct)\n",
    "        \n",
    "#     ct += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test = []\n",
    "# Data_labels_test = []\n",
    "# Idxes_test = []\n",
    "# Idxes_dict_test = {}\n",
    "# ct = 0\n",
    "# for i in tqdm(all_test_idx):\n",
    "        \n",
    "#     Q_loc = Q_locs[i]\n",
    "#     RR_avg = np.median([Q_loc[k+1] - Q_loc[k] for k in range(len(Q_loc)-1)])\n",
    "#     RR_th = (0.3 * RR_avg, 3 * RR_avg)\n",
    "    \n",
    "#     ks = [k for k in range(len(Q_loc)-1) if Q_loc[k+1] - Q_loc[k] > RR_th[0] \n",
    "#               and Q_loc[k+1] - Q_loc[k] < RR_th[1]]\n",
    "    \n",
    "    \n",
    "#     Idxes_dict_test[i] = []\n",
    "#     for k in ks:\n",
    "#         Data_labels_test.append(get_scored_class(Codes[i], labels))\n",
    "#         X_test.append(Datas[i][:,1000+Q_loc[k]:1000+Q_loc[k+1]])\n",
    "#         Idxes_test.append(i)\n",
    "#         Idxes_dict_test[i].append(ct)\n",
    "        \n",
    "#     ct += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Signals_train = np.zeros((len(X_train),12,MAX_RR))\n",
    "# for i in range(len(X_train)):\n",
    "#     Signals_train[i,:,:min(len(X_train[i][0]),MAX_RR)] = X_train[i][:,:MAX_RR]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Signals_test = np.zeros((len(X_test),12,MAX_RR))\n",
    "# for i in range(len(X_test)):\n",
    "#     Signals_test[i,:,:min(len(X_test[i][0]),MAX_RR)] = X_test[i][:,:MAX_RR]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save('../saved/Signals_train.npy', Signals_train)\n",
    "# np.save('../saved/Signals_test.npy', Signals_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# with open('../saved/Data_labels_train.pkl', 'wb') as Data_labels_train_file:\n",
    "#     pickle.dump(Data_labels_train, Data_labels_train_file)\n",
    "\n",
    "# with open('../saved/Data_labels_test.pkl', 'wb') as Data_labels_test_file:\n",
    "#     pickle.dump(Data_labels_test, Data_labels_test_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# On y va"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "Signals_train = np.load('../saved/Signals_train.npy')\n",
    "Signals_test = np.load('../saved/Signals_test.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('../saved/Data_labels_train.pkl', 'rb') as Data_labels_train_file:\n",
    "    Data_labels_train = pickle.load(Data_labels_train_file)\n",
    "\n",
    "with open('../saved/Data_labels_test.pkl', 'rb') as Data_labels_test_file:\n",
    "    Data_labels_test = pickle.load(Data_labels_test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_name = 'resnet50_minibatch_adam'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# default `log_dir` is \"runs\" - we'll be more specific here\n",
    "writer = SummaryWriter('runs/{}'.format(run_name))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:0')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def weighted_binary_cross_entropy2(sigmoid_x, y, weighted_matrix, weight=None, reduction=None):\n",
    "    \"\"\"\n",
    "    Aha this is correct!\n",
    "    sigmoid_x = nn.Sigmoid()(x)\n",
    "    Args:\n",
    "        sigmoid_x: predicted probability of size [N,C], N sample and C Class. Eg. Must be in range of [0,1]\n",
    "        targets: true value, one-hot-like vector of size [N,C]\n",
    "        pos_weight: Weight for postive sample\n",
    "    \"\"\"\n",
    "    if not (y.size() == sigmoid_x.size()):\n",
    "        raise ValueError(\"Target size ({}) must be the same as input size ({})\".format(y.size(), sigmoid_x.size()))\n",
    "   \n",
    "    #print(\"y.size(), sigmoid_x.size()\", y.size(), sigmoid_x.size())\n",
    "    sigmoid_x = torch.clamp(sigmoid_x,min=1e-7,max=1-1e-7) \n",
    "    loss = - torch.matmul(y*sigmoid_x.log() + (1-y)*(1-sigmoid_x).log(), weighted_matrix)\n",
    "    \n",
    "    if weight is not None:\n",
    "        loss = loss * weight\n",
    "        \n",
    "    if reduction is None:\n",
    "        return loss\n",
    "    elif reduction == 'mean':\n",
    "        return loss.mean()\n",
    "    elif reduction == 'sum':\n",
    "        return loss.sum()\n",
    "    return None\n",
    "    \n",
    "class WeightedBCELoss(nn.Module):\n",
    "    def __init__(self, weights, PosWeightIsDynamic= False, WeightIsDynamic= False, \n",
    "                 reduction='mean'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            pos_weight = Weight for postive samples. Size [1,C]\n",
    "            weight = Weight for Each class. Size [1,C]\n",
    "            PosWeightIsDynamic: If True, the pos_weight is computed on each batch. If pos_weight is None, then it remains None.\n",
    "            WeightIsDynamic: If True, the weight is computed on each batch. If weight is None, then it remains None.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.register_buffer('weights', weights)\n",
    "        self.reduction = reduction\n",
    "        self.PosWeightIsDynamic = PosWeightIsDynamic\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        if self.PosWeightIsDynamic:\n",
    "            positive_counts = target.sum(dim=0)\n",
    "            nBatch = len(target)\n",
    "            self.pos_weight = (nBatch - positive_counts)/(positive_counts +1e-5)\n",
    "\n",
    "\n",
    "        return weighted_binary_cross_entropy2(input, target,\n",
    "                                             weighted_matrix=self.weights,\n",
    "                                             reduction=self.reduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "Signal_12leads_train = np.transpose(Signals_train, (1,0,2))\n",
    "Signal_12leads_test= np.transpose(Signals_test, (1,0,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "\n",
    "import os\n",
    "class SignalDataset(Dataset):\n",
    "\n",
    "    def __init__(self, signals, labels):\n",
    "        self.signals = signals\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.signals)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        sample =(torch.cat([torch.Tensor(np.array([self.signals[channel,idx]]).transpose()) for channel in range(12)], axis=1), \n",
    "                  torch.Tensor(self.labels[idx]))\n",
    "\n",
    "        return sample\n",
    "    \n",
    "signal_datasets_train = SignalDataset(Signal_12leads_train, np.array(Data_labels_train)[:,:N_CLASS])\n",
    "signal_datasets_test = SignalDataset(Signal_12leads_test, np.array(Data_labels_test)[:,:N_CLASS])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.array(Data_labels_train).shape[1] == N_CLASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([750, 12])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "signal_datasets_train[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from global_vars import weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_tensor = torch.Tensor(weights).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation.evaluate_12ECG_score import compute_modified_confusion_matrix, compute_challenge_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_score(labels, outputs, weights, normal_index=normal_idx):\n",
    "    num_recordings, num_classes = np.shape(labels)\n",
    "    # Compute the observed score.\n",
    "    A = compute_modified_confusion_matrix(labels, outputs)\n",
    "    observed_score = np.nansum(weights * A)\n",
    "\n",
    "    # Compute the score for the model that always chooses the correct label(s).\n",
    "    correct_outputs = labels\n",
    "    A = compute_modified_confusion_matrix(labels, correct_outputs)\n",
    "    correct_score = np.nansum(weights * A)\n",
    "\n",
    "    # Compute the score for the model that always chooses the normal class.\n",
    "    inactive_outputs = np.zeros((num_recordings, num_classes), dtype=np.bool)\n",
    "    inactive_outputs[:, normal_index] = 1\n",
    "    A = compute_modified_confusion_matrix(labels, inactive_outputs)\n",
    "    inactive_score = np.nansum(weights * A)\n",
    "\n",
    "    if correct_score != inactive_score:\n",
    "        normalized_score = float(observed_score - inactive_score) / float(correct_score - inactive_score)\n",
    "    else:\n",
    "        normalized_score = float('nan')\n",
    "\n",
    "    return normalized_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_pr_curve_tensorboard(class_index, test_probs, test_preds, names, global_step, prefix):\n",
    "    '''\n",
    "    Takes in a \"class_index\" from 0 to 9 and plots the corresponding\n",
    "    precision-recall curve\n",
    "    '''\n",
    "\n",
    "    writer.add_pr_curve(prefix + '_' + names[class_index],\n",
    "                        test_preds[:,class_index],\n",
    "                        test_probs[:,class_index],\n",
    "                        global_step=global_step)\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_weight = np.ones(N_CLASS) * 2\n",
    "pos_weight = torch.Tensor(pos_weight).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "dummy = torch.ones((10, 750, 48))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ECGResNet50(\n",
      "  (encoder): ResNetEncoder(\n",
      "    (gate): Sequential(\n",
      "      (0): Conv1d(750, 64, kernel_size=(9,), stride=(1,), bias=False)\n",
      "      (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): MaxPool1d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (blocks): ModuleList(\n",
      "      (0): ResNetLayer(\n",
      "        (blocks): Sequential(\n",
      "          (0): ResNet753Block(\n",
      "            (blocks): Sequential(\n",
      "              (0): Sequential(\n",
      "                (0): Conv1dAuto(64, 64, kernel_size=(7,), stride=(1,), padding=(3,), bias=False)\n",
      "                (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              )\n",
      "              (1): ReLU(inplace=True)\n",
      "              (2): Sequential(\n",
      "                (0): Conv1dAuto(64, 64, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
      "                (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              )\n",
      "              (3): ReLU(inplace=True)\n",
      "              (4): Sequential(\n",
      "                (0): Conv1dAuto(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
      "                (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              )\n",
      "            )\n",
      "            (activate): ReLU(inplace=True)\n",
      "            (shortcut): Sequential(\n",
      "              (0): Conv1d(64, 64, kernel_size=(1,), stride=(1,), bias=False)\n",
      "              (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (1): ResNet753Block(\n",
      "            (blocks): Sequential(\n",
      "              (0): Sequential(\n",
      "                (0): Conv1dAuto(64, 64, kernel_size=(7,), stride=(1,), padding=(3,), bias=False)\n",
      "                (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              )\n",
      "              (1): ReLU(inplace=True)\n",
      "              (2): Sequential(\n",
      "                (0): Conv1dAuto(64, 64, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
      "                (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              )\n",
      "              (3): ReLU(inplace=True)\n",
      "              (4): Sequential(\n",
      "                (0): Conv1dAuto(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
      "                (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              )\n",
      "            )\n",
      "            (activate): ReLU(inplace=True)\n",
      "            (shortcut): Sequential(\n",
      "              (0): Conv1d(64, 64, kernel_size=(1,), stride=(1,), bias=False)\n",
      "              (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): ResNetLayer(\n",
      "        (blocks): Sequential(\n",
      "          (0): ResNet753Block(\n",
      "            (blocks): Sequential(\n",
      "              (0): Sequential(\n",
      "                (0): Conv1dAuto(64, 64, kernel_size=(7,), stride=(1,), padding=(3,), bias=False)\n",
      "                (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              )\n",
      "              (1): ReLU(inplace=True)\n",
      "              (2): Sequential(\n",
      "                (0): Conv1dAuto(64, 64, kernel_size=(5,), stride=(2,), padding=(2,), bias=False)\n",
      "                (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              )\n",
      "              (3): ReLU(inplace=True)\n",
      "              (4): Sequential(\n",
      "                (0): Conv1dAuto(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
      "                (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              )\n",
      "            )\n",
      "            (activate): ReLU(inplace=True)\n",
      "            (shortcut): Sequential(\n",
      "              (0): Conv1d(64, 64, kernel_size=(1,), stride=(2,), bias=False)\n",
      "              (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (1): ResNet753Block(\n",
      "            (blocks): Sequential(\n",
      "              (0): Sequential(\n",
      "                (0): Conv1dAuto(64, 64, kernel_size=(7,), stride=(1,), padding=(3,), bias=False)\n",
      "                (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              )\n",
      "              (1): ReLU(inplace=True)\n",
      "              (2): Sequential(\n",
      "                (0): Conv1dAuto(64, 64, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
      "                (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              )\n",
      "              (3): ReLU(inplace=True)\n",
      "              (4): Sequential(\n",
      "                (0): Conv1dAuto(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
      "                (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              )\n",
      "            )\n",
      "            (activate): ReLU(inplace=True)\n",
      "            (shortcut): Sequential(\n",
      "              (0): Conv1d(64, 64, kernel_size=(1,), stride=(1,), bias=False)\n",
      "              (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (2): ResNet753Block(\n",
      "            (blocks): Sequential(\n",
      "              (0): Sequential(\n",
      "                (0): Conv1dAuto(64, 64, kernel_size=(7,), stride=(1,), padding=(3,), bias=False)\n",
      "                (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              )\n",
      "              (1): ReLU(inplace=True)\n",
      "              (2): Sequential(\n",
      "                (0): Conv1dAuto(64, 64, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
      "                (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              )\n",
      "              (3): ReLU(inplace=True)\n",
      "              (4): Sequential(\n",
      "                (0): Conv1dAuto(64, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
      "                (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              )\n",
      "            )\n",
      "            (activate): ReLU(inplace=True)\n",
      "            (shortcut): Sequential(\n",
      "              (0): Conv1d(64, 128, kernel_size=(1,), stride=(1,), bias=False)\n",
      "              (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (2): ResNetLayer(\n",
      "        (blocks): Sequential(\n",
      "          (0): ResNet753Block(\n",
      "            (blocks): Sequential(\n",
      "              (0): Sequential(\n",
      "                (0): Conv1dAuto(128, 128, kernel_size=(7,), stride=(1,), padding=(3,), bias=False)\n",
      "                (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              )\n",
      "              (1): ReLU(inplace=True)\n",
      "              (2): Sequential(\n",
      "                (0): Conv1dAuto(128, 128, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
      "                (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              )\n",
      "              (3): ReLU(inplace=True)\n",
      "              (4): Sequential(\n",
      "                (0): Conv1dAuto(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
      "                (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              )\n",
      "            )\n",
      "            (activate): ReLU(inplace=True)\n",
      "            (shortcut): Sequential(\n",
      "              (0): Conv1d(128, 128, kernel_size=(1,), stride=(1,), bias=False)\n",
      "              (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (1): ResNet753Block(\n",
      "            (blocks): Sequential(\n",
      "              (0): Sequential(\n",
      "                (0): Conv1dAuto(128, 128, kernel_size=(7,), stride=(1,), padding=(3,), bias=False)\n",
      "                (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              )\n",
      "              (1): ReLU(inplace=True)\n",
      "              (2): Sequential(\n",
      "                (0): Conv1dAuto(128, 128, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
      "                (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              )\n",
      "              (3): ReLU(inplace=True)\n",
      "              (4): Sequential(\n",
      "                (0): Conv1dAuto(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
      "                (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              )\n",
      "            )\n",
      "            (activate): ReLU(inplace=True)\n",
      "            (shortcut): Sequential(\n",
      "              (0): Conv1d(128, 128, kernel_size=(1,), stride=(1,), bias=False)\n",
      "              (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (2): ResNet753Block(\n",
      "            (blocks): Sequential(\n",
      "              (0): Sequential(\n",
      "                (0): Conv1dAuto(128, 128, kernel_size=(7,), stride=(1,), padding=(3,), bias=False)\n",
      "                (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              )\n",
      "              (1): ReLU(inplace=True)\n",
      "              (2): Sequential(\n",
      "                (0): Conv1dAuto(128, 128, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
      "                (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              )\n",
      "              (3): ReLU(inplace=True)\n",
      "              (4): Sequential(\n",
      "                (0): Conv1dAuto(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
      "                (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              )\n",
      "            )\n",
      "            (activate): ReLU(inplace=True)\n",
      "            (shortcut): Sequential(\n",
      "              (0): Conv1d(128, 128, kernel_size=(1,), stride=(1,), bias=False)\n",
      "              (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): ResnetDecoder(\n",
      "    (avg): AdaptiveAvgPool1d(output_size=(1,))\n",
      "    (decoder): Linear(in_features=128, out_features=27, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from resnet1d import ECGResNet50\n",
    "model = ECGResNet50(750, 27)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0741, -0.0539, -0.0556,  0.0414, -0.0480,  0.0719,  0.0662,  0.0294,\n",
       "          0.0207,  0.0109,  0.0641, -0.0077,  0.0524,  0.0706, -0.0367, -0.0092,\n",
       "          0.0353,  0.0855,  0.0653,  0.0440, -0.0372, -0.0086, -0.0448, -0.0148,\n",
       "          0.0657,  0.0576, -0.0504],\n",
       "        [-0.0741, -0.0539, -0.0556,  0.0414, -0.0480,  0.0719,  0.0662,  0.0294,\n",
       "          0.0207,  0.0109,  0.0641, -0.0077,  0.0524,  0.0706, -0.0367, -0.0092,\n",
       "          0.0353,  0.0855,  0.0653,  0.0440, -0.0372, -0.0086, -0.0448, -0.0148,\n",
       "          0.0657,  0.0576, -0.0504],\n",
       "        [-0.0741, -0.0539, -0.0556,  0.0414, -0.0480,  0.0719,  0.0662,  0.0294,\n",
       "          0.0207,  0.0109,  0.0641, -0.0077,  0.0524,  0.0706, -0.0367, -0.0092,\n",
       "          0.0353,  0.0855,  0.0653,  0.0440, -0.0372, -0.0086, -0.0448, -0.0148,\n",
       "          0.0657,  0.0576, -0.0504],\n",
       "        [-0.0741, -0.0539, -0.0556,  0.0414, -0.0480,  0.0719,  0.0662,  0.0294,\n",
       "          0.0207,  0.0109,  0.0641, -0.0077,  0.0524,  0.0706, -0.0367, -0.0092,\n",
       "          0.0353,  0.0855,  0.0653,  0.0440, -0.0372, -0.0086, -0.0448, -0.0148,\n",
       "          0.0657,  0.0576, -0.0504],\n",
       "        [-0.0741, -0.0539, -0.0556,  0.0414, -0.0480,  0.0719,  0.0662,  0.0294,\n",
       "          0.0207,  0.0109,  0.0641, -0.0077,  0.0524,  0.0706, -0.0367, -0.0092,\n",
       "          0.0353,  0.0855,  0.0653,  0.0440, -0.0372, -0.0086, -0.0448, -0.0148,\n",
       "          0.0657,  0.0576, -0.0504],\n",
       "        [-0.0741, -0.0539, -0.0556,  0.0414, -0.0480,  0.0719,  0.0662,  0.0294,\n",
       "          0.0207,  0.0109,  0.0641, -0.0077,  0.0524,  0.0706, -0.0367, -0.0092,\n",
       "          0.0353,  0.0855,  0.0653,  0.0440, -0.0372, -0.0086, -0.0448, -0.0148,\n",
       "          0.0657,  0.0576, -0.0504],\n",
       "        [-0.0741, -0.0539, -0.0556,  0.0414, -0.0480,  0.0719,  0.0662,  0.0294,\n",
       "          0.0207,  0.0109,  0.0641, -0.0077,  0.0524,  0.0706, -0.0367, -0.0092,\n",
       "          0.0353,  0.0855,  0.0653,  0.0440, -0.0372, -0.0086, -0.0448, -0.0148,\n",
       "          0.0657,  0.0576, -0.0504],\n",
       "        [-0.0741, -0.0539, -0.0556,  0.0414, -0.0480,  0.0719,  0.0662,  0.0294,\n",
       "          0.0207,  0.0109,  0.0641, -0.0077,  0.0524,  0.0706, -0.0367, -0.0092,\n",
       "          0.0353,  0.0855,  0.0653,  0.0440, -0.0372, -0.0086, -0.0448, -0.0148,\n",
       "          0.0657,  0.0576, -0.0504],\n",
       "        [-0.0741, -0.0539, -0.0556,  0.0414, -0.0480,  0.0719,  0.0662,  0.0294,\n",
       "          0.0207,  0.0109,  0.0641, -0.0077,  0.0524,  0.0706, -0.0367, -0.0092,\n",
       "          0.0353,  0.0855,  0.0653,  0.0440, -0.0372, -0.0086, -0.0448, -0.0148,\n",
       "          0.0657,  0.0576, -0.0504],\n",
       "        [-0.0741, -0.0539, -0.0556,  0.0414, -0.0480,  0.0719,  0.0662,  0.0294,\n",
       "          0.0207,  0.0109,  0.0641, -0.0077,  0.0524,  0.0706, -0.0367, -0.0092,\n",
       "          0.0353,  0.0855,  0.0653,  0.0440, -0.0372, -0.0086, -0.0448, -0.0148,\n",
       "          0.0657,  0.0576, -0.0504]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(dummy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from torchvision import datasets, models, transforms\n",
    "from myeval import agg_y_preds_bags, binary_acc, geometry_loss\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from snippets.pytorchtools import EarlyStopping\n",
    "from sklearn.model_selection import GroupKFold\n",
    "import time\n",
    "\n",
    "st = time.time()\n",
    "patience = 50\n",
    "batch_size= 512#65000\n",
    "\n",
    "saved_dir = '../saved/resnet50/'\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "trainDataset = torch.utils.data.Subset(signal_datasets_train, range(0,len(Signals_train), 1))\n",
    "testDataset = torch.utils.data.Subset(signal_datasets_test, range(0,len(Signals_test), 1))\n",
    "\n",
    "trainLoader = torch.utils.data.DataLoader(trainDataset, batch_size=batch_size, shuffle = True, pin_memory=True)#sampler = sampler)\n",
    "testLoader = torch.utils.data.DataLoader(testDataset, batch_size=65000, shuffle = False, pin_memory=True)\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01) \n",
    "#optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9) #\n",
    "# Decay LR by a factor of 0.1 every 100 epochs\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.1)\n",
    "\n",
    "#criterion_train = WeightedBCELoss(weights=weights_tensor, reduction='mean')\n",
    "#criterion_test = WeightedBCELoss(weights=weights_tensor, reduction='mean')\n",
    "\n",
    "class_weights_train = 1.0/np.sum(Data_labels_train, axis=0)\n",
    "class_weights_train = class_weights_train / np.sum(class_weights_train)\n",
    "class_weights_train = torch.Tensor(class_weights_train).to(device)\n",
    "criterion_train = nn.BCEWithLogitsLoss(pos_weight=pos_weight, reduction='mean') \n",
    "criterion_test = nn.BCEWithLogitsLoss(pos_weight=pos_weight, reduction='mean') \n",
    "\n",
    "losses_train = []\n",
    "losses_test = []\n",
    "\n",
    "avg_losses_train = []\n",
    "avg_losses_test = []\n",
    "\n",
    "\n",
    "early_stopping = EarlyStopping(patience, verbose=False, \n",
    "                              saved_dir=saved_dir, \n",
    "                              save_name='resnet50_'+run_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [get_name(label, Dx_map, Dx_map_unscored) for label in labels]\n",
    "\n",
    "assert len(labels) == 27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "893ad76c6a6e4913bf82baacb8ca9e74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "S0 4.47 min |\n",
      " Train Loss: 0.188788, Acc: 0.963, F: 0.034, Fbeta: 0.039, gbeta: 0.016, geo: 0.025, score: -2.034 |\n",
      " Valid Loss: 0.137636, Acc: 0.969, F: 0.822, Fbeta: 0.827, gbeta: 0.799, geo: 0.813, score: -0.661\n",
      " \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "297faa16f2804b68932a69cc267b6adf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "S1 8.99 min |\n",
      " Train Loss: 0.177584, Acc: 0.967, F: 0.071, Fbeta: 0.077, gbeta: 0.037, geo: 0.053, score: -0.449 |\n",
      " Valid Loss: 0.127474, Acc: 0.974, F: 0.740, Fbeta: 0.745, gbeta: 0.707, geo: 0.726, score: 0.234\n",
      " \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07de6f07e3794552a5d772bc2fee6dd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "S2 13.20 min |\n",
      " Train Loss: 0.170642, Acc: 0.968, F: 0.096, Fbeta: 0.105, gbeta: 0.050, geo: 0.072, score: -0.190 |\n",
      " Valid Loss: 0.124044, Acc: 0.973, F: 0.709, Fbeta: 0.713, gbeta: 0.675, geo: 0.694, score: 0.093\n",
      " \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fbe5ab3f4f14474b81b3b75bdbef017",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_trains_tensor = None\n",
    "y_tests_tensor = None\n",
    "for epoch in range(0, 5000):\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    running_loss_train = 0.0\n",
    "\n",
    "    y_trains = [] # ground truth\n",
    "    output_trains = [] # output\n",
    "    for k, (X_train, y_train) in tqdm(enumerate(trainLoader)):\n",
    "        y_train = y_train.to(device)\n",
    "        X_train = X_train.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output_train = model(X_train)\n",
    "        output_trains.append(output_train.cpu())\n",
    "        \n",
    "        loss_train = criterion_train(output_train, y_train)\n",
    "        losses_train.append(loss_train.item())\n",
    "        \n",
    "        avg_loss_train = np.average(losses_train)\n",
    "        avg_losses_train.append(avg_loss_train)\n",
    "    \n",
    "        \n",
    "        \n",
    "        if np.mod(k, 100) == 0:\n",
    "            writer.add_scalar('training loss',\n",
    "            avg_loss_train,\n",
    "            epoch * (len(Data_labels_train)//batch_size//100+1) + k//100)\n",
    "        \n",
    "        y_trains.append(y_train.cpu())\n",
    "            \n",
    "        loss_train.backward()\n",
    "        optimizer.step()\n",
    "            \n",
    "    scheduler.step()\n",
    "\n",
    "    y_tests = [] # ground truth\n",
    "    output_tests = [] # output\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        \n",
    "        for X_test, y_test in testLoader:  \n",
    "            y_test = y_test.to(device)\n",
    "            X_test = X_test.to(device)\n",
    "            output_test = model(X_test)\n",
    "\n",
    "            loss_test = criterion_test(output_test, y_test)\n",
    "            losses_test.append(loss_test.item())\n",
    "\n",
    "            output_tests.append(output_test.cpu())\n",
    "            y_tests.append(y_test.cpu())\n",
    "\n",
    "        avg_loss_test = np.average(losses_test)\n",
    "        avg_losses_test.append(avg_loss_test)\n",
    "\n",
    "        writer.add_scalar('testing loss',\n",
    "                avg_loss_test,\n",
    "                epoch* (len(Data_labels_test)//batch_size+1))\n",
    "\n",
    "\n",
    "\n",
    "    y_trains_tensor = torch.cat(y_trains, axis=0) # ground truth\n",
    "    y_tests_tensor = torch.cat(y_tests, axis=0) # ground truth\n",
    "\n",
    "    output_trains = torch.cat(output_trains, axis=0) \n",
    "    y_train_preds = torch.sigmoid(output_trains)\n",
    "\n",
    "    output_tests = torch.cat(output_tests, axis=0)\n",
    "    y_test_preds = torch.sigmoid(output_tests)\n",
    "\n",
    "    #output_trains = torch.cat(output_trains, axis=0)\n",
    "#     y_train_preds_max, y_train_preds_mean, _ = agg_y_preds_bags(y_train_preds, bag_size=n_segments)\n",
    "#     y_test_preds_max, y_test_preds_mean, _ = agg_y_preds_bags(y_test_preds, bag_size=n_segments)\n",
    "#     _, _, y_trains = agg_y_preds_bags(y_trains, bag_size=n_segments)\n",
    "#     _, _, y_tests = agg_y_preds_bags(y_tests, bag_size=n_segments)\n",
    "\n",
    "\n",
    "    for class_idx in range(N_CLASS):\n",
    "        add_pr_curve_tensorboard(class_idx, y_trains_tensor, y_train_preds, names, global_step=epoch, prefix='train')\n",
    "        add_pr_curve_tensorboard(class_idx, y_tests_tensor, y_test_preds, names, global_step=epoch, prefix='test')\n",
    "\n",
    "\n",
    "\n",
    "    acc, fmeasure, fbeta, gbeta = binary_acc(y_train_preds, y_trains_tensor)           \n",
    "    acc2, fmeasure2, fbeta2, gbeta2 = binary_acc(y_test_preds, y_tests_tensor)\n",
    "    geometry = geometry_loss(fbeta, gbeta)\n",
    "    geometry2 = geometry_loss(fbeta2, gbeta2)\n",
    "    \n",
    "#     output_str = 'S{} {:.2f} min |\\n Train Loss: {:.6f}, Acc: {:.3f}, F: {:.3f}, Fbeta: {:.3f}, gbeta: {:.3f}, geo: {:.3f}, score: {:.3f} |\\n Valid Loss: {:.6f}, Acc: {:.3f}, F: {:.3f}, Fbeta: {:.3f}, gbeta: {:.3f}, geo: {:.3f}, score: {:.3f}\\n '.format(\n",
    "#         epoch, (time.time()-st)/60,\n",
    "#         avg_loss_train, acc, fmeasure, fbeta, gbeta, geometry, np.nan,\n",
    "#         avg_loss_test, acc2, fmeasure2, fbeta2, gbeta2, geometry2, np.nan)\n",
    "#     print(output_str)\n",
    "    score = compute_score(np.round(y_train_preds.data.numpy()), np.round(y_trains_tensor.data.numpy()), weights)\n",
    "    score2 = compute_score(np.round(y_test_preds.data.numpy()), np.round(y_tests_tensor.data.numpy()), weights)\n",
    "    output_str = 'S{} {:.2f} min |\\n Train Loss: {:.6f}, Acc: {:.3f}, F: {:.3f}, Fbeta: {:.3f}, gbeta: {:.3f}, geo: {:.3f}, score: {:.3f} |\\n Valid Loss: {:.6f}, Acc: {:.3f}, F: {:.3f}, Fbeta: {:.3f}, gbeta: {:.3f}, geo: {:.3f}, score: {:.3f}\\n '.format(\n",
    "        epoch, (time.time()-st)/60,\n",
    "        avg_loss_train, acc, fmeasure, fbeta, gbeta, geometry, score,\n",
    "        avg_loss_test, acc2, fmeasure2, fbeta2, gbeta2, geometry2, score2)\n",
    "    \n",
    "    print(output_str)\n",
    "\n",
    "    with open(saved_dir+'loss_{}.txt'.format(run_name), 'a') as f:\n",
    "        print(output_str, file=f)\n",
    "\n",
    "    early_stopping(avg_loss_test, model)\n",
    "\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping\")\n",
    "        break\n",
    "\n",
    "\n",
    "#     output_string = 'AUROC|AUPRC|Accuracy|F-measure|Fbeta-measure|Gbeta-measure|Geomotry\\n{:.3f}|{:.3f}|{:.3f}|{:.3f}|{:.3f}|{:.3f}|{:.3f}'.format(auroc2,auprc2,acc2,fmeasure2,fbeta2,gbeta2,geometry2)\n",
    "#     print(output_string)     \n",
    "#     with open(saved_dir+'score'+ str(i)+ '_epoch' + str(epoch) + '.txt', 'w') as f:\n",
    "#         f.write(output_string)\n",
    "\n",
    "#     avg_losses_train = np.array(avg_losses_train)\n",
    "#     avg_losses_test = np.array(avg_losses_test)\n",
    "\n",
    "#     np.save(saved_dir + 'avg_losses_train' + str(i) + '_epoch' + str(epoch), avg_losses_train)\n",
    "#     np.save(saved_dir + 'avg_losses_test' + str(i) + '_epoch' + str(epoch), avg_losses_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import multilabel_confusion_matrix, confusion_matrix\n",
    "from global_vars import labels\n",
    "\n",
    "cf_matrices = multilabel_confusion_matrix(y_trains_tensor.data.numpy(), np.round(y_train_preds.data.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label, cf_matrix in zip(labels, cf_matrices):\n",
    "    print(get_name(label, Dx_map, Dx_map_unscored))\n",
    "    print(cf_matrix)\n",
    "    print('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_trains_tensor.data.numpy()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 (physioNet)",
   "language": "python",
   "name": "physionet"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
