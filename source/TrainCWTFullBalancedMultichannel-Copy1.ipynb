{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import torch, torchvision \n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_infos4 = pd.read_csv('../saved/infos4.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('../saved/data_imgs2.pkl', 'rb') as f:\n",
    "    data_imgs2 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(data_imgs2[5][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_segments = 11\n",
    "def crop_imgs(data_img, n_segments = n_segments, vis = False):\n",
    "    \n",
    "    shift_len = int((data_img[0].size[0]-224) / (n_segments-1))\n",
    "    if vis:\n",
    "        plt.imshow(data_img)\n",
    "        plt.show()\n",
    "        \n",
    "    # each imgs\n",
    "    imgs = []\n",
    "    for i in range(n_segments):\n",
    "        \n",
    "        # each channel of each segment\n",
    "        img_chns = []\n",
    "        for chn in range(4):\n",
    "            img = data_img[chn].crop((i*shift_len,0,i*shift_len+224,224)) # 0, 0, 224, 224 left, upper, right, and lower\n",
    "            img_chns.append(img)\n",
    "            \n",
    "            if vis:\n",
    "                plt.imshow(img)\n",
    "                plt.show()\n",
    "                \n",
    "        imgs.append(img_chns)\n",
    "    return imgs\n",
    "\n",
    "data_img2_crops = []\n",
    "for i in tqdm(range(len(data_imgs2))):\n",
    "    data_img2_crops += crop_imgs(data_imgs2[i])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_infos4_crops = df_infos4.loc[df_infos4.index.repeat(n_segments)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from global_vars import labels\n",
    "import os\n",
    "class MyImageMultichannelDataset(Dataset):\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, infos, n_segments, channel_imgs):\n",
    "        \"\"\"\n",
    "                \n",
    "        channel_imgs = chn -> array of PImage\n",
    "        \"\"\"\n",
    "        self.infos = infos\n",
    "        self.channel_imgs = channel_imgs\n",
    "        self.n_segments = n_segments\n",
    "        self.transform =  transforms.Compose([\n",
    "                         transforms.ToTensor(),\n",
    "                         transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "                         ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.infos)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        images = [self.channel_imgs[idx][chn] for chn in range(4)]\n",
    "        info_labels = self.infos.iloc[idx][labels]\n",
    "        sample =(torch.cat([self.transform(image) for image in images],0), torch.Tensor(info_labels.astype(int)))\n",
    "\n",
    "        return sample\n",
    "    \n",
    "image_datasets = MyImageMultichannelDataset(df_infos4_crops, n_segments, data_img2_crops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs0, label0 = image_datasets[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torchvision import models\n",
    "# import torch.nn.functional as F\n",
    "# import torch.optim as optim\n",
    "\n",
    "# def load_cwt_nn_model(device, model_saved_path='../saved/modelCWTFullWeightedLoss/modelCWTFull0_model.dict', \n",
    "#                       freeze=True):\n",
    "#     model = models.resnet50(pretrained=True)\n",
    "#     num_ftrs = model.fc.in_features\n",
    "#     model.fc = nn.Linear(num_ftrs, 9)\n",
    "#     model.load_state_dict(torch.load(model_saved_path, map_location=device))\n",
    "    \n",
    "#     if freeze:\n",
    "#         for param in model.parameters():\n",
    "#             param.requires_grad = False\n",
    "#     # change the last output to 9 classes\n",
    "#     model.fc = nn.Linear(num_ftrs, 1000)\n",
    "    \n",
    "#     # load saved model\n",
    "    \n",
    "#     model.eval()\n",
    "#     model.to(device)\n",
    "#     return model\n",
    "\n",
    "# class MultiCWTNet(nn.Module):\n",
    "#     def __init__(self, device, verbose=False):\n",
    "#         super(MultiCWTNet, self).__init__()\n",
    "        \n",
    "#         self.chn_resnets = [load_cwt_nn_model(device) for i in range(4)]\n",
    "#         num_ftrs = self.chn_resnets[0].fc.out_features\n",
    "#         self.fc1 = nn.Linear(num_ftrs*4, 9)\n",
    "#         self.fc2 = nn.Linear(9, 9)\n",
    "#         self.verbose = verbose\n",
    "        \n",
    "#     def forward(self, xs):\n",
    "#         x = [self.chn_resnets[i](xs[i]) for i in range(4)]\n",
    "        \n",
    "#         x = torch.cat(x, 1)\n",
    "#         if self.verbose:\n",
    "#             print('0: ', x.shape, x.device)\n",
    "            \n",
    "#         x = self.fc1(x)\n",
    "#         if self.verbose:\n",
    "#             print('2: ', x.shape)\n",
    "            \n",
    "#         x = F.relu(   x )     \n",
    "#         if self.verbose:\n",
    "#             print('3: ', x.shape)\n",
    "            \n",
    "#         x = self.fc2(x)\n",
    "        \n",
    "#         if self.verbose:\n",
    "#             print('4: ', x.shape)\n",
    "#         return x\n",
    "\n",
    "# model = MultiCWTNet(device, verbose=True)\n",
    "# model.to(device)\n",
    "# for X,y in dataloaders:\n",
    "#     xs = [x.to(device) for x in X]\n",
    "#     model.forward(xs)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torchvision import models\n",
    "    \n",
    "class MultiCWTNet(nn.Module):\n",
    "    def __init__(self, device, verbose=False):\n",
    "        super(MultiCWTNet, self).__init__()\n",
    "        \n",
    "        self.resnet = models.resnet50(pretrained=True)\n",
    "        self.resnet.conv1 = self.increase_channels(self.resnet.conv1, num_channels=12, copy_weights=0)\n",
    "        num_ftrs = self.resnet.fc.in_features\n",
    "        self.resnet.fc = nn.Linear(num_ftrs, 9)\n",
    "\n",
    "        self.verbose = verbose\n",
    "        \n",
    "    def forward(self, xs):\n",
    "        x = self.resnet(xs)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "    def increase_channels(self, m, num_channels=None, copy_weights=0):\n",
    "        \"\"\"\n",
    "        https://github.com/akashpalrecha/Resnet-multichannel/blob/master/multichannel_resnet.py\n",
    "        \n",
    "        takes as input a Conv2d layer and returns the a Conv2d layer with `num_channels` input channels\n",
    "        and all the previous weights copied into the new layer.\n",
    "        \n",
    "        copy_weights (int): copy the weights of the channel (int)\n",
    "        \"\"\"\n",
    "        # number of input channels the new module should have\n",
    "        new_in_channels = num_channels if num_channels is not None else m.in_channels + 1\n",
    "        \n",
    "        # Creating new Conv2d layer\n",
    "        new_m = nn.Conv2d(in_channels=new_in_channels, \n",
    "                          out_channels=m.out_channels, \n",
    "                          kernel_size=m.kernel_size, \n",
    "                          stride=m.stride, \n",
    "                          padding=m.padding,\n",
    "                          bias=False)\n",
    "        \n",
    "        # Copying the weights from the old to the new layer\n",
    "        new_m.weight[:, :m.in_channels, :, :] = m.weight.clone()\n",
    "        \n",
    "        #Copying the weights of the `copy_weights` channel of the old layer to the extra channels of the new layer\n",
    "        for i in range(new_in_channels - m.in_channels): # 12 - 3\n",
    "            channel = m.in_channels + i # 3，4，5，6，7，8，9，10，11\n",
    "            new_m.weight[:, channel:channel+1, :, :] = m.weight[:, copy_weights:copy_weights+1, : :].clone()\n",
    "        new_m.weight = nn.Parameter(new_m.weight)\n",
    "\n",
    "        return new_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def geometry_loss(fbeta, gbeta):\n",
    "    return np.sqrt(fbeta*gbeta)\n",
    "\n",
    "#geometry_loss(fbeta2, gbeta2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from torchvision import datasets, models, transforms\n",
    "from myeval import agg_y_preds_bags, binary_acc\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from snippets.pytorchtools import EarlyStopping\n",
    "from sklearn.model_selection import GroupKFold\n",
    "import time\n",
    "\n",
    "st = time.time()\n",
    "patience = 20\n",
    "kf = GroupKFold(5)\n",
    "batch_size=70\n",
    "\n",
    "saved_dir = '../saved/modelMultiCWTFull/'\n",
    "y = df_infos4_crops[labels].astype(int)\n",
    "\n",
    "for i, (train_idx, test_idx) in enumerate(kf.split(df_infos4_crops, y, df_infos4_crops['ptID'])):\n",
    "    \n",
    "    if i < 2:\n",
    "        continue\n",
    "        \n",
    "    trainDataset = torch.utils.data.Subset(image_datasets, train_idx)\n",
    "    testDataset = torch.utils.data.Subset(image_datasets, test_idx)\n",
    "    \n",
    "    trainLoader = torch.utils.data.DataLoader(trainDataset, batch_size=batch_size, shuffle = True, pin_memory=True)#sampler = sampler)\n",
    "    testLoader = torch.utils.data.DataLoader(testDataset, batch_size = batch_size, shuffle = False, pin_memory=True)\n",
    "\n",
    "    model = MultiCWTNet(device, verbose=False)\n",
    "    model.to(device)\n",
    "    \n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9) #\n",
    "    # Decay LR by a factor of 0.1 every 100 epochs\n",
    "    scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "    \n",
    "    pos_weight = np.ones(9) * 2\n",
    "    pos_weight = torch.Tensor(pos_weight).to(device)\n",
    "    \n",
    "    df_y_train = df_infos4_crops.iloc[train_idx][labels].to_numpy().astype(int)\n",
    "    class_weights = 1.0/np.sum(df_y_train,axis=0)\n",
    "    class_weights = class_weights / np.sum(class_weights)\n",
    "    class_weights = torch.Tensor(class_weights).to(device)\n",
    "    criterion_train = nn.BCEWithLogitsLoss(weight=class_weights, pos_weight=pos_weight, reduction='sum')\n",
    "       \n",
    "    criterion_test = nn.BCEWithLogitsLoss(pos_weight=pos_weight, reduction='mean')\n",
    "\n",
    "    losses_train = []\n",
    "    losses_test = []\n",
    "\n",
    "    avg_losses_train = []\n",
    "    avg_losses_test = []\n",
    "\n",
    "\n",
    "    early_stopping = EarlyStopping(patience, verbose=False, \n",
    "                                  saved_dir=saved_dir, \n",
    "                                   save_name='MutliCWTNetFull11'+str(i))\n",
    "    epoch = 0\n",
    "    auroc = 0\n",
    "    auprc = 0\n",
    "    accuracy = 0\n",
    "    fmeasure = 0\n",
    "    fbeta = 0\n",
    "    gbeta = 0\n",
    "    for epoch in range(25):\n",
    "        \n",
    "        model.train()\n",
    "        output_trains = []\n",
    "        y_trains = []\n",
    "        for X_train, y_train in tqdm(trainLoader):\n",
    "            y_train = y_train.to(device)\n",
    "            X_train = X_train.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output_train = model(X_train)\n",
    "            loss_train = criterion_train(output_train, y_train)\n",
    "            losses_train.append(loss_train.item())\n",
    "            loss_train.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            output_trains.append(output_train.cpu())\n",
    "            y_trains.append(y_train.cpu())\n",
    "\n",
    "        scheduler.step()\n",
    "        \n",
    "        avg_loss_train = np.average(losses_train)\n",
    "        avg_losses_train.append(avg_loss_train)\n",
    "        \n",
    "        output_tests = []\n",
    "        y_tests = []\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            \n",
    "            for X_test, y_test in testLoader:  \n",
    "                y_test = y_test.to(device)\n",
    "                X_test = X_test.to(device)\n",
    "                output_test = model(X_test)\n",
    "                \n",
    "                loss_test = criterion_test(output_test, y_test)\n",
    "                losses_test.append(loss_test.item())\n",
    "                \n",
    "                output_tests.append(output_test.cpu())\n",
    "                y_tests.append(y_test.cpu())\n",
    "                \n",
    "            avg_loss_test = np.average(losses_test)\n",
    "            avg_losses_test.append(avg_loss_test)\n",
    "        \n",
    "        \n",
    "        \n",
    "        y_trains = torch.cat(y_trains, axis=0)\n",
    "        y_tests = torch.cat(y_tests, axis=0)\n",
    "    \n",
    "        output_trains = torch.cat(output_trains, axis=0)\n",
    "        y_train_preds = torch.sigmoid(output_trains)\n",
    "        \n",
    "        output_tests = torch.cat(output_tests, axis=0)\n",
    "        y_test_preds = torch.sigmoid(output_tests)\n",
    "        \n",
    "        #output_trains = torch.cat(output_trains, axis=0)\n",
    "        y_train_preds_max, y_train_preds_mean, _ = agg_y_preds_bags(y_train_preds, bag_size=n_segments)\n",
    "        y_test_preds_max, y_test_preds_mean, _ = agg_y_preds_bags(y_test_preds, bag_size=n_segments)\n",
    "        _, _, y_trains = agg_y_preds_bags(y_trains, bag_size=n_segments)\n",
    "        _, _, y_tests = agg_y_preds_bags(y_tests, bag_size=n_segments)\n",
    "        \n",
    "        for k, (y_train_preds, y_test_preds) in enumerate(zip([y_train_preds_max, y_train_preds_mean],\n",
    "                                                              [y_test_preds_max, y_test_preds_mean])):\n",
    "\n",
    "            acc, fmeasure, fbeta, gbeta, auroc, auprc = binary_acc(y_train_preds, y_trains)\n",
    "            \n",
    "            acc2, fmeasure2, fbeta2, gbeta2, auroc2, auprc2 = binary_acc(y_test_preds, y_tests)\n",
    "\n",
    "            geometry = geometry_loss(fbeta, gbeta)\n",
    "            geometry2 = geometry_loss(fbeta2, gbeta2)\n",
    "            output_str = 'S{}/{} {:.2f} min {}|\\n Train Loss: {:.6f}, Acc: {:.3f}, F: {:.3f}, Fbeta: {:.3f}, gbeta: {:.3f}, auroc: {:.3f}, auprc: {:.3f}, geo: {:.3f} |\\nValid Loss: {:.6f}, Acc: {:.3f}, F: {:.3f}, Fbeta: {:.3f}, gbeta: {:.3f}, auroc: {:.3f}, auprc: {:.3f}, geo: {:.3f}\\n '.format(\n",
    "                i, epoch, (time.time()-st)/60, 'MEAN' if k == 1 else 'MAX',\n",
    "                avg_loss_train, acc, fmeasure, fbeta, gbeta, auroc, auprc, geometry,\n",
    "                avg_loss_test, acc2, fmeasure2, fbeta2, gbeta2, auroc2, auprc2, geometry2)\n",
    "            print(output_str)\n",
    "\n",
    "            with open(saved_dir+'loss11_{}.txt'.format(i), 'a') as f:\n",
    "                print(output_str, file=f)\n",
    "            \n",
    "        early_stopping(-geometry2, model)\n",
    "\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "            \n",
    "    output_string = 'AUROC|AUPRC|Accuracy|F-measure|Fbeta-measure|Gbeta-measure|Geomotry\\n{:.3f}|{:.3f}|{:.3f}|{:.3f}|{:.3f}|{:.3f}|{:.3f}'.format(auroc2,auprc2,acc2,fmeasure2,fbeta2,gbeta2,geometry2)\n",
    "    print(output_string)     \n",
    "    with open(saved_dir+'score'+ str(i)+ '_epoch' + str(epoch) + '.txt', 'w') as f:\n",
    "        f.write(output_string)\n",
    "\n",
    "    avg_losses_train = np.array(avg_losses_train)\n",
    "    avg_losses_test = np.array(avg_losses_test)\n",
    "    \n",
    "    np.save(saved_dir + 'avg_losses_train' + str(i) + '_epoch' + str(epoch), avg_losses_train)\n",
    "    np.save(saved_dir + 'avg_losses_test' + str(i) + '_epoch' + str(epoch), avg_losses_test)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
