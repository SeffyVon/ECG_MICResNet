{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm_notebook\n",
    "from tqdm.notebook import tqdm\n",
    "import torch, torchvision \n",
    "from PIL import Image\n",
    "from global_vars import labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_infos2 = pd.read_csv('../saved/infos2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, models, transforms\n",
    "from myeval import binary_acc\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from snippets.pytorchtools import EarlyStopping\n",
    "from sklearn.model_selection import GroupKFold\n",
    "import time\n",
    "from myeval import binary_acc_core\n",
    "\n",
    "kf = GroupKFold(5)\n",
    "train_indices, test_indices = next(kf.split(df_infos2, df_infos2[labels], df_infos2['ptID']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "def load_cwt_nn_model(model_saved_path='../saved/modelCWTFullBalancedWeightedLoss/modelCWTFull0_model.dict'):\n",
    "    model = models.resnet50(pretrained=True)\n",
    "#     for param in model.parameters():\n",
    "#         param.requires_grad = False\n",
    "    \n",
    "    num_ftrs = model.fc.in_features\n",
    "    # change the last output to 9 classes\n",
    "    model.fc = nn.Linear(num_ftrs, 9)\n",
    "    \n",
    "    # load saved model\n",
    "    model.load_state_dict(torch.load(model_saved_path, map_location=torch.device('cpu')))\n",
    "    model.eval()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from myeval import agg_y_preds, binary_acc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# out type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "daec345e731c431688f28c8b83de66e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=53194.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-a1bf9f7e0f49>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_infos2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../saved/channel_imgs/0/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.png'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mdata_imgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/physioNet/lib/python3.6/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mcopy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1082\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0mreturns\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAn\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mpy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;32mclass\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mPIL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1083\u001b[0m         \"\"\"\n\u001b[0;32m-> 1084\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1085\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1086\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/physioNet/lib/python3.6/site-packages/PIL/ImageFile.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m                             \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m                             \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "data_imgs = []\n",
    "for i in tqdm(range(len(df_infos2))):\n",
    "    with Image.open('../saved/channel_imgs/0/'+str(i)+'.png') as img:\n",
    "        data_imgs.append(img.copy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "\n",
    "with open('../saved/channel_imgs0.pkg', 'wb') as f:\n",
    "    pickle.dump(data_imgs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.24144199291865032 min\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "st = time.time()\n",
    "with open('../saved/channel_imgs0.pkg', 'rb') as f:\n",
    "    data_imgs = pickle.load(f)\n",
    "    \n",
    "print((time.time()-st)/60, 'min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "class MyImageDatasetEvaluate(Dataset):\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, data_imgs):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.data_imgs = data_imgs\n",
    "        self.transform =  transforms.Compose([\n",
    "                         transforms.ToTensor(),\n",
    "                         transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "                         ])\n",
    "    def __len__(self):\n",
    "        return len(self.data_imgs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        image = self.data_imgs[idx]\n",
    "        sample = self.transform(image)\n",
    "        return sample\n",
    "    \n",
    "image_datasets = MyImageDatasetEvaluate(data_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_12ECG_classifier(data_imgs,model):\n",
    "    # Use your classifier here to obtain a label and score for each class. \n",
    "    \n",
    "    fDatas = []\n",
    "    #infos =[]\n",
    "    fData = filter_data(data[:,1000:], highcut=50.0)\n",
    "    #fData = datas[idx][:,1000:]\n",
    "    intervals = sep_rr_interval(fData[:3], height=0.2, distance=200, plot=False)\n",
    "\n",
    "    # basic info\n",
    "    #info = get_basic_info(header_datas[idx], labels)\n",
    "    \n",
    "    # get data\n",
    "    #ptID = info[0]\n",
    "    #print(str(idx) + ' ' + ptID)\n",
    "    for i in range(len(intervals)):\n",
    "        l, r = intervals[i]\n",
    "        fDatas.append(fData[:,l:r])\n",
    "        #infos.append(info)\n",
    "            \n",
    "    fDataRes=np.array([signal.resample(fData, 250, axis=1) for fData in fDatas])\n",
    "    fDataReNorms=np.array([normalize(fDataRe) for fDataRe in fDataRes])\n",
    "    \n",
    "    X = torch.FloatTensor(fDataReNorms)\n",
    "    \n",
    "    try:\n",
    "        output = model(X)\n",
    "        current_score = np.mean(torch.sigmoid(output).data.numpy(), axis=0)\n",
    "        current_label = np.rint(current_score).astype(int)\n",
    "   # print(current_score.shape, current_label.shape)\n",
    "    except:\n",
    "        print(fDataRes, header_data)\n",
    "        current_label = np.zeros((9,), dtype=int)\n",
    "        current_score = np.zeros((9,))\n",
    "    return current_label, current_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_cwt_nn_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3295a1d2f154ca6ab6b04250125ef71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1375.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "y_trues = []\n",
    "y_prob_maxs = []\n",
    "y_prob_means = []\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    for _, test_patient_indices in tqdm(df_infos2.iloc[test_indices].groupby('ptID').indices.items()):\n",
    "        y_true = df_infos2.iloc[test_patient_indices[0]][labels]\n",
    "        y_trues.append(y_true)\n",
    "        X_imgs_patient = torch.utils.data.Subset(image_datasets, test_patient_indices)\n",
    "        loader = torch.utils.data.DataLoader(X_imgs_patient, batch_size=len(test_indices), shuffle = False)#sampler = sampler)\n",
    "\n",
    "        output_patient = None\n",
    "        for X_batch in loader:   \n",
    "            output = model(X_batch)\n",
    "            y_prob_max, y_prob_mean = agg_y_preds(output)\n",
    "            y_prob_maxs.append(y_prob_max)\n",
    "            y_prob_means.append(y_prob_mean)\n",
    "            \n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "auroc,auprc,acc,fmeasure,fbeta,gbeta = binary_acc(torch.stack(y_prob_maxs), torch.Tensor(y_trues))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUROC|AUPRC|Accuracy|F-measure|Fbeta-measure|Gbeta-measure\n",
      "0.120|0.381|0.209|0.120|0.914|0.547\n"
     ]
    }
   ],
   "source": [
    "output_string = 'AUROC|AUPRC|Accuracy|F-measure|Fbeta-measure|Gbeta-measure\\n{:.3f}|{:.3f}|{:.3f}|{:.3f}|{:.3f}|{:.3f}'.format(auroc,auprc,acc,fmeasure,fbeta,gbeta)\n",
    "print(output_string) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "auroc,auprc,acc,fmeasure,fbeta,gbeta = binary_acc(torch.stack(y_prob_means), torch.Tensor(y_trues))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUROC|AUPRC|Accuracy|F-measure|Fbeta-measure|Gbeta-measure\n",
      "0.120|0.381|0.209|0.120|0.919|0.665\n"
     ]
    }
   ],
   "source": [
    "output_string = 'AUROC|AUPRC|Accuracy|F-measure|Fbeta-measure|Gbeta-measure\\n{:.3f}|{:.3f}|{:.3f}|{:.3f}|{:.3f}|{:.3f}'.format(auroc,auprc,acc,fmeasure,fbeta,gbeta)\n",
    "print(output_string) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# use drive format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_infos3 = pd.read_csv('../saved/infos3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('../saved/datas.pkl', 'rb') as f:\n",
    "    datas = pickle.load(f)\n",
    "with open('../saved/header_datas.pkl', 'rb') as f:\n",
    "    header_datas = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pywt\n",
    "def cwt(signals,name='', chn=6, wavelet_type = 'morl', vis=False):\n",
    "    #print(signals.shape)\n",
    "    # signals: channels x time\n",
    "    num_steps = signals.shape[1]\n",
    "    x = np.arange(num_steps) * 0.002\n",
    "    delta_t = x[1] - x[0] # 500Hz 0.02s\n",
    "    #print(delta_t)\n",
    "    #Freq (5, 100)\n",
    "    if wavelet_type == 'morl':\n",
    "        scales = np.linspace(8,200,100)\n",
    "    elif wavelet_type == 'mexh':\n",
    "        scales = np.linspace(5,50,100)\n",
    "    elif wavelet_type == 'gaus8':\n",
    "        scales = np.linspace(12,120,100)\n",
    "    else: # cmor\n",
    "        scales = np.linspace(12,120,100)\n",
    "    \n",
    "\n",
    "    coef, freqs = pywt.cwt(signals[chn], scales, wavelet_type, delta_t)\n",
    "    coef2 = (coef-np.min(coef))/(np.max(coef) - np.min(coef))\n",
    "    #np.save('../saved/cwt/'+name, coef)\n",
    "    if vis:\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(211)\n",
    "        plt.plot(x,signals[chn])\n",
    "\n",
    "        ax = fig.add_subplot(212)\n",
    "        #plt.matshow(coefs.astype(float)) \n",
    "        plt.imshow(coef.astype(float), aspect='auto', extent=(1, signals.shape[1], freqs[-1], freqs[0]), cmap='jet')\n",
    "       # plt.colorbar()\n",
    "        #print((freqs[0], freqs[-1]))\n",
    "       # plt.yticks(freqs)\n",
    "        plt.suptitle(name  + ' ' +str(freqs[-1]) + ' '+  str(freqs[0]))\n",
    "        plt.ylabel('Frequency(Hz)')\n",
    "        plt.xlabel('Time(ms)')\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "        \n",
    "    return coef2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from global_vars import labels\n",
    "def get_image_and_labels(data, header_data):\n",
    "    cmap = plt.get_cmap('jet')\n",
    "    resize = torchvision.transforms.Resize((244, 244))\n",
    "    info = get_basic_info(header_data, labels)\n",
    "    \n",
    "    n_length = data.shape[1]\n",
    "    n_segments = int((n_length)/1000)\n",
    "    \n",
    "    coef = cwt(data, str(i)+' '+str(info[3]), chn=0, vis=False)\n",
    "    data_imgs = []\n",
    "    for j in range(n_segments):\n",
    "        s = j*1000\n",
    "        rgba_img = cmap(coef[:,s:s+1000])\n",
    "        data_img = Image.fromarray((rgba_img[:, :, :3] * 255).astype(np.uint8))\n",
    "        data_imgs.append(resize(data_img))\n",
    "        \n",
    "    return data_imgs, info[4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from global_vars import labels\n",
    "def get_basic_info(header_data, labels):\n",
    "    tmp_hea = header_data[0].split(' ')\n",
    "    ptID = tmp_hea[0]\n",
    "    age = None\n",
    "    sex = None\n",
    "    label = None\n",
    "    Dx = None\n",
    "    for iline in header_data:\n",
    "        if iline.startswith('#Age'):\n",
    "            tmp_age = iline.split(': ')[1].strip()\n",
    "            age = int(tmp_age if tmp_age != 'NaN' else 57)\n",
    "        elif iline.startswith('#Sex'):\n",
    "            tmp_sex = iline.split(': ')[1]\n",
    "            if tmp_sex.strip()=='Female':\n",
    "                sex =1\n",
    "            else:\n",
    "                sex=0\n",
    "        elif iline.startswith('#Dx'):\n",
    "            Dx = np.char.rstrip(iline.split(': ')[1].split(','))\n",
    "            label = [ll in Dx for ll in labels]\n",
    "            \n",
    "    \n",
    "    return [ptID, sex, age, ' '.join(Dx)] + label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6025a8e668374609b4291eb27bc96f70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1375.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yfeng/anaconda3/envs/physioNet/lib/python3.6/site-packages/ipykernel_launcher.py:21: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n"
     ]
    }
   ],
   "source": [
    "model = load_cwt_nn_model()\n",
    "transform =  transforms.Compose([\n",
    "                         transforms.ToTensor(),\n",
    "                         transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "                         ])\n",
    "\n",
    "y_prob_maxs = []\n",
    "y_prob_means = []\n",
    "labels = []\n",
    "with torch.no_grad():\n",
    "    for i in tqdm(np.unique(df_infos3.iloc[test_indices]['idx']), leave=False):#fDataReNorms.shape[0])): #\n",
    "        data_imgs, label = get_image_and_labels(datas[i], header_datas[i])\n",
    "        imgs_tensor = torch.stack([transform(data_img) for data_img in data_imgs])\n",
    "        test_output = model(imgs_tensor)\n",
    "        y_prob_max, y_prob_mean = agg_y_preds(test_output)\n",
    "        y_prob_maxs.append(y_prob_max)\n",
    "        y_prob_means.append(y_prob_mean)\n",
    "        labels.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   2,   12,   13, ..., 6863, 6864, 6872])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(df_infos3.iloc[test_indices]['idx'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#model = load_cwt_nn_model()\n",
    "transform =  transforms.Compose([\n",
    "                         transforms.ToTensor(),\n",
    "                         transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "                         ])\n",
    "\n",
    "y_prob_maxs = []\n",
    "y_prob_means = []\n",
    "labels = []\n",
    "for i in tqdm(test_indices, leave=False):#fDataReNorms.shape[0])): #\n",
    "    data_imgs, label = get_image_and_labels(datas[i], header_datas[i])\n",
    "    imgs_tensor = torch.stack([transform(data_img) for data_img in data_imgs])\n",
    "    test_output = model(imgs_tensor)\n",
    "    y_prob_max, y_prob_mean = agg_y_preds(test_output)\n",
    "    y_prob_maxs.append(y_prob_max)\n",
    "    y_prob_means.append(y_prob_mean)\n",
    "    labels = label"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 (physioNet)",
   "language": "python",
   "name": "physionet"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
